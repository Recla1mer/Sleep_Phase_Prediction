{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Johannes Peter Knoll\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Within this notebook you will learn and test everything that was implemented to preprocess the data\n",
    "for the neural network.\n",
    "\n",
    "Note:   This notebook is rather for those who want to make sure everything works correctly. It is very thorough\n",
    "        and therefore unnecessary if you only want to get a quick start into the predictions. If that is the case, head\n",
    "        to 'Classification_Demo.ipynb'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thorough Demonstration of 'dataset_processing.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The autoreload extension allows you to tweak the code in the imported modules\n",
    "# and rerun cells to reflect the changes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we demonstrate the implemented class that helps you to manage the data you want to pass to the\n",
    "neural network model. \\\n",
    "Its main purpose is to store the data in a uniform way, distribute it into pids and make it\n",
    "easily accessible in a memory efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_processing import *\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Database' is a directory that consists of multiple .pkl filesâ€”one for the database configuration, the others\n",
    "for storing data into one or multiple pids.\n",
    "\n",
    "Each datapoint is saved as dictionary and can contain the following keys:\n",
    "- unique identifier (key: \"ID\")\n",
    "- RRI signal (key: \"RRI\")\n",
    "- MAD signal (key: \"MAD\")\n",
    "- SLP signal (key: \"SLP\")\n",
    "- predicted Sleep-Labels (key: \"SLP_predicted\")\n",
    "- predicted individual probabilities for every sleep stage (key: \"SLP_predicted_probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initializing a new database (calling 'SleepDataManager' on non-existent path) the class will\n",
    "automatically create a directory containing the default database configuration saved as .pkl file. \\\n",
    "When initializing on an existing path, the class accesses the database configuration from the existing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRI_frequency: 4\n",
      "MAD_frequency: 1\n",
      "SLP_frequency: 0.03333333333333333\n",
      "sleep_stage_label: None\n",
      "signal_length_seconds: None\n",
      "wanted_shift_length_seconds: None\n",
      "absolute_shift_deviation_seconds: None\n",
      "number_datapoints: [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\")\n",
    "database_configuration = data_manager.database_configuration\n",
    "\n",
    "for key in database_configuration.keys():\n",
    "    print(f\"{key}: {database_configuration[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't mind all the parameters yet. Necessary ones will be explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Database Configuration\n",
    "\n",
    "Of the above parameters, only the uniform signal frequencies can be changed, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database configuration in new instance on same path:\n",
      "\n",
      "RRI_frequency: 2\n",
      "MAD_frequency: 1\n",
      "SLP_frequency: 2\n",
      "sleep_stage_label: None\n",
      "signal_length_seconds: None\n",
      "wanted_shift_length_seconds: None\n",
      "absolute_shift_deviation_seconds: None\n",
      "number_datapoints: [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "updated_frequencies = {\"RRI_frequency\": 2, \"SLP_frequency\": 2}\n",
    "data_manager.change_uniform_frequencies(updated_frequencies)\n",
    "\n",
    "del data_manager, database_configuration\n",
    "\n",
    "# the change is saved globally:\n",
    "another_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\")\n",
    "database_configuration = another_data_manager.database_configuration\n",
    "\n",
    "print(\"\\nDatabase configuration in new instance on same path:\\n\")\n",
    "for key in database_configuration.keys():\n",
    "    print(f\"{key}: {database_configuration[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_and_remove_directory(\"Processing_Demonstration/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data\n",
    "\n",
    "To ensure the data is uniform you must always provide the sampling frequency for each signal when saving \n",
    "(keys: \"RRI_frequency\", \"MAD_frequency\", \"SLP_frequency\"). \\\n",
    "Furthermore, when adding SLP signals, you need to provide the key: \"sleep_stage_label\" which is a dictionary\n",
    "that is supposed to tell what sleep stage your label correspond to. (Example below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleep stage labels in shhs dataset:\n",
    "# \"wake\": 0,    \"N1\": 1,    \"N2\": 2,    \"N3\": 3,    \"REM\": 5,   \"artifact\": \"other integers\"\n",
    "\n",
    "# in the nn we only divide between wake, LS, DS, REM, and artifact. Above, N1 must be redeclared as \"wake\", \n",
    "# N2 as \"LS\" and N3 as \"DS\":\n",
    "shhs_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifact\": [\"other\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some data with differing sampling frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First datapoints of RRI signal: [4. 5. 1. 4. 5. 1. 3. 1. 3. 5.] (shape: (360,))\n",
      "First datapoints of SLP signal: [4, 4, 5, 3, 2, 1] (shape: 6)\n"
     ]
    }
   ],
   "source": [
    "signal_time_in_seconds = 120\n",
    "rri_frequency = 3 # instead of default: 4 Hz\n",
    "slp_frequency = 1/20 # instead of default: 1/30 Hz\n",
    "\n",
    "# creating signals and printing manually scaled versions\n",
    "rri_signal = np.array([random.randint(1, 5) for i in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "print(f\"First datapoints of RRI signal: {rri_signal[:10]} (shape: {rri_signal.shape})\")\n",
    "slp_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "print(f\"First datapoints of SLP signal: {slp_signal[:10]} (shape: {len(slp_signal)})\")\n",
    "\n",
    "random_sleep_stage_labels = {\"wake\": [0], \"LS\": [1], \"DS\": [2], \"REM\": [3], \"artifact\": [\"other\"]}\n",
    "\n",
    "new_datapoint = {\n",
    "    \"ID\": \"any\",\n",
    "    \"RRI\": rri_signal,\n",
    "    \"RRI_frequency\": rri_frequency,\n",
    "    \"SLP\": slp_signal,\n",
    "    \"SLP_frequency\": slp_frequency,\n",
    "    \"sleep_stage_label\": random_sleep_stage_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the data and investigate the scaling applied to the signals.\n",
    "(Note: Saving a datapoint with an already existing ID overwrites the \"old\" values. You are notified in this case.)\n",
    "\n",
    "The idea is to assign a time stamp to each datapoint in the original and the (new, still unexisting) scaled signal. (index within signal / sampling frequency -> recording time (in seconds))\n",
    "Then, for signals like RRI and MAD, containing continous values, to calculate a scaled datapoint, we just interpolate its value from the\n",
    "two original datapoints its corresponding time stamp lies inbetween.\n",
    "For signals like SLP, containing classification labels, we just take the value of the original datapoint with the closest time stamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 'any' already exists in the data file. Existing keys will be overwritten with new values.\n"
     ]
    }
   ],
   "source": [
    "# initialize database\n",
    "data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\")\n",
    "\n",
    "# saving the new datapoint\n",
    "data_manager.save(copy.deepcopy(new_datapoint))\n",
    "\n",
    "# overwriting old datapoint (with same values for demonstration)\n",
    "data_manager.save(copy.deepcopy(new_datapoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: any\n",
      "RRI: [4.   4.75 3.   1.75 4.   4.75 3.   1.5  3.   1.5 ] (480,)\n",
      "SLP: [4 4 3 2] (4,)\n"
     ]
    }
   ],
   "source": [
    "# load and print the data\n",
    "data_dict = data_manager.load(0)\n",
    "\n",
    "for key in data_dict.keys(): # type: ignore\n",
    "    if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "        print(key + \":\", data_dict[key][:10], data_dict[key].shape) # type: ignore\n",
    "    else:\n",
    "        print(key + \":\", data_dict[key]) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed up data saving\n",
    "\n",
    "As indicated above, every ID in the database will be checked when saving a new datapoint, leading to unnecessary\n",
    "computation time when saving many datapoints. To speed up saving, it is recommended to check if all ID's you are\n",
    "about to save beforehand and then disable the ID checking (with setting: 'unique_id=True')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All ID's are unique.\n"
     ]
    }
   ],
   "source": [
    "# ID's of new datapoints:\n",
    "list_of_ids = [\"1\", \"two\", \"11\"]\n",
    "\n",
    "# check if IDs are unique (raises an error if not)\n",
    "data_manager.check_if_ids_are_unique(list_of_ids)\n",
    "\n",
    "# save new datapoints without checking for uniqueness\n",
    "for id in list_of_ids:\n",
    "    new_datapoint[\"ID\"] = id\n",
    "    data_manager.save(copy.deepcopy(new_datapoint), unique_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Data can be loaded in multiple ways using a string or an integer:\n",
    "- If it's a string that equals a key in the data dictionaries, it will return all entities of that specific key in the database.\n",
    "- If it's a different string, then it will treat it as an ID and return the corresponding data dictionary.\n",
    "- If it's an integer, it will treat it as position in the database and return the corresponding data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "loaded_data = data_manager.load(\"RRI\")\n",
    "# loaded_data = data_manager[\"RRI\"] # same as above\n",
    "print(len(loaded_data)) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1\n",
      "RRI: (480,)\n",
      "SLP: (4,)\n"
     ]
    }
   ],
   "source": [
    "# load data by ID\n",
    "loaded_data = data_manager.load(\"1\") # or data_manager[\"1\"]\n",
    "\n",
    "for key in loaded_data.keys(): # type: ignore\n",
    "    if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "        print(key + \":\", loaded_data[key].shape) # type: ignore\n",
    "    else:\n",
    "        print(key + \":\", loaded_data[key]) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: two\n",
      "RRI: (480,)\n",
      "SLP: (4,)\n"
     ]
    }
   ],
   "source": [
    "# load data by index\n",
    "loaded_data = data_manager.load(2) # or data_manager[2]\n",
    "\n",
    "for key in loaded_data.keys(): # type: ignore\n",
    "    if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "        print(key + \":\", loaded_data[key].shape) # type: ignore\n",
    "    else:\n",
    "        print(key + \":\", loaded_data[key]) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Data\n",
    "\n",
    "Data can be loaded in multiple ways using a string or an integer:\n",
    "- If it's a string that equals a key in the data dictionaries, it will remove this key and corresponding value from all dictionaries within the database.\n",
    "- If it's a different string, it will treat it as an ID and remove the corresponding data dictionary.\n",
    "- If it's an integer, it will treat it as position in the database and remove the corresponding data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "ID: any\n",
      "SLP: (4,)\n",
      "--------------------\n",
      "ID: 1\n",
      "SLP: (4,)\n",
      "--------------------\n",
      "ID: two\n",
      "SLP: (4,)\n",
      "--------------------\n",
      "ID: 11\n",
      "SLP: (4,)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "data_manager.remove(\"RRI\")\n",
    "\n",
    "# print all data\n",
    "for dict in data_manager:\n",
    "    print(\"-\"*20)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "            print(key + \":\", dict[key].shape) # type: ignore\n",
    "        else:\n",
    "            print(key + \":\", dict[key]) # type: ignore\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['any', '1', '11']\n"
     ]
    }
   ],
   "source": [
    "data_manager.remove(\"two\")\n",
    "\n",
    "# print all data ID's\n",
    "print(data_manager[\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '11']\n"
     ]
    }
   ],
   "source": [
    "data_manager.remove(0)\n",
    "\n",
    "# print all data ID's\n",
    "print(data_manager[\"ID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Minor Operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide an overview of other minor operations, we'll restore the datapoints first (by emptying the database and resaving them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all data\n",
    "data_manager.empty_database()\n",
    "\n",
    "new_datapoint[\"ID\"] = \"any\"  # reset ID for new datapoint\n",
    "data_manager.save(copy.deepcopy(new_datapoint))\n",
    "\n",
    "list_of_ids = [\"1\", \"two\", \"11\"]\n",
    "for id in list_of_ids:\n",
    "    new_datapoint[\"ID\"] = id\n",
    "    data_manager.save(copy.deepcopy(new_datapoint), unique_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterating over Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any\n",
      "1\n",
      "two\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for datapoint in data_manager:\n",
    "    print(datapoint[\"ID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if database holds datapoint with certain ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoint with \"ID\" = 1 is in the database.\n"
     ]
    }
   ],
   "source": [
    "specific_id = \"1\"\n",
    "if specific_id in data_manager:\n",
    "    print(f\"Datapoint with \\\"ID\\\" = {specific_id} is in the database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: Processing_Demonstration/data.pkl\n",
      "Database Configuration: {'RRI_frequency': 4, 'MAD_frequency': 1, 'SLP_frequency': 0.03333333333333333, 'sleep_stage_label': {'wake': [0], 'LS': [1], 'DS': [2], 'REM': [3], 'artifact': ['other']}, 'signal_length_seconds': None, 'wanted_shift_length_seconds': None, 'absolute_shift_deviation_seconds': None, 'number_datapoints': [4, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(data_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping Signal Length\n",
    "\n",
    "A neural network can only process data of the same shape. Datapoints with reduced size can be padded to ensure they match the required size.\n",
    "This will of course not be done beforehand, as it would cost unnecessary storage space and is therefore not demonstrated here.\n",
    "\n",
    "Unlike shorter signals, longer ones need to be cropped or splitted into multiple datapoints to ensure they are not oversized.\n",
    "Splitting oversized datapoints within the database into multiple ones with the desired length is shown below.\n",
    "\n",
    "Every datapoint holds multiple signals (RRI, MAD, SLP, etc.).\n",
    "The number of splits resulting from an oversized datapoint depends on the parameter: 'wanted_shift_length_seconds'.\n",
    "The starting points of consecutive parts (splits) are shifted by this value, approximately, resulting in a certain number of splitted parts.\n",
    "\n",
    "Of course, not all shift lengths are useful.\n",
    "We want each starting point to correspond with a value for each signal.\n",
    "As each signal might have a different sampling frequency, the algorithm will look for a shift length close to the user desired one ('wanted_shift_length_seconds') so that the shift length multiplied by the sampling frequency equals a natural number for every sampling frequency.\n",
    "The parameter 'absolute_shift_deviation_seconds' defines how far this shift length is allowed to deviate from the wanted shift length.\n",
    "\n",
    "Note that the algorithm will raise an error if no suitable shift length can be found for the set parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long Maximum Signal Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all data\n",
    "data_manager.empty_database()\n",
    "\n",
    "# create and save data of different lengths\n",
    "signal_times = [28800, 36000, 36030, 54000] # [8h, 10h, 10h + 30s, 15h] in seconds\n",
    "rri_frequency, mad_frequency, slp_frequency = 4, 1, 1/30\n",
    "random_sleep_stage_labels = {\"wake\": [0], \"LS\": [1], \"DS\": [2], \"REM\": [3], \"artifact\": [\"other\"]}\n",
    "\n",
    "for i in range(len(signal_times)):\n",
    "    signal_time_in_seconds = signal_times[i]\n",
    "\n",
    "    rri_signal = np.array([random.randint(1, 5) for _ in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "    mad_signal = np.array([random.randint(1, 5) for _ in range(int(signal_time_in_seconds * mad_frequency))], dtype=np.float64)\n",
    "    slp_signal = [random.randint(1, 5) for _ in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "\n",
    "    new_datapoint = {\n",
    "        \"ID\": str(i),\n",
    "        \"RRI\": rri_signal,\n",
    "        \"RRI_frequency\": rri_frequency,\n",
    "        \"MAD\": mad_signal,\n",
    "        \"MAD_frequency\": mad_frequency,\n",
    "        \"SLP\": slp_signal,\n",
    "        \"SLP_frequency\": slp_frequency,\n",
    "        \"sleep_stage_label\": random_sleep_stage_labels\n",
    "    }\n",
    "\n",
    "    data_manager.save(new_datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoints within the database: 4\n",
      "ID: ['0', '1', '2', '3']\n",
      "Corresponding Signal duration in seconds: [28800. 36000. 36030. 54000.]\n",
      "Total duration of all signals in seconds: 154830.0 (= 154830.0 ?)\n"
     ]
    }
   ],
   "source": [
    "# printing some pre-split information\n",
    "signal_durations = np.array([len(entry) for entry in data_manager[\"RRI\"]])/rri_frequency # type: ignore\n",
    "print(f\"Datapoints within the database: {len(data_manager)}\")\n",
    "print(f\"ID: {data_manager[\"ID\"]}\")\n",
    "print(f\"Corresponding Signal duration in seconds: {signal_durations}\")\n",
    "print(f\"Total duration of all signals in seconds: {np.sum(signal_durations)} (= {data_manager.calculate_total_signal_duration()} ?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting entries within Processing_Demonstration/data.pkl into multiple ones to ensure the contained signals span at most across: 36000 seconds.\n",
      "Initializing progress bar..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ…: 100.0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 4 / 4 | 11 ms / 11 ms (2.6 ms/it) |\n"
     ]
    }
   ],
   "source": [
    "# splitting oversized data into multiple database entries with signal lengths of at most 10 hours\n",
    "data_manager.crop_oversized_data(\n",
    "    signal_length_seconds = 36000,  # 10 hours in seconds\n",
    "    wanted_shift_length_seconds = 5400, # 1.5 hours in seconds\n",
    "    absolute_shift_deviation_seconds = 1800, # 30 minutes in seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoints within the database: 8\n",
      "ID: ['0', '1', '2', '2*', '3', '3*', '3*', '3*']\n",
      "Corresponding Signal duration in seconds: [28800. 36000. 36000. 30630. 36000. 36000. 36000. 36000.]\n",
      "Total duration of all signals in seconds: 275430.0 (= 275430.0 ?)\n"
     ]
    }
   ],
   "source": [
    "# printing some post-split information\n",
    "signal_durations = np.array([len(entry) for entry in data_manager[\"RRI\"]])/rri_frequency # type: ignore\n",
    "print(f\"Datapoints within the database: {len(data_manager)}\")\n",
    "print(f\"ID: {data_manager[\"ID\"]}\")\n",
    "print(f\"Corresponding Signal duration in seconds: {signal_durations}\")\n",
    "print(f\"Total duration of all signals in seconds: {np.sum(signal_durations)} (= {data_manager.calculate_total_signal_duration()} ?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have more datapoints than before (8 instead of 4) and the total signal duration increased, as the splitted parts overlap due to our settings.\n",
    "As soon as a signal exceeds the maximum length of 10 hours, the datapoints are split (see ID=2).\n",
    "\n",
    "As we can see, they do not all have a duration of exactly 10 hours.\n",
    "This is due to the last splitted part containing the final datapoints after the shifting the starting position.\n",
    "This can but must not be 10 hours (compare ID = 2 and 3)\n",
    "\n",
    "The splitted parts have a similar ID: one with the original ID, the others with the original ID plus a '*'.\n",
    "Do not worry to much about this, the one without the star just holds information that applies to all others (see below).\n",
    "\n",
    "To ensure the datapoints can later be refused correctly, the algorithm that splits the individual datapoints saves some\n",
    "additional information to the splitted datapoints. For us, they are not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "ID: 0\n",
      "------------------------------\n",
      "ID: 1\n",
      "------------------------------\n",
      "ID: 2\n",
      "shift_length_seconds: 5400\n",
      "shift: 0\n",
      "------------------------------\n",
      "ID: 2*\n",
      "shift: 1\n",
      "------------------------------\n",
      "ID: 3\n",
      "shift_length_seconds: 6000\n",
      "shift: 0\n",
      "------------------------------\n",
      "ID: 3*\n",
      "shift: 1\n",
      "------------------------------\n",
      "ID: 3*\n",
      "shift: 2\n",
      "------------------------------\n",
      "ID: 3*\n",
      "shift: 3\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print keys of splitted datapoints\n",
    "for data_point in data_manager:\n",
    "    print(\"-\"*30)\n",
    "    for key in data_point.keys(): # type: ignore\n",
    "        if key not in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "            print(f\"{key}: {data_point[key]}\")\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Maximum Signal Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, this does not change compared to the case above.\n",
    "This section functions as a test to ensure that massive splitting does not take too much computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all data\n",
    "data_manager.empty_database()\n",
    "\n",
    "# create and save data of different lengths\n",
    "signal_time_in_seconds = 54000 # [15h] in seconds\n",
    "rri_frequency, mad_frequency, slp_frequency = 4, 1, 1/30\n",
    "random_sleep_stage_labels = {\"wake\": [0], \"LS\": [1], \"DS\": [2], \"REM\": [3], \"artifact\": [\"other\"]}\n",
    "\n",
    "rri_signal = np.array([random.randint(1, 5) for _ in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "mad_signal = np.array([random.randint(1, 5) for _ in range(int(signal_time_in_seconds * mad_frequency))], dtype=np.float64)\n",
    "slp_signal = [random.randint(1, 5) for _ in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "\n",
    "new_datapoint = {\n",
    "    \"ID\": \"0\",\n",
    "    \"RRI\": rri_signal,\n",
    "    \"RRI_frequency\": rri_frequency,\n",
    "    \"MAD\": mad_signal,\n",
    "    \"MAD_frequency\": mad_frequency,\n",
    "    \"SLP\": slp_signal,\n",
    "    \"SLP_frequency\": slp_frequency,\n",
    "    \"sleep_stage_label\": random_sleep_stage_labels\n",
    "}\n",
    "\n",
    "data_manager.save(new_datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoints within the database: 1\n",
      "ID: ['0']\n",
      "Corresponding Signal duration in seconds: [54000.]\n",
      "Total duration of all signals in seconds: 54000.0 (= 54000.0 ?)\n"
     ]
    }
   ],
   "source": [
    "# printing some pre-split information\n",
    "signal_durations = np.array([len(entry) for entry in data_manager[\"RRI\"]])/rri_frequency # type: ignore\n",
    "print(f\"Datapoints within the database: {len(data_manager)}\")\n",
    "print(f\"ID: {data_manager[\"ID\"]}\")\n",
    "print(f\"Corresponding Signal duration in seconds: {signal_durations}\")\n",
    "print(f\"Total duration of all signals in seconds: {np.sum(signal_durations)} (= {data_manager.calculate_total_signal_duration()} ?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting entries within Processing_Demonstration/data.pkl into multiple ones to ensure the contained signals span at most across: 30 seconds.\n",
      "   âœ…: 100.0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 1 / 1 | 34 ms / 34 ms (34 ms/it) |\n"
     ]
    }
   ],
   "source": [
    "# splitting oversized data into multiple database entries with signal lengths of at most 10 hours\n",
    "data_manager.crop_oversized_data(\n",
    "    signal_length_seconds = 30,\n",
    "    wanted_shift_length_seconds = 30,\n",
    "    absolute_shift_deviation_seconds = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than factor 10 increase in computation time seems a lot. But it actually is alright. It used to be worse, ..., 'Maybe get a coffee, book and go on a 2 week trip' worse to be exact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoints within the database: 1800\n",
      "ID: ['0' '0*' '0*' ... '0*' '0*' '0*']\n",
      "Corresponding Signal duration in seconds: [30. 30. 30. ... 30. 30. 30.]\n",
      "Total duration of all signals in seconds: 54000.0 (= 54000.0 ?)\n"
     ]
    }
   ],
   "source": [
    "# printing some post-split information\n",
    "signal_durations = np.array([len(entry) for entry in data_manager[\"RRI\"]])/rri_frequency # type: ignore\n",
    "print(f\"Datapoints within the database: {len(data_manager)}\")\n",
    "print(f\"ID: {np.array(data_manager[\"ID\"])}\")\n",
    "print(f\"Corresponding Signal duration in seconds: {signal_durations}\")\n",
    "print(f\"Total duration of all signals in seconds: {np.sum(signal_durations)} (= {data_manager.calculate_total_signal_duration()} ?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversing Signal Split\n",
    "\n",
    "The ultimate goal is to either train and validate the network model or to predict sleep stages for some data.\n",
    "In the latter scenario, after predicting, you might want to reverse the signal split to restore the original shape of your data.\n",
    "\n",
    "Depending on your settings, some parts may overlap, providing you with multiple predictions for certain parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting entries within Processing_Demonstration/data.pkl into multiple ones to ensure the contained signals span at most across: 36000 seconds.\n",
      "   âœ…: 100.0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 2 / 2 | 5.9 ms / 5.9 ms (2.9 ms/it) |\n",
      "ID '0' already exists in the data file. Existing keys will be overwritten with new values.\n",
      "ID '1' already exists in the data file. Existing keys will be overwritten with new values.\n",
      "ID '1*' already exists in the data file. Existing keys will be overwritten with new values.\n",
      "ID '1*' already exists in the data file. Existing keys will be overwritten with new values.\n",
      "ID '1*' already exists in the data file. Existing keys will be overwritten with new values.\n",
      "ID '1*' already exists in the data file. Existing keys will be overwritten with new values.\n",
      "ID '1*' already exists in the data file. Existing keys will be overwritten with new values.\n",
      "ID '1*' already exists in the data file. Existing keys will be overwritten with new values.\n",
      "ID '1*' already exists in the data file. Existing keys will be overwritten with new values.\n",
      "ID '1*' already exists in the data file. Existing keys will be overwritten with new values.\n",
      "ID '1*' already exists in the data file. Existing keys will be overwritten with new values.\n",
      "------------------------------\n",
      "ID: 1*\n",
      "shift: 1\n",
      "RRI: (144000,)\n",
      "MAD: (36000,)\n",
      "SLP: (1200,)\n",
      "SLP_predicted: (1200,)\n",
      "SLP_predicted_probability: (1200, 5)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# delete all data\n",
    "data_manager.empty_database()\n",
    "\n",
    "# create and save data of different lengths\n",
    "signal_times = [36000, 54000] # [10h, 15h] in seconds\n",
    "rri_frequency, mad_frequency, slp_frequency = 4, 1, 1/30\n",
    "random_sleep_stage_labels = {\"wake\": [0], \"LS\": [1], \"DS\": [2], \"REM\": [3], \"artifact\": [\"other\"]}\n",
    "\n",
    "for i in range(len(signal_times)):\n",
    "    signal_time_in_seconds = signal_times[i]\n",
    "\n",
    "    rri_signal = np.array([random.randint(1, 5) for _ in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "    mad_signal = np.array([random.randint(1, 5) for _ in range(int(signal_time_in_seconds * mad_frequency))], dtype=np.float64)\n",
    "    slp_signal = [random.randint(1, 5) for _ in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "\n",
    "    new_datapoint = {\n",
    "        \"ID\": str(i),\n",
    "        \"RRI\": rri_signal,\n",
    "        \"RRI_frequency\": rri_frequency,\n",
    "        \"MAD\": mad_signal,\n",
    "        \"MAD_frequency\": mad_frequency,\n",
    "        \"SLP\": slp_signal,\n",
    "        \"SLP_frequency\": slp_frequency,\n",
    "        \"sleep_stage_label\": random_sleep_stage_labels\n",
    "    }\n",
    "\n",
    "    data_manager.save(new_datapoint)\n",
    "\n",
    "original_rri = data_manager[\"RRI\"] # type: ignore\n",
    "original_mad = data_manager[\"MAD\"] # type: ignore\n",
    "original_slp = data_manager[\"SLP\"] # type: ignore\n",
    "\n",
    "# splitting oversized data into multiple database entries with signal lengths of at most 10 hours\n",
    "data_manager.crop_oversized_data(\n",
    "    signal_length_seconds = 36000,  # 10 hours in seconds\n",
    "    wanted_shift_length_seconds = 5400, # 1.5 hours in seconds\n",
    "    absolute_shift_deviation_seconds = 1800, # 30 minutes in seconds\n",
    ")\n",
    "\n",
    "# add artificial data in shape of predicted sleep stages to the database\n",
    "for data_point in data_manager:\n",
    "    artificial_data = {\n",
    "        \"ID\": data_point[\"ID\"],\n",
    "        \"SLP_predicted\": np.array([random.randint(0, 4) for _ in range(len(data_point[\"SLP\"]))], dtype=np.int64),\n",
    "        \"SLP_predicted_probability\": np.array([[random.random() for _ in range(5)] for _ in range(len(data_point[\"SLP\"]))], dtype=np.float64),\n",
    "        \"SLP_frequency\": data_manager.database_configuration[\"SLP_frequency\"],\n",
    "    }\n",
    "    data_manager.save(artificial_data)\n",
    "\n",
    "# print data\n",
    "print(\"-\"*30)\n",
    "for data_point in data_manager:\n",
    "    if data_point[\"ID\"][-1] == \"*\": # just print the first splitted datapoint\n",
    "        for key in data_point.keys(): # type: ignore\n",
    "            if key in [\"RRI\", \"MAD\", \"SLP\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "                print(f\"{key}: {data_point[key].shape}\") # type: ignore\n",
    "            else:\n",
    "                print(f\"{key}: {data_point[key]}\")\n",
    "        break\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distributing splitted data parts into individual files (Subprocess of Reversing Signal Split):\n",
      "   âœ…: 100.0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 5 / 5 | 4.0 ms / 4.0 ms (798 Âµs/it) |\n",
      "\n",
      "Merging data points back into the main file and reversing the Signal Split:\n",
      "   âœ…: 100.0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 1 / 1 | 252 ms / 252 ms (252 ms/it) |\n"
     ]
    }
   ],
   "source": [
    "data_manager.reverse_signal_crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data restored successfully!\n"
     ]
    }
   ],
   "source": [
    "# check if the original data is restored\n",
    "split_reversed_rri = data_manager[\"RRI\"] # type: ignore\n",
    "split_reversed_mad = data_manager[\"MAD\"] # type: ignore\n",
    "split_reversed_slp = data_manager[\"SLP\"] # type: ignore\n",
    "\n",
    "rri_distances, mad_distances, slp_distances = 0, 0, 0\n",
    "for i in range(len(original_rri)): # type: ignore\n",
    "    rri_distances += np.abs(np.array(original_rri[i]) - np.array(split_reversed_rri[i])).sum() # type: ignore\n",
    "for i in range(len(original_mad)): # type: ignore\n",
    "    mad_distances += np.abs(np.array(original_mad[i]) - np.array(split_reversed_mad[i])).sum() # type: ignore\n",
    "for i in range(len(original_slp)): # type: ignore\n",
    "    slp_distances += np.abs(np.array(original_slp[i]) - np.array(split_reversed_slp[i])).sum() # type: ignore\n",
    "\n",
    "if rri_distances == 0 and mad_distances == 0 and slp_distances == 0:\n",
    "    print(\"Original data restored successfully!\")\n",
    "else:\n",
    "    print(\"Data restoration failed!\")\n",
    "    print(f\"RRI distances: {rri_distances}, MAD distances: {mad_distances}, SLP distances: {slp_distances}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLP_predicted_probability shape: (1800, 5)\n",
      "Unique lengths of entries within SLP_predicted: [1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "reconstructed_dict = data_manager.load(\"1\")\n",
    "print(\"SLP_predicted_probability shape:\", reconstructed_dict[\"SLP_predicted_probability\"].shape) # type: ignore\n",
    "\n",
    "different_number_entries = []\n",
    "for slp in reconstructed_dict[\"SLP_predicted\"]: # type: ignore\n",
    "    different_number_entries.append(len(slp))\n",
    "print(\"Unique lengths of entries within SLP_predicted:\", np.unique(different_number_entries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, everything worked as expected.\n",
    "The original signals were restored successfully and the predicted signals have the expected shape.\n",
    "\n",
    "`SLP_predicted_probability` holds for every position the probabilities of every sleep stage (a list).\n",
    "The overlapping parts are fused by returning the mean of the corresponding (in different datasets) calculated probabilities.\n",
    "Therefore, for every original SLP stage position (in total 1800 = 15 h * 3600 s * 1 / 30 Hz) it holds the probabilities for every classification label.\n",
    "\n",
    "In contrast, `SLP_predicted` is an array similar to `SLP`, holding the predicted sleep stages (majority probability).\n",
    "In the reverse process, the overlapping sleep stages were just assigned to a list.\n",
    "As different parts of the signal overlap more in the splitted parts (middle parts the most, start and end the least) we accummulate different amount of predicted sleep stages across this signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-, Validation-, Test- Split\n",
    "\n",
    "We aim to train a machine learning model with the data handled by this class.\n",
    "Therefore, we want to be able to separate the data into training-, validation- and test- pids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints in file: 100\n"
     ]
    }
   ],
   "source": [
    "# delete all data\n",
    "data_manager.empty_database()\n",
    "\n",
    "# add a lot of data to the database\n",
    "add_number_datapoints = 100\n",
    "\n",
    "signal_time_in_seconds = 25200 # 7h in seconds\n",
    "rri_frequency = 4\n",
    "mad_frequency = 1\n",
    "slp_frequency = 1/30\n",
    "\n",
    "random_sleep_stage_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "\n",
    "for i in range(add_number_datapoints):\n",
    "    rri_signal = np.array([random.randint(1, 5) for _ in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "    mad_signal = [random.randint(1, 5) for _ in range(int(signal_time_in_seconds * mad_frequency))]\n",
    "    slp_signal = [random.randint(1, 5) for _ in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "    \n",
    "    signal_time_in_seconds += 252 # increase signal time by 252 seconds for each new datapoint (101th datapoint: 14h)\n",
    "\n",
    "    decide_what_data_to_add = random.randint(0, 2)\n",
    "\n",
    "    # add data with RRI, MAD and SLP signals (valid training data)\n",
    "    if decide_what_data_to_add == 0:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"MAD\": mad_signal,\n",
    "            \"MAD_frequency\": mad_frequency,\n",
    "            \"SLP\": slp_signal,\n",
    "            \"SLP_frequency\": slp_frequency,\n",
    "            \"sleep_stage_label\": random_sleep_stage_labels\n",
    "        }\n",
    "    # add data with RRI and MAD signals, but no SLP signal (invalid training data)\n",
    "    elif decide_what_data_to_add == 1:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"MAD\": mad_signal,\n",
    "            \"MAD_frequency\": mad_frequency,\n",
    "        }\n",
    "    # add data with RRI and SLP signals, but no MAD signal (valid training data)\n",
    "    else:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"SLP\": slp_signal,\n",
    "            \"SLP_frequency\": slp_frequency,\n",
    "            \"sleep_stage_label\": random_sleep_stage_labels\n",
    "        }\n",
    "    \n",
    "    data_manager.save(new_datapoint, unique_id=True)\n",
    "\n",
    "print(f\"Number of datapoints in file: {len(data_manager)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on whether we want to distribute our data into two or three pids (`test_size` is provided or `None`), the algorithm will distribute the valid data into corresponding separate files.\n",
    "\n",
    "Data that can not be used to train the network (i.e. missing \"RRI\" and \"SLP\") will be left in the main file (`data.py`).\n",
    "        \n",
    "As we can manage data with \"RRI\" and \"MAD\" and data with \"RRI\" only, the algorithm makes sure that only one of the two types of data is used (the one with more samples).\n",
    "The other type will be left in the main file.\n",
    "This must be done to ensure each batch contains the same data.\n",
    "To also train with the type of data left behind, we must save these datapoints to another file from the begin with and train the network in separate steps on the individual datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: 32 datapoints do not contain a SLP and/or RRI signal and will be left in the main file.\n",
      "Attention: 31 datapoints without MAD signal will be left in the main file.\n",
      "\n",
      "Distributing 80.0% / 10.0% / 10.0% of datapoints into training / validation / test pids, respectively:\n",
      "   âœ…: 100.0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100 / 100 | 82 ms / 82 ms (819 Âµs/it) |\n"
     ]
    }
   ],
   "source": [
    "data_manager.separate_train_test_validation(\n",
    "    train_size = 0.8, \n",
    "    validation_size = 0.1, \n",
    "    test_size = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training-, validation- or test- files can be accessed by stating the pid when creating a new instance of the class on the database:\n",
    "\n",
    "ATTENTION:  \n",
    "-   The instances on all files will have reduced functionality from now on. As the data should\n",
    "    be fully prepared for the network now, the instances are designed to only load data and\n",
    "    not save or edit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Datapoints / Total Duration of each pid:\n",
      "--------------------------------------------------\n",
      "Main: 63 / 2419452.0 seconds (63.00 / 64.22%)\n",
      "Train: 29 / 1052352.0 seconds (29.00 / 27.93%)\n",
      "Validation: 4 / 150696.0 seconds (4.00 / 4.00%)\n",
      "Test: 4 / 144900.0 seconds (4.00 / 3.85%)\n"
     ]
    }
   ],
   "source": [
    "data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\")\n",
    "train_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\", pid = \"train\")\n",
    "validation_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\", pid = \"validation\")\n",
    "test_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\", pid = \"test\")\n",
    "\n",
    "# print some basic information of the datasets\n",
    "total_duration = data_manager.calculate_total_signal_duration(only_current_pid = False)\n",
    "main_duration = data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "train_duration = train_data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "validation_duration = validation_data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "test_duration = test_data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "\n",
    "print(\"Number Datapoints / Total Duration of each pid:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Main: {len(data_manager)} / {main_duration} seconds ({len(data_manager)/add_number_datapoints*100:.2f} / {main_duration/total_duration*100:.2f}%)\")\n",
    "print(f\"Train: {len(train_data_manager)} / {train_duration} seconds ({len(train_data_manager)/add_number_datapoints*100:.2f} / {train_duration/total_duration*100:.2f}%)\")\n",
    "print(f\"Validation: {len(validation_data_manager)} / {validation_duration} seconds ({len(validation_data_manager)/add_number_datapoints*100:.2f} / {validation_duration/total_duration*100:.2f}%)\")\n",
    "print(f\"Test: {len(test_data_manager)} / {test_duration} seconds ({len(test_data_manager)/add_number_datapoints*100:.2f} / {test_duration/total_duration*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the boolean parameters `join_splitted_parts` (default: True) and `equally_distribute_signal_durations` (default: True) control whether all database entries resulting from splitting the originially saved datapoint (due to overlength) should be included in the same pid and if the datapoints should be distributed equally with respect to their signal duration.\n",
    "Note that the latter can only be true if the former parameter is as well.\n",
    "(If you do not join splitted parts, then it does not matter how long the original signal was.)\n",
    "\n",
    "`join_splitted_parts` mostly works by calling `crop_oversized_data` before or after `separate_train_test_validation`.\n",
    "During the distribution, the algorithm mostly checks if the cropping was already performed and will act appropriately.\n",
    "\n",
    "So let just quickly check the impact of `equally_distribute_signal_durations`.\n",
    "Saving only valid data will make this easier for us.\n",
    "We'll see that the actual duration of data will be closer to the desired ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints in file: 100\n"
     ]
    }
   ],
   "source": [
    "# delete all data\n",
    "data_manager.empty_database()\n",
    "\n",
    "# add a lot of data to the database\n",
    "add_number_datapoints = 100\n",
    "\n",
    "signal_time_in_seconds = 25200 # 7h in seconds\n",
    "rri_frequency = 4\n",
    "mad_frequency = 1\n",
    "slp_frequency = 1/30\n",
    "\n",
    "random_sleep_stage_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "\n",
    "for i in range(add_number_datapoints):\n",
    "    rri_signal = np.array([random.randint(1, 5) for _ in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "    mad_signal = [random.randint(1, 5) for _ in range(int(signal_time_in_seconds * mad_frequency))]\n",
    "    slp_signal = [random.randint(1, 5) for _ in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "    \n",
    "    signal_time_in_seconds += 252 # increase signal time by 252 seconds for each new datapoint (101th datapoint: 14h)\n",
    "\n",
    "    new_datapoint = {\n",
    "        \"ID\": str(i),\n",
    "        \"RRI\": rri_signal,\n",
    "        \"RRI_frequency\": rri_frequency,\n",
    "        \"MAD\": mad_signal,\n",
    "        \"MAD_frequency\": mad_frequency,\n",
    "        \"SLP\": slp_signal,\n",
    "        \"SLP_frequency\": slp_frequency,\n",
    "        \"sleep_stage_label\": random_sleep_stage_labels\n",
    "    }\n",
    "    \n",
    "    data_manager.save(new_datapoint, unique_id=True)\n",
    "\n",
    "print(f\"Number of datapoints in file: {len(data_manager)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distributing 80.0% / 20.0% of datapoints into training / validation pids, respectively:\n",
      "   âœ…: 100.0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100 / 100 | 81 ms / 81 ms (802 Âµs/it) |\n",
      "\n",
      "Number Datapoints / Total Duration of each pid:\n",
      "--------------------------------------------------\n",
      "Main: 0 / 0 seconds (0.00 / 0.00%)\n",
      "Train: 80 / 2967552.0 seconds (80.00 / 78.77%)\n",
      "Validation: 20 / 799848.0 seconds (20.00 / 21.23%)\n"
     ]
    }
   ],
   "source": [
    "data_manager.separate_train_test_validation(\n",
    "    train_size = 0.8, \n",
    "    validation_size = 0.2, \n",
    "    random_state = None,\n",
    "    shuffle = True,\n",
    "    join_splitted_parts = True,\n",
    "    equally_distribute_signal_durations = False,\n",
    ")\n",
    "\n",
    "data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\")\n",
    "train_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\", pid = \"train\")\n",
    "validation_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\", pid = \"validation\")\n",
    "\n",
    "# print some basic information of the datasets\n",
    "total_duration = data_manager.calculate_total_signal_duration(only_current_pid = False)\n",
    "main_duration = data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "train_duration = train_data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "validation_duration = validation_data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "\n",
    "print(\"\\nNumber Datapoints / Total Duration of each pid:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Main: {len(data_manager)} / {main_duration} seconds ({len(data_manager)/add_number_datapoints*100:.2f} / {main_duration/total_duration*100:.2f}%)\")\n",
    "print(f\"Train: {len(train_data_manager)} / {train_duration} seconds ({len(train_data_manager)/add_number_datapoints*100:.2f} / {train_duration/total_duration*100:.2f}%)\")\n",
    "print(f\"Validation: {len(validation_data_manager)} / {validation_duration} seconds ({len(validation_data_manager)/add_number_datapoints*100:.2f} / {validation_duration/total_duration*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distributing 80.0% / 20.0% of datapoints into training / validation pids, respectively:\n",
      "   âœ…: 100.0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100 / 100 | 74 ms / 74 ms (739 Âµs/it) |\n",
      "\n",
      "Number Datapoints / Total Duration of each pid:\n",
      "--------------------------------------------------\n",
      "Main: 0 / 0 seconds (0.00 / 0.00%)\n",
      "Train: 80 / 3017448.0 seconds (80.00 / 80.09%)\n",
      "Validation: 20 / 749952.0 seconds (20.00 / 19.91%)\n"
     ]
    }
   ],
   "source": [
    "data_manager.separate_train_test_validation(\n",
    "    train_size = 0.8, \n",
    "    validation_size = 0.2, \n",
    "    random_state = None,\n",
    "    shuffle = True,\n",
    "    join_splitted_parts = True,\n",
    "    equally_distribute_signal_durations = True,\n",
    ")\n",
    "\n",
    "data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\")\n",
    "train_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\", pid = \"train\")\n",
    "validation_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\", pid = \"validation\")\n",
    "\n",
    "# print some basic information of the datasets\n",
    "total_duration = data_manager.calculate_total_signal_duration(only_current_pid = False)\n",
    "main_duration = data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "train_duration = train_data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "validation_duration = validation_data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "\n",
    "print(\"\\nNumber Datapoints / Total Duration of each pid:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Main: {len(data_manager)} / {main_duration} seconds ({len(data_manager)/add_number_datapoints*100:.2f} / {main_duration/total_duration*100:.2f}%)\")\n",
    "print(f\"Train: {len(train_data_manager)} / {train_duration} seconds ({len(train_data_manager)/add_number_datapoints*100:.2f} / {train_duration/total_duration*100:.2f}%)\")\n",
    "print(f\"Validation: {len(validation_data_manager)} / {validation_duration} seconds ({len(validation_data_manager)/add_number_datapoints*100:.2f} / {validation_duration/total_duration*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now applying signal cropping performs this within each dataset separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ATTENTION: No matter in which pid you are calling this function, the data will be split in all of them.\n",
      "\n",
      "Splitting entries within Processing_Demonstration/training_pid.pkl into multiple ones to ensure the contained signals span at most across: 36000 seconds.\n",
      "   âœ…: 100.0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 80 / 80 | 201 ms / 201 ms (2.5 ms/it) |\n",
      "\n",
      "Splitting entries within Processing_Demonstration/validation_pid.pkl into multiple ones to ensure the contained signals span at most across: 36000 seconds.\n",
      "   âœ…: 100.0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 20 / 20 | 70 ms / 70 ms (3.5 ms/it) |\n"
     ]
    }
   ],
   "source": [
    "data_manager.crop_oversized_data(\n",
    "    signal_length_seconds = 36000,  # 10 hours in seconds\n",
    "    wanted_shift_length_seconds = 5400, # 1.5 hours in seconds\n",
    "    absolute_shift_deviation_seconds = 1800, # 30 minutes in seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number Datapoints / Total Duration of each pid:\n",
      "--------------------------------------------------\n",
      "Main: 0 / 0 seconds (0.00 / 0.00%)\n",
      "Train: 152 / 5242008.0 seconds (80.00 / 80.05%)\n",
      "Validation: 38 / 1306332.0 seconds (20.00 / 19.95%)\n"
     ]
    }
   ],
   "source": [
    "data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\")\n",
    "train_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\", pid = \"train\")\n",
    "validation_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\", pid = \"validation\")\n",
    "\n",
    "total_splitted_datapoints = sum(data_manager.database_configuration[\"number_datapoints\"])\n",
    "\n",
    "# print some basic information of the datasets\n",
    "total_duration = data_manager.calculate_total_signal_duration(only_current_pid = False)\n",
    "main_duration = data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "train_duration = train_data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "validation_duration = validation_data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "\n",
    "print(\"\\nNumber Datapoints / Total Duration of each pid:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Main: {len(data_manager)} / {main_duration} seconds ({len(data_manager)/total_splitted_datapoints*100:.2f} / {main_duration/total_duration*100:.2f}%)\")\n",
    "print(f\"Train: {len(train_data_manager)} / {train_duration} seconds ({len(train_data_manager)/total_splitted_datapoints*100:.2f} / {train_duration/total_duration*100:.2f}%)\")\n",
    "print(f\"Validation: {len(validation_data_manager)} / {validation_duration} seconds ({len(validation_data_manager)/total_splitted_datapoints*100:.2f} / {validation_duration/total_duration*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proofing that all splitted parts of the same original datapoint ended up in the same pid (`join_splitted_parts` = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proofing that all splitted parts of the same original datapoint ended up in the same pid\n",
    "train_ids = train_data_manager[\"ID\"]\n",
    "validation_ids = validation_data_manager[\"ID\"]\n",
    "\n",
    "for train_id in train_ids: # type: ignore\n",
    "    for val_id in validation_ids: # type: ignore\n",
    "        if train_id == val_id or train_id + \"*\" == val_id or train_id == val_id + \"*\":\n",
    "            print(f\"Splitted parts of the same original datapoint ended up in different pids.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fuse all (still splitted) data back into the main pid and check (`join_splitted_parts` = False).\n",
    "Note: Signals must be cropped before to choose this setting, we did this above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n"
     ]
    }
   ],
   "source": [
    "data_manager.fuse_train_test_validation()\n",
    "print(len(data_manager))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distributing 80.0% / 20.0% of datapoints into training / validation pids, respectively:\n",
      "   âœ…: 100.0% [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 190 / 190 | 119 ms / 119 ms (623 Âµs/it) |\n"
     ]
    }
   ],
   "source": [
    "data_manager.separate_train_test_validation(\n",
    "    train_size = 0.8, \n",
    "    validation_size = 0.2, \n",
    "    random_state = None,\n",
    "    shuffle = True,\n",
    "    join_splitted_parts = False,\n",
    "    equally_distribute_signal_durations = False,\n",
    ")\n",
    "\n",
    "data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\")\n",
    "train_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\", pid = \"train\")\n",
    "validation_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\", pid = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number Datapoints / Total Duration of each pid:\n",
      "--------------------------------------------------\n",
      "Main: 0 / 0 seconds (0.00 / 0.00%)\n",
      "Train: 152 / 5224410.0 seconds (80.00 / 79.78%)\n",
      "Validation: 38 / 1323930.0 seconds (20.00 / 20.22%)\n"
     ]
    }
   ],
   "source": [
    "# print some basic information of the datasets\n",
    "total_duration = data_manager.calculate_total_signal_duration(only_current_pid = False)\n",
    "main_duration = data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "train_duration = train_data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "validation_duration = validation_data_manager.calculate_total_signal_duration(only_current_pid = True)\n",
    "\n",
    "print(\"\\nNumber Datapoints / Total Duration of each pid:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Main: {len(data_manager)} / {main_duration} seconds ({len(data_manager)/total_splitted_datapoints*100:.2f} / {main_duration/total_duration*100:.2f}%)\")\n",
    "print(f\"Train: {len(train_data_manager)} / {train_duration} seconds ({len(train_data_manager)/total_splitted_datapoints*100:.2f} / {train_duration/total_duration*100:.2f}%)\")\n",
    "print(f\"Validation: {len(validation_data_manager)} / {validation_duration} seconds ({len(validation_data_manager)/total_splitted_datapoints*100:.2f} / {validation_duration/total_duration*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted parts of the same original datapoint ended up in different pids: train and validation.\n"
     ]
    }
   ],
   "source": [
    "# proofing that not all splitted parts of the same original datapoint ended up in the same pid\n",
    "train_ids = train_data_manager[\"ID\"]\n",
    "validation_ids = validation_data_manager[\"ID\"]\n",
    "\n",
    "stop_loop = False\n",
    "for train_id in train_ids: # type: ignore\n",
    "    for val_id in validation_ids: # type: ignore\n",
    "        if train_id == val_id or train_id + \"*\" == val_id or train_id == val_id + \"*\":\n",
    "            print(f\"Splitted parts of the same original datapoint ended up in different pids.\")\n",
    "            stop_loop = True\n",
    "            break\n",
    "    if stop_loop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_remove_directory(directory):\n",
    "    \"\"\"\n",
    "    Cleans and removes the specified directory if it exists.\n",
    "    \"\"\"\n",
    "    entries = os.listdir(directory)\n",
    "    for entry in entries:\n",
    "        if os.path.isdir(os.path.join(directory, entry)):\n",
    "            clean_and_remove_directory(os.path.join(directory, entry))\n",
    "        else:\n",
    "            os.remove(os.path.join(directory, entry))\n",
    "    os.rmdir(directory)\n",
    "\n",
    "clean_and_remove_directory(\"Processing_Demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the implemented functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # type: ignore\n",
    "import random\n",
    "import h5py # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you can check whether the implemented functions in this project work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling number of datapoints from signal- to target- frequency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would highly suggest to provide data where the signals don't need to be scaled to the frequencies of the data\n",
    "used to train the neural network.\n",
    "\n",
    "If there is no other option, then so be it. Here is a demonstration of the functions that will be applied to \n",
    "your data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Classification Frequency: 0.05 -> Target Frequency: 0.03333333333333333\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "Classification array:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "Classification array shape:  (15,)\n",
      "\n",
      "Scaled array:  [ 0  1  3  4  6  7  9 10 12 13]\n",
      "Scaled array shape:  (10,)\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Classification Frequency: 0.02 -> Target Frequency: 0.03333333333333333\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "Classification array:  [0 1 2 3 4 5 6 7 8]\n",
      "Classification array shape:  (9,)\n",
      "\n",
      "Scaled array:  [0 1 1 2 2 3 4 4 5 5 6 7 7 8 8]\n",
      "Scaled array shape:  (15,)\n"
     ]
    }
   ],
   "source": [
    "classification_array = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
    "classification_frequency = 1/20\n",
    "target_frequency = 1/30\n",
    "\n",
    "print(\"-\"*71)\n",
    "print(f\"Classification Frequency: {classification_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*71)\n",
    "print(\"\\nClassification array: \", classification_array)\n",
    "print(\"Classification array shape: \", classification_array.shape)\n",
    "\n",
    "reshaped_array = scale_classification_signal(\n",
    "        signal = classification_array, # type: ignore\n",
    "        signal_frequency = classification_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(\"\\nScaled array: \", reshaped_array)\n",
    "print(\"Scaled array shape: \", reshaped_array.shape)\n",
    "\n",
    "classification_array = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "classification_frequency = 1/50\n",
    "target_frequency = 1/30\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-\"*71)\n",
    "print(f\"Classification Frequency: {classification_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*71)\n",
    "print(\"\\nClassification array: \", classification_array)\n",
    "print(\"Classification array shape: \", classification_array.shape)\n",
    "\n",
    "reshaped_array = scale_classification_signal(\n",
    "        signal = classification_array, # type: ignore\n",
    "        signal_frequency = classification_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(\"\\nScaled array: \", reshaped_array)\n",
    "print(\"Scaled array shape: \", reshaped_array.shape)\n",
    "\n",
    "del reshaped_array, classification_array, classification_frequency, target_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Continuous Frequency: 3 -> Target Frequency: 4\n",
      "---------------------------------------------------------------------------\n",
      "Continuous array: [0 1 2 3 4 5] / [0. 1. 2. 3. 4. 5.]\n",
      "Continuous array shape:  (6,)\n",
      "\n",
      "Scaled array: [0 1 2 2 3 4 4 5] / [0.   0.75 1.5  2.25 3.   3.75 4.5  5.  ]\n",
      "Scaled array shape:  (8,)\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Continuous Frequency: 5 -> Target Frequency: 4\n",
      "---------------------------------------------------------------------------\n",
      "Continuous array: [0 1 2 3 4 5 6 7 8 9] / [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "Continuous array shape:  (10,)\n",
      "\n",
      "Scaled array: [0 1 2 4 5 6 8 9] / [0.   1.25 2.5  3.75 5.   6.25 7.5  8.75]\n",
      "Scaled array shape:  (8,)\n"
     ]
    }
   ],
   "source": [
    "continuous_array_int = np.array([0, 1, 2, 3, 4, 5])\n",
    "continuous_array_float = np.array([0, 1, 2, 3, 4, 5], dtype = float)\n",
    "continuous_frequency = 3\n",
    "target_frequency = 4\n",
    "\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous Frequency: {continuous_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous array: {continuous_array_int} / {continuous_array_float}\")\n",
    "print(\"Continuous array shape: \", continuous_array_int.shape)\n",
    "\n",
    "reshaped_array_int = interpolate_signal(\n",
    "        signal = continuous_array_int, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "reshaped_array_float = interpolate_signal(\n",
    "        signal = continuous_array_float, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(f\"\\nScaled array: {reshaped_array_int} / {reshaped_array_float}\")\n",
    "print(\"Scaled array shape: \", reshaped_array_int.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "continuous_array_int = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "continuous_array_float = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype = float)\n",
    "continuous_frequency = 5\n",
    "target_frequency = 4\n",
    "\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous Frequency: {continuous_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous array: {continuous_array_int} / {continuous_array_float}\")\n",
    "print(\"Continuous array shape: \", continuous_array_int.shape)\n",
    "\n",
    "reshaped_array_int = interpolate_signal(\n",
    "        signal = continuous_array_int, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "reshaped_array_float = interpolate_signal(\n",
    "        signal = continuous_array_float, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(f\"\\nScaled array: {reshaped_array_int} / {reshaped_array_float}\")\n",
    "print(\"Scaled array shape: \", reshaped_array_int.shape)\n",
    "\n",
    "del reshaped_array_int, reshaped_array_float, continuous_array_int, continuous_array_float, continuous_frequency, target_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting a signal which is too long for the neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A signal which is too short will be padded with zeros. No big deal. On the other hand: A signal which is too \n",
    "long will be splitted into multiple signals. To create more data, the 10 hour range will be shifted along the\n",
    "signal.\n",
    "\n",
    "This shift should not be too small, to create redundant data but also not too big, because the more data the \n",
    "better. So we try to find a shift size close to 1 hour, which lets us shift an integer amount of times\n",
    "easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding optimal shift size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "Optimal shift length for signal which is 2.5 hours longer than desired length of 10 hours: 0.833 hours\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "signal_length_addition_hours = 2.5\n",
    "desired_length_hours = 10\n",
    "\n",
    "optimal_shift_length = calculate_optimal_shift_length(\n",
    "        signal_length_seconds = (desired_length_hours + signal_length_addition_hours) * 3600, # type: ignore\n",
    "        desired_length_seconds = desired_length_hours*3600, \n",
    "        wanted_shift_length_seconds = 3600,\n",
    "        absolute_shift_deviation_seconds = 1800,\n",
    "        all_signal_frequencies = [4, 1, 1/30, 1/120]\n",
    ")\n",
    "print(optimal_shift_length)\n",
    "\n",
    "print(f\"Optimal shift length for signal which is {signal_length_addition_hours} hours longer than desired length of {desired_length_hours} hours: {round(optimal_shift_length/3600, 3)} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function to find optimal shift length is embedded in the following split funtion. The optimal\n",
    "shift size will be estimated for every signal individually.\n",
    "\n",
    "If there is no integer shift size in range, that lets you shift the signal so, that you perfectly enclose the\n",
    "last datapoints of the long signal, then the last shift will be altered so that it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random signal\n",
    "frequency = 4\n",
    "length_signal_seconds = 12.1 * 3600\n",
    "signal = np.random.rand(int(length_signal_seconds * frequency))\n",
    "\n",
    "# Only important parameters here:\n",
    "nn_signal_seconds = 10 * 3600\n",
    "shift_length_seconds = 3600\n",
    "absolute_shift_deviation_seconds = 1800\n",
    "\n",
    "signals_from_splitting, shift_length = split_long_signal(\n",
    "        signal = signal, # type: ignore\n",
    "        sampling_frequency = frequency,\n",
    "        target_frequency = frequency,\n",
    "        nn_signal_duration_seconds = nn_signal_seconds,\n",
    "        wanted_shift_length_seconds = shift_length_seconds,\n",
    "        absolute_shift_deviation_seconds = absolute_shift_deviation_seconds\n",
    "        )\n",
    "\n",
    "print(\"Shift length:\", shift_length)\n",
    "print(f\"Shift length: {shift_length / frequency} seconds\")\n",
    "print(\"Signal shape: \", signal.shape)\n",
    "print(f\"Datapoints in NN: {nn_signal_seconds * frequency}\")\n",
    "print(\"Signals from splitting shape: \", list_shape(signals_from_splitting))\n",
    "\n",
    "del signals_from_splitting, signal, shift_length, frequency, nn_signal_seconds, shift_length_seconds, absolute_shift_deviation_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting signals within dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dictionary:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: (174240,)\n",
      "RRI_frequency: 4\n",
      "MAD: (43560,)\n",
      "MAD_frequency: 1\n",
      "\n",
      "New dictionaries:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: (144000,)\n",
      "RRI_frequency: 4\n",
      "MAD: (36000,)\n",
      "MAD_frequency: 1\n",
      "shift_length_seconds: 3780\n",
      "\n",
      "ID: 1_shift_x1\n",
      "RRI: (144000,)\n",
      "RRI_frequency: 4\n",
      "MAD: (36000,)\n",
      "MAD_frequency: 1\n",
      "shift_length_seconds: 3780\n",
      "\n",
      "ID: 1_shift_x2\n",
      "RRI: (144000,)\n",
      "RRI_frequency: 4\n",
      "MAD: (36000,)\n",
      "MAD_frequency: 1\n",
      "shift_length_seconds: 3780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create random signal\n",
    "length_signal_seconds = 12.1 * 3600\n",
    "rri_frequency = 4\n",
    "mad_frequency = 1\n",
    "rri_signal = np.random.rand(int(length_signal_seconds * rri_frequency))\n",
    "mad_signal = np.random.rand(int(length_signal_seconds * mad_frequency))\n",
    "\n",
    "data_dict = {\n",
    "    \"ID\": \"1\",\n",
    "    \"RRI\": rri_signal,\n",
    "    \"RRI_frequency\": rri_frequency,\n",
    "    \"MAD\": mad_signal,\n",
    "    \"MAD_frequency\": mad_frequency,\n",
    "}\n",
    "\n",
    "new_dictionaries = split_signals_within_dictionary(\n",
    "    data_dict = data_dict,\n",
    "    id_key = \"ID\",\n",
    "    valid_signal_keys = [\"RRI\", \"MAD\"],\n",
    "    signal_frequencies = [rri_frequency, mad_frequency],\n",
    "    signal_target_frequencies = [rri_frequency, mad_frequency],\n",
    "    nn_signal_duration_seconds = 10 * 3600,\n",
    "    wanted_shift_length_seconds = 3600,\n",
    "    absolute_shift_deviation_seconds = 1800,\n",
    "    all_signal_frequencies = [rri_frequency, mad_frequency]\n",
    ")\n",
    "\n",
    "print(\"Original dictionary:\")\n",
    "print(\"-\"*20)\n",
    "for key, value in data_dict.items():\n",
    "    if key == \"RRI\" or key == \"MAD\" or key == \"SLP\":\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "print(\"\\nNew dictionaries:\")\n",
    "print(\"-\"*20)\n",
    "for new_dict in new_dictionaries:\n",
    "    for key, value in new_dict.items():\n",
    "        if key == \"RRI\" or key == \"MAD\" or key == \"SLP\":\n",
    "            print(f\"{key}: {value.shape}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fusing signals back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted Signals:\n",
      " [array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15]), array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21])]\n",
      "\n",
      "Fused signal:\n",
      " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n"
     ]
    }
   ],
   "source": [
    "signals_from_splitting, shift_length = split_long_signal(\n",
    "        signal = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], # type: ignore\n",
    "        sampling_frequency = 1,\n",
    "        target_frequency = 1,\n",
    "        nn_signal_duration_seconds = 10,\n",
    "        wanted_shift_length_seconds = 5,\n",
    "        absolute_shift_deviation_seconds = 1,\n",
    "        all_signal_frequencies = [1]\n",
    "        )\n",
    "\n",
    "print(\"Splitted Signals:\\n\", signals_from_splitting)\n",
    "\n",
    "fused_signal = fuse_splitted_signals(\n",
    "    signals = signals_from_splitting, # type: ignore\n",
    "    shift_length = int(shift_length), # type: ignore\n",
    "    signal_type = \"feature\"\n",
    ")\n",
    "\n",
    "print(\"\\nFused signal:\\n\", fused_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fusing splitted dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dictionary:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: (46,)\n",
      "RRI_frequency: 2\n",
      "MAD: (23,)\n",
      "MAD_frequency: 1\n",
      "SLP: (12,)\n",
      "SLP_frequency: 0.5\n"
     ]
    }
   ],
   "source": [
    "data_dict = {\n",
    "    \"ID\": \"1\",\n",
    "    \"RRI\": np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22]),\n",
    "    \"RRI_frequency\": 2,\n",
    "    \"MAD\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]),\n",
    "    \"MAD_frequency\": 1,\n",
    "    \"SLP\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n",
    "    \"SLP_frequency\": 0.5\n",
    "}\n",
    "\n",
    "new_dictionaries = split_signals_within_dictionary(\n",
    "    data_dict = data_dict,\n",
    "    id_key = \"ID\",\n",
    "    valid_signal_keys = [\"RRI\", \"MAD\", \"SLP\"],\n",
    "    signal_frequencies = [2, 1, 0.5],\n",
    "    signal_target_frequencies = [2, 1, 0.5],\n",
    "    nn_signal_duration_seconds = 10,\n",
    "    wanted_shift_length_seconds = 5,\n",
    "    absolute_shift_deviation_seconds = 2,\n",
    "    all_signal_frequencies = [2, 1, 0.5]\n",
    ")\n",
    "\n",
    "print(\"Original dictionary:\")\n",
    "print(\"-\"*20)\n",
    "for key, value in data_dict.items():\n",
    "    if key == \"RRI\" or key == \"MAD\" or key == \"SLP\":\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New dictionaries:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: [0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9]\n",
      "RRI_frequency: 2\n",
      "MAD: [0 1 2 3 4 5 6 7 8 9]\n",
      "MAD_frequency: 1\n",
      "SLP: [0 1 2 3 4]\n",
      "SLP_frequency: 0.5\n",
      "shift_length_seconds: 6\n",
      "\n",
      "ID: 1_shift_x1\n",
      "RRI: [ 6  6  7  7  8  8  9  9 10 10 11 11 12 12 13 13 14 14 15 15]\n",
      "RRI_frequency: 2\n",
      "MAD: [ 6  7  8  9 10 11 12 13 14 15]\n",
      "MAD_frequency: 1\n",
      "SLP: [3 4 5 6 7]\n",
      "SLP_frequency: 0.5\n",
      "shift_length_seconds: 6\n",
      "\n",
      "ID: 1_shift_x2\n",
      "RRI: [12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21]\n",
      "RRI_frequency: 2\n",
      "MAD: [12 13 14 15 16 17 18 19 20 21]\n",
      "MAD_frequency: 1\n",
      "SLP: [ 6  7  8  9 10]\n",
      "SLP_frequency: 0.5\n",
      "shift_length_seconds: 6\n",
      "\n",
      "ID: 1_shift_x3\n",
      "RRI: [18 18 19 19 20 20 21 21 22 22]\n",
      "RRI_frequency: 2\n",
      "MAD: [18 19 20 21 22]\n",
      "MAD_frequency: 1\n",
      "SLP: [ 9 10 11]\n",
      "SLP_frequency: 0.5\n",
      "shift_length_seconds: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNew dictionaries:\")\n",
    "print(\"-\"*20)\n",
    "for new_dict in new_dictionaries:\n",
    "    for key, value in new_dict.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1\n",
      "RRI: [ 0  0  1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10 11 11\n",
      " 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22]\n",
      "MAD: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]\n",
      "SLP: [ 0  1  2  3  4  5  6  7  8  9 10  9 10 11]\n",
      "RRI_frequency: 2\n",
      "MAD_frequency: 1\n",
      "SLP_frequency: 0.5\n"
     ]
    }
   ],
   "source": [
    "fused_dictionary = fuse_splitted_signals_within_dictionaries(\n",
    "    data_dictionaries = new_dictionaries,\n",
    "    valid_signal_keys = [\"RRI\", \"MAD\", \"SLP\"],\n",
    "    valid_signal_frequencies = [2, 1, 0.5],\n",
    ")\n",
    "\n",
    "for key, value in fused_dictionary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a .h5 - file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the available training datasets for our neural network model are stored in a .h5 file. So we need\n",
    "to be able to read it. These are the important operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random patient: 201966_1\n",
      "[0 1 2 3 5]\n",
      "\n",
      "key: slp\n",
      "Data shape: (747,)\n",
      "Data frequency: 0.03333333333333333\n",
      "Inverse data frequency: 30.0\n",
      "Data length: 22410.0 s\n",
      "\n",
      "key: rri\n",
      "Data shape: (89640,)\n",
      "Data frequency: 4\n",
      "Inverse data frequency: 0.25\n",
      "Data length: 22410.0 s\n"
     ]
    }
   ],
   "source": [
    "shhs_dataset = h5py.File(\"Raw_Data/SHHS_dataset.h5\", 'r')\n",
    "patients = list(shhs_dataset['slp'].keys()) # type: ignore\n",
    "\n",
    "random_patient = patients[np.random.randint(0, len(patients))]\n",
    "print(f\"Random patient: {random_patient}\")\n",
    "\n",
    "print(np.unique(shhs_dataset[\"slp\"][random_patient][:])) # type: ignore\n",
    "\n",
    "for key in [\"slp\", \"rri\"]:\n",
    "    print(f\"\\nkey: {key}\")\n",
    "\n",
    "    patient_data = shhs_dataset[key][random_patient][:] # type: ignore\n",
    "    print(f\"Data shape: {patient_data.shape}\") # type: ignore\n",
    "\n",
    "    data_freq = shhs_dataset[key].attrs[\"freq\"] # type: ignore\n",
    "    print(f\"Data frequency: {data_freq}\")\n",
    "    print(f\"Inverse data frequency: {1/data_freq}\") # type: ignore\n",
    "\n",
    "    print(f\"Data length: {patient_data.shape[0]/data_freq} s\") # type: ignore\n",
    "\n",
    "del shhs_dataset, patients, random_patient, key, patient_data, data_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide up a signal into overlapping windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardest thing about this is, that 'window_overlap' and 'datapoints_per_window' must be chosen so that\n",
    "the whole signal fits perfectly into n windows. \n",
    "\n",
    "Additionally, those values must be integers. This means that 'window_duration_seconds' and 'overlap_seconds'\n",
    "multiplied with 'target_fequency' as well as 'sampling_frequency' must be integers. (The features and the target labels\n",
    "must fit equally well into the windows, so that we can find the correlation between a feature- and target- window.)\n",
    "\n",
    "We have the RRI and MAD values as features and the sleep phase as target classification. As we will see,\n",
    "RRI and MAD values were recorded with an integer sampling frequency. While the sampling frequency of the \n",
    "sleep classification is 1/30. \n",
    "\n",
    "Finding window parameters that fullfill the conditions mentioned is easier than it sounds. We will always pass data\n",
    "to the neural network that is 10 hours long. Now, we just need to think in seconds and find integer values\n",
    "for 'window_duration_seconds' and 'overlap_seconds' that are a multiple of 30:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding optimal window_parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suitable window parameters for signal of length: 36000:\n",
      "-------------------------------------------------------\n",
      "Number of windows: 1025, Window size: 160, Overlap: 125.0\n",
      "Number of windows: 1026, Window size: 125, Overlap: 90.0\n",
      "Number of windows: 1055, Window size: 164, Overlap: 130.0\n",
      "Number of windows: 1056, Window size: 130, Overlap: 96.0\n",
      "Number of windows: 1087, Window size: 162, Overlap: 129.0\n",
      "Number of windows: 1088, Window size: 129, Overlap: 96.0\n",
      "Number of windows: 1121, Window size: 160, Overlap: 128.0\n",
      "Number of windows: 1122, Window size: 128, Overlap: 96.0\n",
      "Number of windows: 1157, Window size: 164, Overlap: 133.0\n",
      "Number of windows: 1158, Window size: 133, Overlap: 102.0\n",
      "Number of windows: 1196, Window size: 150, Overlap: 120.0\n",
      "Number of windows: 1197, Window size: 120, Overlap: 90.0\n"
     ]
    }
   ],
   "source": [
    "find_suitable_window_parameters(\n",
    "        signal_length = 10 * 3600,\n",
    "        number_windows_range = (1000, 1400),\n",
    "        window_size_range = (120, 180),\n",
    "        minimum_window_size_overlap_difference = 30\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our options are:\n",
    "\n",
    "Number of windows: 1196, Window size: 150, Overlap: 120.0 \\\n",
    "Number of windows: 1197, Window size: 120, Overlap: 90.0\n",
    "\n",
    "We will choose the latter, because we don't want the window_size to be too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When transforming a classification signal into windows, which is supposed to be the target in the neural \n",
    "network, then each window will only be represented by the most common sleep stage. If there is a tie\n",
    "between the labels, then the one with the highest priority will be chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal shape: (1200,)\n",
      "Signal in windows shape: (1197,)\n"
     ]
    }
   ],
   "source": [
    "signal_length_seconds = 10 * 3600\n",
    "frequency = 1/30\n",
    "signal_length = int(signal_length_seconds * frequency)\n",
    "\n",
    "signal = np.array([random.randint(0, 5) for _ in range(signal_length)])\n",
    "\n",
    "signal_in_windows = signal_to_windows(\n",
    "    signal = signal, # type: ignore\n",
    "    datapoints_per_window = int(120 * frequency),\n",
    "    window_overlap = int(90 * frequency),\n",
    "    signal_type = \"target\",\n",
    "    priority_order = [0, 1, 2, 3, 4, 5, -1]\n",
    "    )\n",
    "\n",
    "print(f\"Signal shape: {signal.shape}\")\n",
    "print(f\"Signal in windows shape: {signal_in_windows.shape}\")\n",
    "\n",
    "del signal, signal_in_windows, signal_length_seconds, frequency, signal_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal shape: (36000,)\n",
      "Signal in windows shape: (1197, 120)\n"
     ]
    }
   ],
   "source": [
    "signal = np.random.rand(36000)\n",
    "\n",
    "signal_in_windows = signal_to_windows(\n",
    "    signal = signal, # type: ignore\n",
    "    datapoints_per_window = 120,\n",
    "    window_overlap = 90,\n",
    "    signal_type = \"feature\"\n",
    "    )\n",
    "\n",
    "print(f\"Signal shape: {signal.shape}\")\n",
    "print(f\"Signal in windows shape: {signal_in_windows.shape}\")\n",
    "\n",
    "del signal, signal_in_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will be applied to transform a signal into overlapping windows. It will make sure\n",
    "that the data is passed correctly to the function mentioned above. \n",
    "\n",
    "This means it will:\n",
    "- check if 'number_nn_datapoints', 'datapoints_per_window' and 'window_overlap' are integers\n",
    "- check if 'datapoints_per_window' and 'window_overlap' perfectly fit into 'number_nn_datapoints'\n",
    "- compare length of provided signal to length of signal in nn ('number_nn_datapoints')\n",
    "    - if smaller: Pad with Zeros\n",
    "    - if bigger: Print warning, but continue by cropping last datapoints\n",
    "- check if signal transformed to windows has the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random array shape: (36000,)\n",
      "Reshaped array shape: (1197, 480)\n",
      "Random array shape: (1200,)\n",
      "Reshaped array shape: (1197,)\n"
     ]
    }
   ],
   "source": [
    "random_array = np.random.rand(36000)\n",
    "reshaped_array = reshape_signal_to_overlapping_windows(\n",
    "    signal = random_array, # type: ignore\n",
    "    target_frequency = 4, \n",
    "    number_windows = 1197, \n",
    "    window_duration_seconds = 120, \n",
    "    overlap_seconds = 90,\n",
    "    signal_type = \"feature\",\n",
    "    nn_signal_duration_seconds = 10*3600,\n",
    "    )\n",
    "\n",
    "print(f\"Random array shape: {random_array.shape}\")\n",
    "print(f\"Reshaped array shape: {reshaped_array.shape}\")\n",
    "\n",
    "random_array = np.array([random.randint(0, 3) for _ in range(int(36000/30))])\n",
    "reshaped_array = reshape_signal_to_overlapping_windows(\n",
    "    signal = random_array, # type: ignore\n",
    "    target_frequency = 1/30, \n",
    "    number_windows = 1197, \n",
    "    window_duration_seconds = 120, \n",
    "    overlap_seconds = 90,\n",
    "    signal_type = \"target\",\n",
    "    nn_signal_duration_seconds = 10*3600,\n",
    "    )\n",
    "\n",
    "print(f\"Random array shape: {random_array.shape}\")\n",
    "print(f\"Reshaped array shape: {reshaped_array.shape}\")\n",
    "\n",
    "del random_array, reshaped_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse Reshape\n",
    "\n",
    "Reversing Reshape of feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original signal:\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "Signal reshaped to overlapping windows:\n",
      "[[ 1  2  3  4  5]\n",
      " [ 2  3  4  5  6]\n",
      " [ 3  4  5  6  7]\n",
      " [ 4  5  6  7  8]\n",
      " [ 5  6  7  8  9]\n",
      " [ 6  7  8  9 10]\n",
      " [ 7  8  9 10  0]\n",
      " [ 8  9 10  0  0]\n",
      " [ 9 10  0  0  0]\n",
      " [10  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]]\n",
      "\n",
      "Last window when padding was cropped:\n",
      "[10  0  0  0  0]\n",
      "\n",
      "Signal reshaped back to original:\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original signal:\")\n",
    "test = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "print(test)\n",
    "\n",
    "print(\"\\nSignal reshaped to overlapping windows:\")\n",
    "reshaped_test = reshape_signal_to_overlapping_windows(\n",
    "    signal = test,\n",
    "    target_frequency = 1,\n",
    "    nn_signal_duration_seconds = 16,\n",
    "    number_windows = 12,\n",
    "    window_duration_seconds = 5,\n",
    "    overlap_seconds = 4,\n",
    "    signal_type = \"feature\"\n",
    "    )\n",
    "print(reshaped_test)\n",
    "\n",
    "print(\"\\nLast window when padding was cropped:\")\n",
    "cropped_padding = remove_padding_from_windows(\n",
    "    signal_in_windows = copy.deepcopy(reshaped_test), # type: ignore\n",
    "    target_frequency = 1,\n",
    "    original_signal_length = 10,\n",
    "    window_duration_seconds = 5, \n",
    "    overlap_seconds = 4,\n",
    "    )\n",
    "print(cropped_padding[-1])\n",
    "\n",
    "print(\"\\nSignal reshaped back to original:\")\n",
    "reversed_test = reverse_signal_to_windows_reshape(\n",
    "    signal_in_windows = reshaped_test, # type: ignore\n",
    "    target_frequency = 1,\n",
    "    original_signal_length = 10,\n",
    "    number_windows = 12,\n",
    "    window_duration_seconds = 5,\n",
    "    overlap_seconds = 4\n",
    "    )\n",
    "print(reversed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sleep stage labels were reshaped differently, as we only keep one label for each window and therefore won't\n",
    "create a 2d array. \n",
    "\n",
    "After predicting the sleep stage labels, we will transform them into a 2d array, that is computable by our \n",
    "reverse reshape function. Effectively, we will create an array from each label, containing only the label as\n",
    "elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original signal:\n",
      "[1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
      "\n",
      "Signal reshaped to overlapping windows:\n",
      "[1 1 1 2 2 2 3 3 3]\n",
      "\n",
      "Expanded signal:\n",
      "[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [2, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2], [3, 3, 3, 3], [3, 3, 3, 3], [3, 3, 3, 3]]\n",
      "\n",
      "Expanded signal reshaped to original:\n",
      "[1.   1.   1.   1.25 1.5  1.75 2.25 2.5  2.75 3.   3.   3.  ]\n",
      "[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original signal:\")\n",
    "test = [1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
    "print(test)\n",
    "reshaped_test = reshape_signal_to_overlapping_windows(\n",
    "    signal = test,\n",
    "    target_frequency = 1/3,\n",
    "    nn_signal_duration_seconds = 36,\n",
    "    number_windows = 9,\n",
    "    window_duration_seconds = 12,\n",
    "    overlap_seconds = 9,\n",
    "    signal_type = \"target\"\n",
    "    )\n",
    "\n",
    "print(\"\\nSignal reshaped to overlapping windows:\")\n",
    "print(reshaped_test)\n",
    "\n",
    "expanded_reshaped_test = []\n",
    "for slp_stg in reshaped_test:\n",
    "    expanded_reshaped_test.append([slp_stg for _ in range(int(12 * 1/3))])\n",
    "\n",
    "print(\"\\nExpanded signal:\")\n",
    "print(expanded_reshaped_test)\n",
    "\n",
    "reversed_test = reverse_signal_to_windows_reshape(\n",
    "    signal_in_windows = expanded_reshaped_test, # type: ignore\n",
    "    target_frequency = 1/3, # type: ignore\n",
    "    original_signal_length = 12,\n",
    "    number_windows = 9,\n",
    "    window_duration_seconds = 12,\n",
    "    overlap_seconds = 9\n",
    "    )\n",
    "\n",
    "print(\"\\nExpanded signal reshaped to original:\")\n",
    "print(reversed_test)\n",
    "print([round(i) for i in reversed_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Signal\n",
    "\n",
    "The implemented unity normalization function can either normalize a multi-dimensional array across all\n",
    "arrays (global) or normalize each array indivudally (local)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dimensional = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "two_dimensional = np.array([[0, 2, 4], [4, 5, 6], [6, 8, 10]])\n",
    "three_dimensional = np.array([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [8, 9, 10]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization_Mode: 'global'\n",
      "----------------------------\n",
      "\n",
      "Normalized One dimensional array:\n",
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
      "\n",
      "Normalized Two dimensional array:\n",
      "[[0.  0.2 0.4]\n",
      " [0.4 0.5 0.6]\n",
      " [0.6 0.8 1. ]]\n",
      "\n",
      "Normalized Three dimensional array:\n",
      "[[[0.  0.1 0.2]\n",
      "  [0.3 0.4 0.5]]\n",
      "\n",
      " [[0.6 0.7 0.8]\n",
      "  [0.8 0.9 1. ]]]\n"
     ]
    }
   ],
   "source": [
    "message = \"Normalization_Mode: \\'global\\'\"\n",
    "print(message)\n",
    "print(\"-\"*len(message))\n",
    "print(\"\\nNormalized One dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = one_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"global\"\n",
    "    ))\n",
    "print(\"\\nNormalized Two dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = two_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"global\"\n",
    "    ))\n",
    "print(\"\\nNormalized Three dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = three_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"global\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization_Mode: 'local'\n",
      "---------------------------\n",
      "\n",
      "Normalized One dimensional array:\n",
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
      "\n",
      "Normalized Two dimensional array:\n",
      "[[0.  0.5 1. ]\n",
      " [0.  0.5 1. ]\n",
      " [0.  0.5 1. ]]\n",
      "\n",
      "Normalized Three dimensional array:\n",
      "[[[0.  0.5 1. ]\n",
      "  [0.  0.5 1. ]]\n",
      "\n",
      " [[0.  0.5 1. ]\n",
      "  [0.  0.5 1. ]]]\n"
     ]
    }
   ],
   "source": [
    "message = \"Normalization_Mode: \\'local\\'\"\n",
    "print(message)\n",
    "print(\"-\"*len(message))\n",
    "print(\"\\nNormalized One dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = one_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"local\"\n",
    "    ))\n",
    "print(\"\\nNormalized Two dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = two_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"local\"\n",
    "    ))\n",
    "print(\"\\nNormalized Three dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = three_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"local\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alter Sleep Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function makes sure to keep labels unfiform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2 -1  0  1  2  3  4  5  6  7]\n",
      "[-1 -1  0  0  1  2 -1  3 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "slp = np.array([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7])\n",
    "print(slp)\n",
    "\n",
    "current_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "desired_labels = {\"wake\": 0, \"LS\": 1, \"DS\": 2, \"REM\": 3, \"artifect\": -1}\n",
    "\n",
    "print(alter_slp_labels(\n",
    "        slp_labels = slp, # type: ignore\n",
    "        current_labels = current_labels,\n",
    "        desired_labels = desired_labels,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['light_sleep' 'deep_sleep' 'deep_sleep_2' 'WAKE' 'REM' 'bla' 'blub']\n",
      "['1' '2' '2' '0' '3' '-1' '-1']\n"
     ]
    }
   ],
   "source": [
    "slp = np.array([\"light_sleep\", \"deep_sleep\", \"deep_sleep_2\", \"WAKE\", \"REM\", \"bla\", \"blub\"])\n",
    "print(slp)\n",
    "\n",
    "current_labels = {\"wake\": [\"WAKE\"], \"LS\": [\"light_sleep\"], \"DS\": [\"deep_sleep\", \"deep_sleep_2\"], \"REM\": [\"REM\"], \"artifect\": [\"other\"]}\n",
    "desired_labels = {\"wake\": 0, \"LS\": 1, \"DS\": 2, \"REM\": 3, \"artifect\": -1}\n",
    "\n",
    "print(alter_slp_labels(\n",
    "        slp_labels = slp, # type: ignore\n",
    "        current_labels = current_labels,\n",
    "        desired_labels = desired_labels,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Transformation from previous (not mine) Sleep Stage Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2 -1  0  1  2  3  4  5  6  7]\n",
      "[-2  0  0  0  1  2  3  3  0  6]\n"
     ]
    }
   ],
   "source": [
    "slp = np.array([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7])\n",
    "print(slp)\n",
    "\n",
    "slp[slp>=1] = slp[slp>=1] - 1\n",
    "slp[slp==4] = 3\n",
    "slp[slp==5] = 0\n",
    "slp[slp==-1] = 0 # set artifact as wake stage\n",
    "\n",
    "print(slp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
