{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Johannes Peter Knoll\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Within this notebook you will learn and test everything that was implemented to preprocess the data\n",
    "for the neural network.\n",
    "\n",
    "Note:   This notebook is rather for those who want to make sure everything works correctly. It is very thorough\n",
    "        and therefore unnecessary if you only want to get a quick start into the predictions. If that is the case, head\n",
    "        to 'Classification_Demo.ipynb'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thorough Demonstration of 'dataset_processing.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The autoreload extension allows you to tweak the code in the imported modules\n",
    "# and rerun cells to reflect the changes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we demonstrate the implemented class that helps you to manage the data you want to pass to the\n",
    "neural network model. \\\n",
    "Its main purpose is to store the data in a uniform way, distribute it into pids and make it\n",
    "easily accessible in a memory efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_processing import *\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "\n",
    "def clean_and_remove_directory(directory):\n",
    "    \"\"\"\n",
    "    Cleans and removes the specified directory if it exists.\n",
    "    \"\"\"\n",
    "    entries = os.listdir(directory)\n",
    "    for entry in entries:\n",
    "        if os.path.isdir(os.path.join(directory, entry)):\n",
    "            clean_and_remove_directory(os.path.join(directory, entry))\n",
    "        else:\n",
    "            os.remove(os.path.join(directory, entry))\n",
    "    os.rmdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Database' is a directory that consists of multiple .pkl files—one for the database configuration, the others\n",
    "for storing data into one or multiple pids.\n",
    "\n",
    "Each datapoint is saved as dictionary and can contain the following keys:\n",
    "- unique identifier (key: \"ID\")\n",
    "- RRI signal (key: \"RRI\")\n",
    "- MAD signal (key: \"MAD\")\n",
    "- SLP signal (key: \"SLP\")\n",
    "- predicted Sleep-Labels (key: \"SLP_predicted\")\n",
    "- predicted individual probabilities for every sleep stage (key: \"SLP_predicted_probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initializing a new database (calling 'SleepDataManager' on non-existent path) the class will\n",
    "automatically create a directory containing the default database configuration saved as .pkl file. \\\n",
    "When initializing on an existing path, the class accesses the database configuration from the existing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRI_frequency: 4\n",
      "MAD_frequency: 1\n",
      "SLP_frequency: 0.03333333333333333\n",
      "sleep_stage_label: None\n",
      "signal_length_seconds: None\n",
      "wanted_shift_length_seconds: None\n",
      "absolute_shift_deviation_seconds: None\n",
      "number_datapoints: [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\")\n",
    "database_configuration = data_manager.database_configuration\n",
    "\n",
    "for key in database_configuration.keys():\n",
    "    print(f\"{key}: {database_configuration[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't mind all the parameters yet. Necessary ones will be explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Database Configuration\n",
    "\n",
    "Of the above parameters, only the uniform signal frequencies can be changed, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database configuration in new instance on same path:\n",
      "\n",
      "RRI_frequency: 2\n",
      "MAD_frequency: 1\n",
      "SLP_frequency: 2\n",
      "sleep_stage_label: None\n",
      "signal_length_seconds: None\n",
      "wanted_shift_length_seconds: None\n",
      "absolute_shift_deviation_seconds: None\n",
      "number_datapoints: [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "updated_frequencies = {\"RRI_frequency\": 2, \"SLP_frequency\": 2}\n",
    "data_manager.change_uniform_frequencies(updated_frequencies)\n",
    "\n",
    "del data_manager, database_configuration\n",
    "\n",
    "# the change is saved globally:\n",
    "another_data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\")\n",
    "database_configuration = another_data_manager.database_configuration\n",
    "\n",
    "print(\"\\nDatabase configuration in new instance on same path:\\n\")\n",
    "for key in database_configuration.keys():\n",
    "    print(f\"{key}: {database_configuration[key]}\")\n",
    "\n",
    "clean_and_remove_directory(\"Processing_Demonstration/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data\n",
    "\n",
    "To ensure the data is uniform you must always provide the sampling frequency for each signal when saving \n",
    "(keys: \"RRI_frequency\", \"MAD_frequency\", \"SLP_frequency\"). \\\n",
    "Furthermore, when adding SLP signals, you need to provide the key: \"sleep_stage_label\" which is a dictionary\n",
    "that is supposed to tell what sleep stage your label correspond to. (Example below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleep stage labels in shhs dataset:\n",
    "# \"wake\": 0,    \"N1\": 1,    \"N2\": 2,    \"N3\": 3,    \"REM\": 5,   \"artifact\": \"other integers\"\n",
    "\n",
    "# in the nn we only divide between wake, LS, DS, REM, and artifact. Above, N1 must be redeclared as \"wake\", \n",
    "# N2 as \"LS\" and N3 as \"DS\":\n",
    "shhs_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifact\": [\"other\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some data with differing sampling frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First datapoints of RRI signal: [4. 5. 1. 4. 5. 1. 3. 1. 3. 5.] (shape: (360,))\n",
      "First datapoints of SLP signal: [4, 4, 5, 3, 2, 1] (shape: 6)\n"
     ]
    }
   ],
   "source": [
    "signal_time_in_seconds = 120\n",
    "rri_frequency = 3 # instead of default: 4 Hz\n",
    "slp_frequency = 1/20 # instead of default: 1/30 Hz\n",
    "\n",
    "# creating signals and printing manually scaled versions\n",
    "rri_signal = np.array([random.randint(1, 5) for i in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "print(f\"First datapoints of RRI signal: {rri_signal[:10]} (shape: {rri_signal.shape})\")\n",
    "slp_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "print(f\"First datapoints of SLP signal: {slp_signal[:10]} (shape: {len(slp_signal)})\")\n",
    "\n",
    "random_sleep_stage_labels = {\"wake\": [0], \"LS\": [1], \"DS\": [2], \"REM\": [3], \"artifact\": [\"other\"]}\n",
    "\n",
    "new_datapoint = {\n",
    "    \"ID\": \"any\",\n",
    "    \"RRI\": rri_signal,\n",
    "    \"RRI_frequency\": rri_frequency,\n",
    "    \"SLP\": slp_signal,\n",
    "    \"SLP_frequency\": slp_frequency,\n",
    "    \"sleep_stage_label\": random_sleep_stage_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the data and investigate the scaling applied to the signals.\n",
    "(Note: Saving a datapoint with an already existing ID overwrites the \"old\" values. You are notified in this case.)\n",
    "\n",
    "The idea is to assign a time stamp to each datapoint in the original and the (new, still unexisting) scaled signal. (index within signal / sampling frequency -> recording time (in seconds))\n",
    "Then, for signals like RRI and MAD, containing continous values, to calculate a scaled datapoint, we just interpolate its value from the\n",
    "two original datapoints its corresponding time stamp lies inbetween.\n",
    "For signals like SLP, containing classification labels, we just take the value of the original datapoint with the closest time stamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 'any' already exists in the data file. Existing keys will be overwritten with new values.\n"
     ]
    }
   ],
   "source": [
    "# initialize database\n",
    "data_manager = SleepDataManager(directory_path = \"Processing_Demonstration/\")\n",
    "\n",
    "# saving the new datapoint\n",
    "data_manager.save(copy.deepcopy(new_datapoint))\n",
    "\n",
    "# overwriting old datapoint (with same values for demonstration)\n",
    "data_manager.save(copy.deepcopy(new_datapoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: any\n",
      "RRI: [4.   4.75 3.   1.75 4.   4.75 3.   1.5  3.   1.5 ] (480,)\n",
      "SLP: [4 4 3 2] (4,)\n"
     ]
    }
   ],
   "source": [
    "# load and print the data\n",
    "data_dict = data_manager.load(0)\n",
    "\n",
    "for key in data_dict.keys(): # type: ignore\n",
    "    if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "        print(key + \":\", data_dict[key][:10], data_dict[key].shape) # type: ignore\n",
    "    else:\n",
    "        print(key + \":\", data_dict[key]) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed up data saving\n",
    "\n",
    "As indicated above, every ID in the database will be checked when saving a new datapoint, leading to unnecessary\n",
    "computation time when saving many datapoints. To speed up saving, it is recommended to check if all ID's you are\n",
    "about to save beforehand and then disable the ID checking (with setting: 'unique_id=True')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All IDs are unique.\n"
     ]
    }
   ],
   "source": [
    "# ID's of new datapoints:\n",
    "list_of_ids = [\"1\", \"two\", \"11\"]\n",
    "\n",
    "# check if IDs are unique (raises an error if not)\n",
    "data_manager.check_if_ids_are_unique(list_of_ids)\n",
    "\n",
    "# save new datapoints without checking for uniqueness\n",
    "for id in list_of_ids:\n",
    "    new_datapoint[\"ID\"] = id\n",
    "    data_manager.save(copy.deepcopy(new_datapoint), unique_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Data can be loaded in multiple ways using a string or an integer:\n",
    "- If it's an integer, it will treat it as position in the database and return the whole data dictionary.\n",
    "- If it's a string that equals a key in the data dictionaries, it will return all entities of that specific key in the database.\n",
    "- If it's a different string, then it will treat it as an ID and look for a match. Equal to index, it will return\n",
    "the whole dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: two\n",
      "RRI: (480,)\n",
      "SLP: (4,)\n"
     ]
    }
   ],
   "source": [
    "# load data by index\n",
    "loaded_data = data_manager.load(2) # or data_manager[2]\n",
    "\n",
    "for key in loaded_data.keys(): # type: ignore\n",
    "    if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "        print(key + \":\", loaded_data[key].shape) # type: ignore\n",
    "    else:\n",
    "        print(key + \":\", loaded_data[key]) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1\n",
      "RRI: (480,)\n",
      "SLP: (4,)\n"
     ]
    }
   ],
   "source": [
    "# load data by ID\n",
    "loaded_data = data_manager.load(\"1\") # or data_manager[\"1\"]\n",
    "\n",
    "for key in loaded_data.keys(): # type: ignore\n",
    "    if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "        print(key + \":\", loaded_data[key].shape) # type: ignore\n",
    "    else:\n",
    "        print(key + \":\", loaded_data[key]) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2. , 2. , 2. , ..., 1.5, 1. , 2. ]), array([1. , 2. , 2. , ..., 1.5, 2. , 2. ]), array([1. , 1.5, 2. , ..., 2. , 2. , 2. ])]\n"
     ]
    }
   ],
   "source": [
    "loaded_data = some_data_manager.load(\"RRI\")\n",
    "# loaded_data = some_data_manager[\"RRI\"] # same as above\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Data\n",
    "\n",
    "Removing takes the same argument as loading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting a signal from all entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "ID 4\n",
      "shift_length_seconds 3780\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "SLP_predicted (1200,)\n",
      "SLP_predicted_probability (1200,)\n",
      "--------------------\n",
      "ID 4_shift_x1\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "--------------------\n",
      "ID 4_shift_x2\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "some_data_manager.remove(\"RRI\")\n",
    "\n",
    "# print all data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*20)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "            print(key, dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting an entry by ID (If a signal was splitted and one of the ID's is being removed, all other will be \n",
    "removed as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some data\n",
    "new_datapoint_2 = copy.deepcopy(new_datapoint)\n",
    "new_datapoint_2[\"ID\"] = \"5\"\n",
    "some_data_manager.save(new_datapoint_2, overwrite_id=True, unique_id=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "ID 5\n",
      "shift_length_seconds 3780\n",
      "RRI (144000,)\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "--------------------\n",
      "ID 5_shift_x1\n",
      "RRI (144000,)\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "--------------------\n",
      "ID 5_shift_x2\n",
      "RRI (144000,)\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "some_data_manager.remove(\"4_shift_x1\")\n",
    "\n",
    "# print all data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*20)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "            print(key, dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing by index works analogous to removing by ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some data\n",
    "new_datapoint_2 = copy.deepcopy(new_datapoint)\n",
    "new_datapoint_2[\"ID\"] = \"6\"\n",
    "some_data_manager.save(new_datapoint_2, overwrite_id=True, unique_id=False)\n",
    "del new_datapoint_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "ID 6\n",
      "shift_length_seconds 3780\n",
      "RRI (144000,)\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "------------------------------\n",
      "ID 6_shift_x1\n",
      "RRI (144000,)\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "------------------------------\n",
      "ID 6_shift_x2\n",
      "RRI (144000,)\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "some_data_manager.remove(0)\n",
    "\n",
    "# print all data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*30)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "            print(key, dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's restore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data_manager.remove(0)\n",
    "some_data_manager.save(copy.deepcopy(new_datapoint), overwrite_id=True, unique_id=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over Database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4_shift_x1\n",
      "4_shift_x2\n"
     ]
    }
   ],
   "source": [
    "for datapoint in some_data_manager:\n",
    "    print(datapoint[\"ID\"])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if datapoint with certain ID is in database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoint with \"ID\" = 4 is in the data manager\n"
     ]
    }
   ],
   "source": [
    "if \"4\" in some_data_manager:\n",
    "    print(\"Datapoint with \\\"ID\\\" = 4 is in the data manager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: Processing_Demonstration/messing_around_1.pkl\n",
      "file_info: {'RRI_frequency': 4, 'MAD_frequency': 1, 'SLP_frequency': 0.03333333333333333, 'SLP_predicted_frequency': 0.03333333333333333, 'RRI_inlier_interval': [0.3, 2.0], 'MAD_inlier_interval': [None, None], 'sleep_stage_label': {'wake': '0', 'LS': '1', 'DS': '2', 'REM': '3', 'artifect': '0'}, 'signal_length_seconds': 36000, 'wanted_shift_length_seconds': 5400, 'absolute_shift_deviation_seconds': 1800, 'signal_split_reversed': False, 'train_val_test_split_applied': False, 'main_file_path': 'Processing_Demonstration/messing_around_1.pkl', 'train_file_path': 'Processing_Demonstration/messing_around_1_training_pid.pkl', 'validation_file_path': 'Processing_Demonstration/messing_around_1_validation_pid.pkl', 'test_file_path': 'Processing_Demonstration/messing_around_1_test_pid.pkl'}\n"
     ]
    }
   ],
   "source": [
    "print(some_data_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-, Validation-, Test- Split\n",
    "\n",
    "Of course, we aim to train a machine learning model with the data handled by this class. So, we want to\n",
    "be able to separate the data into training-, validation- and test- pids.\n",
    "\n",
    "First, let's create a new file and add some more data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints in file: 100\n"
     ]
    }
   ],
   "source": [
    "many_files_data_manager = SleepDataManager(file_path = \"Processing_Demonstration/whole_night_distribution.pkl\")\n",
    "\n",
    "add_number_datapoints = 100\n",
    "\n",
    "# optimal signal (fitting sampling frequencies and length):\n",
    "signal_time_in_seconds = 10 * 3600\n",
    "rri_frequency = 4\n",
    "mad_frequency = 1\n",
    "slp_frequency = 1/30\n",
    "\n",
    "random_sleep_stage_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "\n",
    "for i in range(add_number_datapoints):\n",
    "    rri_signal = np.array([random.randint(1, 5) for i in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "    mad_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * mad_frequency))]\n",
    "    slp_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "\n",
    "    decide_what_data_to_add = random.randint(0, 2)\n",
    "\n",
    "    if decide_what_data_to_add == 0:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"MAD\": mad_signal,\n",
    "            \"MAD_frequency\": mad_frequency,\n",
    "            \"SLP\": slp_signal,\n",
    "            \"SLP_frequency\": slp_frequency,\n",
    "            \"sleep_stage_label\": random_sleep_stage_labels\n",
    "        } # optimal data (rri and mad to slp)\n",
    "    elif decide_what_data_to_add == 1:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"MAD\": mad_signal,\n",
    "            \"MAD_frequency\": mad_frequency,\n",
    "        } # invalid data (no target: slp)\n",
    "    else:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"SLP\": slp_signal,\n",
    "            \"SLP_frequency\": slp_frequency,\n",
    "            \"sleep_stage_label\": random_sleep_stage_labels\n",
    "        } # only rri to slp\n",
    "    \n",
    "    many_files_data_manager.save(new_datapoint, overwrite_id=False)\n",
    "\n",
    "print(f\"Number of datapoints in file: {len(many_files_data_manager)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on whether `test_size` is provided or `None` we can create separate files where training-, validation- and \n",
    "test- data or just training- and validation data is stored. Furthermore, the boolean parameters `join_splitted_parts`\n",
    "and `stratify` control whether all database entries resulting from splitting the originially saved datapoint\n",
    "(due to overlength) should be included in the same pid and if a stratification array should be used during\n",
    "the distribution into train-, validation- and test pid.\n",
    "\n",
    "Data that can not be used to train the network (i.e. missing \"RRI\" and \"SLP\") will be left in the main file. \n",
    "        \n",
    "As we can manage data with \"RRI\" and \"MAD\" and data with \"RRI\" only, the algorithm makes sure\n",
    "that only one of the two types of data is used (the one with more samples). The other type will \n",
    "be left in the main file. This must be done to ensure each batch contains the same data.\n",
    "To also train with the type of data left behind, we must save these datapoints to another file from the begin\n",
    "with and train the network in separate steps on the individual datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: 36 datapoints do not contain a SLP and/or RRI signal and will be left in the main file.\n",
      "Attention: 30 datapoints with MAD signal will be left in the main file.\n",
      "\n",
      "Distributing 80.0% / 10.0% / 10.0% of datapoints into training / validation / test pids, respectively:\n",
      "   ✅: 100.0% [█████████████████████] 100 / 100 | 58 ms / 58 ms (579 µs/it) |\n"
     ]
    }
   ],
   "source": [
    "many_files_data_manager.separate_train_test_validation(\n",
    "    train_size = 0.8, \n",
    "    validation_size = 0.1, \n",
    "    test_size = 0.1,\n",
    "    random_state = None,\n",
    "    shuffle = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual files can be accessed by another instance of this class. \n",
    "\n",
    "ATTENTION:  \n",
    "\n",
    "-   The instances on all files will have reduced functionality from now on. As the data should\n",
    "    be fully prepared for the network now, the instances are designed to only load data and\n",
    "    not save or edit it.\n",
    "\n",
    "-   The functionality of the instance on the main file is not as restricted as the ones on the\n",
    "    training, validation, and test files. The main file instance can additionally save data\n",
    "    (only to main file, won't be forwarded to training, validation, or test files), reshuffle \n",
    "    the data in the secondary files or pull them back into the main file for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing training-, validation- and test- data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each dataset:\n",
      "------------------------------\n",
      "Main: 66\n",
      "Train: 27\n",
      "Validation: 3\n",
      "Test: 4\n"
     ]
    }
   ],
   "source": [
    "main_file_info = many_files_data_manager.file_info\n",
    "\n",
    "train_data_manager = SleepDataManager(file_path = main_file_info[\"train_file_path\"])\n",
    "validation_data_manager = SleepDataManager(file_path = main_file_info[\"validation_file_path\"])\n",
    "test_data_manager = SleepDataManager(file_path = main_file_info[\"test_file_path\"])\n",
    "\n",
    "print(\"Length of each dataset:\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Main: {len(many_files_data_manager)}\")\n",
    "print(f\"Train: {len(train_data_manager)}\")\n",
    "print(f\"Validation: {len(validation_data_manager)}\")\n",
    "print(f\"Test: {len(test_data_manager)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main file info is the same as train, validation and test file info!\n"
     ]
    }
   ],
   "source": [
    "train_file_info = train_data_manager.file_info\n",
    "validation_file_info = validation_data_manager.file_info\n",
    "test_file_info = test_data_manager.file_info\n",
    "\n",
    "equal = True\n",
    "for key in main_file_info.keys():\n",
    "    if main_file_info[key] != train_file_info[key] or main_file_info[key] != validation_file_info[key] or main_file_info[key] != test_file_info[key]:\n",
    "        equal = False\n",
    "        break\n",
    "\n",
    "if equal:\n",
    "    print(\"Main file info is the same as train, validation and test file info!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the general functionality was demonstrated, we will show different parameter configurations (`join_splitted_parts` and `stratify`)\n",
    "and elaborate their intended use case.\n",
    "\n",
    "In the case where we train a neural network on 'whole nights' of data (10 hours) at once there is not much use\n",
    "for stratification. To enable it we would need to assign only one sleep stage to each of these (10 hour) datapoints\n",
    "which usually span thousands of sleep stages, making the majority sleep stage quite unrepresentative, as it will\n",
    "probably be the same for all datapoints.\n",
    "\n",
    "Whether joining all parts is useful or not depends on your splitting settings. If the original datapoints are only\n",
    "a bit longer than 10 hours, or the overlapping parts quite long in general, then training on both might not be\n",
    "very useful for the network, as this datapoint will have more impact on weight adjustments. On the other hand,\n",
    "if it ends up in the validation pids, then the network would predict a datapoint it has kind of already seen, leading\n",
    "to a less unrepresentative test error. As both options have their drawbacks, and forcing the splitted parts to be\n",
    "joined takes more computation time, leaving `join_splitted_parts` at `False` might be the better option. Here,\n",
    "each datapoint in the database (already splitted) is distributed randomly, meaning in some cases the splitted parts will\n",
    "end up in the same pid, and sometimes not.\n",
    "\n",
    "Calling the function again (this time without assigning datapoints to the test pid) reshuffles the data within\n",
    "the pids independently of previous distribution settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: 30 datapoints do not contain a SLP and/or RRI signal and will be left in the main file.\n",
      "Attention: 32 datapoints with MAD signal will be left in the main file.\n",
      "\n",
      "Distributing 80.0% / 20.0% of datapoints into training / validation pids, respectively:\n",
      "   ✅: 100.0% [█████████████████████] 100 / 100 | 43 ms / 43 ms (430 µs/it) |\n"
     ]
    }
   ],
   "source": [
    "many_files_data_manager.separate_train_test_validation(\n",
    "    train_size = 0.8, \n",
    "    validation_size = 0.2, \n",
    "    test_size = None,\n",
    "    random_state = None,\n",
    "    shuffle = True,\n",
    "    join_splitted_parts = False,\n",
    "    stratify = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each dataset:\n",
      "------------------------------\n",
      "Main: 62\n",
      "Train: 30\n",
      "Validation: 8\n",
      "No test data manager\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of each dataset:\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Main: {len(many_files_data_manager)}\")\n",
    "print(f\"Train: {len(train_data_manager)}\")\n",
    "print(f\"Validation: {len(validation_data_manager)}\")\n",
    "try:\n",
    "    print(f\"Test: {len(test_data_manager)}\")\n",
    "except:\n",
    "    print(\"No test data manager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fuse the data again (do not forget to close your active data managers!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "many_files_data_manager.fuse_train_test_validation()\n",
    "\n",
    "del train_data_manager\n",
    "del validation_data_manager\n",
    "del test_data_manager\n",
    "\n",
    "print(len(many_files_data_manager))\n",
    "del many_files_data_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets consider the case where we want to train our network only on small time-periods of data at once (enabling\n",
    "it to perform real-time predictions). In this case, every datapoint will likely be splitted into thousands of parts,\n",
    "without overlap (if it represented a whole night recording).\n",
    "\n",
    "In this case, it can be highly effective to assign all splitted parts of the original datapoint to the same pid, \n",
    "automatically ensuring that each part of the night is represented with the same amount.\n",
    "\n",
    "In the following example, we start originally with 10 hour recordings, and split them into 5 parts (unuseful in practice,\n",
    "but better for visualizing the effect at the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints in file: 50\n"
     ]
    }
   ],
   "source": [
    "many_files_data_manager = SleepDataManager(file_path = \"Processing_Demonstration/time_period_distribution.pkl\")\n",
    "\n",
    "new_file_info = {\"signal_length_seconds\": 7200, \"wanted_shift_length_seconds\": 7200, \"absolute_shift_deviation_seconds\": 100}\n",
    "many_files_data_manager.change_file_information(new_file_info)\n",
    "\n",
    "add_number_datapoints = 10\n",
    "\n",
    "# optimal signal (fitting sampling frequencies and length):\n",
    "signal_time_in_seconds = 10 * 3600\n",
    "rri_frequency = 4\n",
    "mad_frequency = 1\n",
    "slp_frequency = 1/30\n",
    "\n",
    "random_sleep_stage_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "\n",
    "for i in range(add_number_datapoints):\n",
    "    rri_signal = np.array([random.randint(1, 5) for i in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "    mad_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * mad_frequency))]\n",
    "    slp_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "\n",
    "    decide_what_data_to_add = random.randint(0, 2)\n",
    "\n",
    "    if decide_what_data_to_add == 0:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"MAD\": mad_signal,\n",
    "            \"MAD_frequency\": mad_frequency,\n",
    "            \"SLP\": slp_signal,\n",
    "            \"SLP_frequency\": slp_frequency,\n",
    "            \"sleep_stage_label\": random_sleep_stage_labels\n",
    "        } # optimal data (rri and mad to slp)\n",
    "    elif decide_what_data_to_add == 1:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"MAD\": mad_signal,\n",
    "            \"MAD_frequency\": mad_frequency,\n",
    "        } # invalid data (no target: slp)\n",
    "    else:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"SLP\": slp_signal,\n",
    "            \"SLP_frequency\": slp_frequency,\n",
    "            \"sleep_stage_label\": random_sleep_stage_labels\n",
    "        } # only rri to slp\n",
    "    \n",
    "    many_files_data_manager.save(new_datapoint, overwrite_id=False)\n",
    "\n",
    "print(f\"Number of datapoints in file: {len(many_files_data_manager)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: 15 datapoints do not contain a SLP and/or RRI signal and will be left in the main file.\n",
      "Attention: 10 datapoints with MAD signal will be left in the main file.\n",
      "\n",
      "Distributing 80.0% / 10.0% / 10.0% of datapoints into training / validation / test pids, respectively:\n",
      "   ✅: 100.0% [███████████████████████] 50 / 50 | 11 ms / 11 ms (206 µs/it) |\n"
     ]
    }
   ],
   "source": [
    "many_files_data_manager.separate_train_test_validation(\n",
    "    train_size = 0.8, \n",
    "    validation_size = 0.1, \n",
    "    test_size = 0.1,\n",
    "    random_state = None,\n",
    "    shuffle = True,\n",
    "    join_splitted_parts = True,\n",
    "    stratify = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each dataset:\n",
      "------------------------------\n",
      "Main: 25\n",
      "Train: 15\n",
      "Validation: 5\n",
      "Test: 5\n",
      "\n",
      "Id's in each dataset:\n",
      "------------------------------\n",
      "Train: ['2', '2_shift_x1', '2_shift_x2', '2_shift_x3', '2_shift_x4', '4', '4_shift_x1', '4_shift_x2', '4_shift_x3', '4_shift_x4', '8', '8_shift_x1', '8_shift_x2', '8_shift_x3', '8_shift_x4']\n",
      "Validation: ['1', '1_shift_x1', '1_shift_x2', '1_shift_x3', '1_shift_x4']\n",
      "Test: ['5', '5_shift_x1', '5_shift_x2', '5_shift_x3', '5_shift_x4']\n"
     ]
    }
   ],
   "source": [
    "main_file_info = many_files_data_manager.file_info\n",
    "\n",
    "train_data_manager = SleepDataManager(file_path = main_file_info[\"train_file_path\"])\n",
    "validation_data_manager = SleepDataManager(file_path = main_file_info[\"validation_file_path\"])\n",
    "test_data_manager = SleepDataManager(file_path = main_file_info[\"test_file_path\"])\n",
    "\n",
    "print(\"Length of each dataset:\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Main: {len(many_files_data_manager)}\")\n",
    "print(f\"Train: {len(train_data_manager)}\")\n",
    "print(f\"Validation: {len(validation_data_manager)}\")\n",
    "print(f\"Test: {len(test_data_manager)}\")\n",
    "\n",
    "print(\"\\nId's in each dataset:\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Train: {train_data_manager[\"ID\"]}\")\n",
    "print(f\"Validation: {validation_data_manager[\"ID\"]}\")\n",
    "print(f\"Test: {test_data_manager[\"ID\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should, however, you only have small time periods of data originally, then it of course makes sense to leave the\n",
    "distribution random (as their might not be many splitted parts anyway). To ensure we still distribute each sleep stage\n",
    "usefully, in this case, we need to activate the stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention: 15 datapoints do not contain a SLP and/or RRI signal and will be left in the main file.\n",
      "Attention: 10 datapoints with MAD signal will be left in the main file.\n",
      "WARNING: To enable stratification, every datapoint can only be represented by one sleep stage. Each of your datapoints currently contains 240 sleep stages. As usual, the most common sleep stage will be used to represent the datapoint. However, due to the number of sleep stages, this may not be representative of the actual sleep stage distribution in your data.\n",
      "\n",
      "Distributing 80.0% / 10.0% / 10.0% of datapoints into training / validation / test pids, respectively:\n",
      "   ✅: 100.0% [█████████████████████] 50 / 50 | 9.3 ms / 9.3 ms (187 µs/it) |\n"
     ]
    }
   ],
   "source": [
    "many_files_data_manager.separate_train_test_validation(\n",
    "    train_size = 0.8, \n",
    "    validation_size = 0.1, \n",
    "    test_size = 0.1,\n",
    "    random_state = None,\n",
    "    shuffle = True,\n",
    "    join_splitted_parts = False,\n",
    "    stratify = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each dataset:\n",
      "------------------------------\n",
      "Main: 25\n",
      "Train: 20\n",
      "Validation: 2\n",
      "Test: 3\n",
      "\n",
      "Id's in each dataset:\n",
      "------------------------------\n",
      "Train: ['2', '2_shift_x2', '2_shift_x3', '4', '4_shift_x1', '4_shift_x2', '4_shift_x3', '4_shift_x4', '8', '8_shift_x1', '8_shift_x4', '1', '1_shift_x1', '1_shift_x2', '1_shift_x4', '5', '5_shift_x1', '5_shift_x2', '5_shift_x3', '5_shift_x4']\n",
      "Validation: ['2_shift_x1', '1_shift_x3']\n",
      "Test: ['2_shift_x4', '8_shift_x2', '8_shift_x3']\n"
     ]
    }
   ],
   "source": [
    "main_file_info = many_files_data_manager.file_info\n",
    "\n",
    "train_data_manager = SleepDataManager(file_path = main_file_info[\"train_file_path\"])\n",
    "validation_data_manager = SleepDataManager(file_path = main_file_info[\"validation_file_path\"])\n",
    "test_data_manager = SleepDataManager(file_path = main_file_info[\"test_file_path\"])\n",
    "\n",
    "print(\"Length of each dataset:\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Main: {len(many_files_data_manager)}\")\n",
    "print(f\"Train: {len(train_data_manager)}\")\n",
    "print(f\"Validation: {len(validation_data_manager)}\")\n",
    "print(f\"Test: {len(test_data_manager)}\")\n",
    "\n",
    "print(\"\\nId's in each dataset:\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Train: {train_data_manager[\"ID\"]}\")\n",
    "print(f\"Validation: {validation_data_manager[\"ID\"]}\")\n",
    "print(f\"Test: {test_data_manager[\"ID\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversing Signal Split\n",
    "\n",
    "After you added predicted sleep stages to the database, you might want to reverse the signal split that was \n",
    "applied to the data during the saving process:\n",
    "\n",
    "Calling the function will combine all signals, including the predicted sleep stages, providing you with\n",
    "multiple results for the sleep stage of the overlapping parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "ID: 0, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 0_shift_x1, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 0_shift_x2, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 1, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 1_shift_x1, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 1_shift_x2, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 2, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 2_shift_x1, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 2_shift_x2, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 3, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 3_shift_x1, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 3_shift_x2, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 4, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "RRI [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19.]\n",
      "MAD [0 1 2 3 4 5 6 7 8 9]\n",
      "SLP_predicted [39 40 41 42 43]\n",
      "SLP_predicted_probability [[0.76 0.5  0.85 0.31]\n",
      " [0.42 0.7  0.53 0.31]\n",
      " [0.49 0.72 0.98 0.09]\n",
      " [0.23 0.04 0.18 0.31]\n",
      " [0.75 0.04 0.73 0.2 ]]\n",
      "------------------------------------------------------------------------------------------\n",
      "ID: 4_shift_x1, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "RRI [12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29.\n",
      " 30. 31.]\n",
      "MAD [ 6  7  8  9 10 11 12 13 14 15]\n",
      "SLP_predicted [42 43 44 45 46]\n",
      "SLP_predicted_probability [[0.23 0.04 0.18 0.31]\n",
      " [0.75 0.04 0.73 0.2 ]\n",
      " [0.37 0.29 0.   0.74]\n",
      " [0.01 0.06 0.14 0.47]\n",
      " [0.38 0.93 0.22 0.46]]\n",
      "------------------------------------------------------------------------------------------\n",
      "ID: 4_shift_x2, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "RRI [24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41.\n",
      " 42. 43.]\n",
      "MAD [12 13 14 15 16 17 18 19 20 21]\n",
      "SLP_predicted [45 46 47 48 49]\n",
      "SLP_predicted_probability [[0.01 0.06 0.14 0.47]\n",
      " [0.38 0.93 0.22 0.46]\n",
      " [0.61 0.14 0.42 0.99]\n",
      " [0.94 0.19 0.34 0.13]\n",
      " [0.14 0.02 0.83 0.56]]\n",
      "------------------------------------------------------------------------------------------\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# initialize the data manager\n",
    "splitting_data_manager = SleepDataManager(file_path = \"Processing_Demonstration/Reverse_Splitting.pkl\")\n",
    "\n",
    "# change the file information\n",
    "new_file_info = {\"RRI_frequency\": 2, \"MAD_frequency\": 1, \"signal_length_seconds\": 10, \"wanted_shift_length_seconds\": 5, \"absolute_shift_deviation_seconds\": 2, \"SLP_frequency\": 1, \"SLP_predicted_frequency\": 0.5, \"RRI_inlier_interval\": [None, None]}\n",
    "splitting_data_manager.change_file_information(new_file_info)\n",
    "\n",
    "data_dict = {\n",
    "    \"RRI\": np.array([i for i in range(44)], dtype=np.float64),\n",
    "    \"RRI_frequency\": 2,\n",
    "    \"MAD\": np.array([i for i in range(22)], dtype=np.int64),\n",
    "    \"MAD_frequency\": 1,\n",
    "}\n",
    "\n",
    "# add data that will be splitted\n",
    "for i in range(5):\n",
    "    data_dict[\"ID\"] = str(i)\n",
    "    splitting_data_manager.save(data_dict, overwrite_id=False)\n",
    "\n",
    "file_generator = load_from_pickle(\"Processing_Demonstration/Reverse_Splitting.pkl\")\n",
    "next(file_generator)\n",
    "\n",
    "# add \"predicted\" sleep stages\n",
    "count = 1\n",
    "old_slp_pred_prob = np.array([np.round(np.random.rand(4), 2) for _ in range(2)], dtype=np.float64)\n",
    "for file in file_generator:\n",
    "    new_slp_pred_prob = np.array([np.round(np.random.rand(4), 2) for _ in range(3)], dtype=np.float64)\n",
    "    new_slp_pred_prob = np.append(old_slp_pred_prob, new_slp_pred_prob, axis=0)\n",
    "    old_slp_pred_prob = new_slp_pred_prob[3:]\n",
    "    additional_info = {\n",
    "        \"SLP_predicted\": np.array([i for i in range(5)], dtype=np.int64)+3*count,\n",
    "        \"SLP_predicted_probability\": new_slp_pred_prob,\n",
    "        \"SLP_predicted_frequency\": 0.5\n",
    "    }\n",
    "    additional_info[\"ID\"] = file[\"ID\"]\n",
    "    splitting_data_manager.save(additional_info, overwrite_id=True)\n",
    "    count += 1\n",
    "\n",
    "del file_generator\n",
    "\n",
    "# print the data\n",
    "print(\"=\"*90)\n",
    "for dict in splitting_data_manager:\n",
    "    message = f\"ID: {dict['ID']}\"\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "            message += f\", {key}: {dict[key].shape}\"\n",
    "    print(message)\n",
    "\n",
    "    if \"4\" in dict['ID']:\n",
    "        for key in dict.keys():\n",
    "            if key in [\"RRI\", \"MAD\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "                print(key, dict[key])\n",
    "        print(\"-\"*90)\n",
    "\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_shape(list):\n",
    "    shape = \"(\"\n",
    "    while True:\n",
    "        try:\n",
    "            shape += str(len(list))\n",
    "            list = list[0]\n",
    "            shape += \", \"\n",
    "        except:\n",
    "            break\n",
    "    shape += \")\"\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distributing split data parts into individual files (Subprocess of Reversing Signal Split):\n",
      "   ✅: 100.0% [█████████████████████] 15 / 15 | 2.3 ms / 2.3 ms (151 µs/it) |\n",
      "\n",
      "Merging data points back into the main file and reversing the Signal Split:\n",
      "   ✅: 100.0% [███████████████████████] 5 / 5 | 2.3 ms / 2.3 ms (462 µs/it) |\n",
      "------------------------------------------------------------------------------------------\n",
      "ID: 0, RRI: (20,), MAD: (10,), SLP_predicted: (5, ), SLP_predicted_probability: (5, 4)\n",
      "ID: 1, RRI: (20,), MAD: (10,), SLP_predicted: (5, ), SLP_predicted_probability: (5, 4)\n",
      "ID: 2, RRI: (20,), MAD: (10,), SLP_predicted: (5, ), SLP_predicted_probability: (5, 4)\n",
      "ID: 3, RRI: (20,), MAD: (10,), SLP_predicted: (5, ), SLP_predicted_probability: (5, 4)\n",
      "ID: 4, RRI: (20,), MAD: (10,), SLP_predicted: (5, ), SLP_predicted_probability: (5, 4)\n",
      "------------------------------------------------------------------------------------------\n",
      "RRI [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19.]\n",
      "MAD [0 1 2 3 4 5 6 7 8 9]\n",
      "SLP_predicted [39 40 41 42 43]\n",
      "SLP_predicted_probability [[0.76 0.5  0.85 0.31]\n",
      " [0.42 0.7  0.53 0.31]\n",
      " [0.49 0.72 0.98 0.09]\n",
      " [0.23 0.04 0.18 0.31]\n",
      " [0.75 0.04 0.73 0.2 ]]\n"
     ]
    }
   ],
   "source": [
    "# apply the reverse splitting\n",
    "splitting_data_manager.reverse_signal_split()\n",
    "\n",
    "# print the data\n",
    "print(\"-\"*90)\n",
    "for dict in splitting_data_manager:\n",
    "    message = f\"ID: {dict['ID']}\"\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP_predicted_probability\"]:\n",
    "            message += f\", {key}: {dict[key].shape}\"\n",
    "        if key in [\"SLP_predicted\"]:\n",
    "            message += f\", {key}: {list_shape(dict[key])}\"\n",
    "    print(message)\n",
    "print(\"-\"*90)\n",
    "\n",
    "for key in dict.keys():\n",
    "    if key in [\"RRI\", \"MAD\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "        print(key, dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_files = os.listdir(\"Processing_Demonstration\")\n",
    "for file in created_files:\n",
    "    try:\n",
    "        os.remove(f\"Processing_Demonstration/{file}\")\n",
    "    except:\n",
    "        pass\n",
    "os.rmdir(\"Processing_Demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the implemented functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # type: ignore\n",
    "import random\n",
    "import h5py # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you can check whether the implemented functions in this project work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling number of datapoints from signal- to target- frequency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would highly suggest to provide data where the signals don't need to be scaled to the frequencies of the data\n",
    "used to train the neural network.\n",
    "\n",
    "If there is no other option, then so be it. Here is a demonstration of the functions that will be applied to \n",
    "your data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Classification Frequency: 0.05 -> Target Frequency: 0.03333333333333333\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "Classification array:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "Classification array shape:  (15,)\n",
      "\n",
      "Scaled array:  [ 0  1  3  4  6  7  9 10 12 13]\n",
      "Scaled array shape:  (10,)\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Classification Frequency: 0.02 -> Target Frequency: 0.03333333333333333\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "Classification array:  [0 1 2 3 4 5 6 7 8]\n",
      "Classification array shape:  (9,)\n",
      "\n",
      "Scaled array:  [0 1 1 2 2 3 4 4 5 5 6 7 7 8 8]\n",
      "Scaled array shape:  (15,)\n"
     ]
    }
   ],
   "source": [
    "classification_array = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
    "classification_frequency = 1/20\n",
    "target_frequency = 1/30\n",
    "\n",
    "print(\"-\"*71)\n",
    "print(f\"Classification Frequency: {classification_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*71)\n",
    "print(\"\\nClassification array: \", classification_array)\n",
    "print(\"Classification array shape: \", classification_array.shape)\n",
    "\n",
    "reshaped_array = scale_classification_signal(\n",
    "        signal = classification_array, # type: ignore\n",
    "        signal_frequency = classification_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(\"\\nScaled array: \", reshaped_array)\n",
    "print(\"Scaled array shape: \", reshaped_array.shape)\n",
    "\n",
    "classification_array = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "classification_frequency = 1/50\n",
    "target_frequency = 1/30\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-\"*71)\n",
    "print(f\"Classification Frequency: {classification_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*71)\n",
    "print(\"\\nClassification array: \", classification_array)\n",
    "print(\"Classification array shape: \", classification_array.shape)\n",
    "\n",
    "reshaped_array = scale_classification_signal(\n",
    "        signal = classification_array, # type: ignore\n",
    "        signal_frequency = classification_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(\"\\nScaled array: \", reshaped_array)\n",
    "print(\"Scaled array shape: \", reshaped_array.shape)\n",
    "\n",
    "del reshaped_array, classification_array, classification_frequency, target_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Continuous Frequency: 3 -> Target Frequency: 4\n",
      "---------------------------------------------------------------------------\n",
      "Continuous array: [0 1 2 3 4 5] / [0. 1. 2. 3. 4. 5.]\n",
      "Continuous array shape:  (6,)\n",
      "\n",
      "Scaled array: [0 1 2 2 3 4 4 5] / [0.   0.75 1.5  2.25 3.   3.75 4.5  5.  ]\n",
      "Scaled array shape:  (8,)\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Continuous Frequency: 5 -> Target Frequency: 4\n",
      "---------------------------------------------------------------------------\n",
      "Continuous array: [0 1 2 3 4 5 6 7 8 9] / [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "Continuous array shape:  (10,)\n",
      "\n",
      "Scaled array: [0 1 2 4 5 6 8 9] / [0.   1.25 2.5  3.75 5.   6.25 7.5  8.75]\n",
      "Scaled array shape:  (8,)\n"
     ]
    }
   ],
   "source": [
    "continuous_array_int = np.array([0, 1, 2, 3, 4, 5])\n",
    "continuous_array_float = np.array([0, 1, 2, 3, 4, 5], dtype = float)\n",
    "continuous_frequency = 3\n",
    "target_frequency = 4\n",
    "\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous Frequency: {continuous_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous array: {continuous_array_int} / {continuous_array_float}\")\n",
    "print(\"Continuous array shape: \", continuous_array_int.shape)\n",
    "\n",
    "reshaped_array_int = interpolate_signal(\n",
    "        signal = continuous_array_int, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "reshaped_array_float = interpolate_signal(\n",
    "        signal = continuous_array_float, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(f\"\\nScaled array: {reshaped_array_int} / {reshaped_array_float}\")\n",
    "print(\"Scaled array shape: \", reshaped_array_int.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "continuous_array_int = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "continuous_array_float = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype = float)\n",
    "continuous_frequency = 5\n",
    "target_frequency = 4\n",
    "\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous Frequency: {continuous_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous array: {continuous_array_int} / {continuous_array_float}\")\n",
    "print(\"Continuous array shape: \", continuous_array_int.shape)\n",
    "\n",
    "reshaped_array_int = interpolate_signal(\n",
    "        signal = continuous_array_int, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "reshaped_array_float = interpolate_signal(\n",
    "        signal = continuous_array_float, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(f\"\\nScaled array: {reshaped_array_int} / {reshaped_array_float}\")\n",
    "print(\"Scaled array shape: \", reshaped_array_int.shape)\n",
    "\n",
    "del reshaped_array_int, reshaped_array_float, continuous_array_int, continuous_array_float, continuous_frequency, target_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting a signal which is too long for the neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A signal which is too short will be padded with zeros. No big deal. On the other hand: A signal which is too \n",
    "long will be splitted into multiple signals. To create more data, the 10 hour range will be shifted along the\n",
    "signal.\n",
    "\n",
    "This shift should not be too small, to create redundant data but also not too big, because the more data the \n",
    "better. So we try to find a shift size close to 1 hour, which lets us shift an integer amount of times\n",
    "easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding optimal shift size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "Optimal shift length for signal which is 2.5 hours longer than desired length of 10 hours: 0.833 hours\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "signal_length_addition_hours = 2.5\n",
    "desired_length_hours = 10\n",
    "\n",
    "optimal_shift_length = calculate_optimal_shift_length(\n",
    "        signal_length_seconds = (desired_length_hours + signal_length_addition_hours) * 3600, # type: ignore\n",
    "        desired_length_seconds = desired_length_hours*3600, \n",
    "        wanted_shift_length_seconds = 3600,\n",
    "        absolute_shift_deviation_seconds = 1800,\n",
    "        all_signal_frequencies = [4, 1, 1/30, 1/120]\n",
    ")\n",
    "print(optimal_shift_length)\n",
    "\n",
    "print(f\"Optimal shift length for signal which is {signal_length_addition_hours} hours longer than desired length of {desired_length_hours} hours: {round(optimal_shift_length/3600, 3)} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function to find optimal shift length is embedded in the following split funtion. The optimal\n",
    "shift size will be estimated for every signal individually.\n",
    "\n",
    "If there is no integer shift size in range, that lets you shift the signal so, that you perfectly enclose the\n",
    "last datapoints of the long signal, then the last shift will be altered so that it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random signal\n",
    "frequency = 4\n",
    "length_signal_seconds = 12.1 * 3600\n",
    "signal = np.random.rand(int(length_signal_seconds * frequency))\n",
    "\n",
    "# Only important parameters here:\n",
    "nn_signal_seconds = 10 * 3600\n",
    "shift_length_seconds = 3600\n",
    "absolute_shift_deviation_seconds = 1800\n",
    "\n",
    "signals_from_splitting, shift_length = split_long_signal(\n",
    "        signal = signal, # type: ignore\n",
    "        sampling_frequency = frequency,\n",
    "        target_frequency = frequency,\n",
    "        nn_signal_duration_seconds = nn_signal_seconds,\n",
    "        wanted_shift_length_seconds = shift_length_seconds,\n",
    "        absolute_shift_deviation_seconds = absolute_shift_deviation_seconds\n",
    "        )\n",
    "\n",
    "print(\"Shift length:\", shift_length)\n",
    "print(f\"Shift length: {shift_length / frequency} seconds\")\n",
    "print(\"Signal shape: \", signal.shape)\n",
    "print(f\"Datapoints in NN: {nn_signal_seconds * frequency}\")\n",
    "print(\"Signals from splitting shape: \", list_shape(signals_from_splitting))\n",
    "\n",
    "del signals_from_splitting, signal, shift_length, frequency, nn_signal_seconds, shift_length_seconds, absolute_shift_deviation_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting signals within dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dictionary:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: (174240,)\n",
      "RRI_frequency: 4\n",
      "MAD: (43560,)\n",
      "MAD_frequency: 1\n",
      "\n",
      "New dictionaries:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: (144000,)\n",
      "RRI_frequency: 4\n",
      "MAD: (36000,)\n",
      "MAD_frequency: 1\n",
      "shift_length_seconds: 3780\n",
      "\n",
      "ID: 1_shift_x1\n",
      "RRI: (144000,)\n",
      "RRI_frequency: 4\n",
      "MAD: (36000,)\n",
      "MAD_frequency: 1\n",
      "shift_length_seconds: 3780\n",
      "\n",
      "ID: 1_shift_x2\n",
      "RRI: (144000,)\n",
      "RRI_frequency: 4\n",
      "MAD: (36000,)\n",
      "MAD_frequency: 1\n",
      "shift_length_seconds: 3780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create random signal\n",
    "length_signal_seconds = 12.1 * 3600\n",
    "rri_frequency = 4\n",
    "mad_frequency = 1\n",
    "rri_signal = np.random.rand(int(length_signal_seconds * rri_frequency))\n",
    "mad_signal = np.random.rand(int(length_signal_seconds * mad_frequency))\n",
    "\n",
    "data_dict = {\n",
    "    \"ID\": \"1\",\n",
    "    \"RRI\": rri_signal,\n",
    "    \"RRI_frequency\": rri_frequency,\n",
    "    \"MAD\": mad_signal,\n",
    "    \"MAD_frequency\": mad_frequency,\n",
    "}\n",
    "\n",
    "new_dictionaries = split_signals_within_dictionary(\n",
    "    data_dict = data_dict,\n",
    "    id_key = \"ID\",\n",
    "    valid_signal_keys = [\"RRI\", \"MAD\"],\n",
    "    signal_frequencies = [rri_frequency, mad_frequency],\n",
    "    signal_target_frequencies = [rri_frequency, mad_frequency],\n",
    "    nn_signal_duration_seconds = 10 * 3600,\n",
    "    wanted_shift_length_seconds = 3600,\n",
    "    absolute_shift_deviation_seconds = 1800,\n",
    "    all_signal_frequencies = [rri_frequency, mad_frequency]\n",
    ")\n",
    "\n",
    "print(\"Original dictionary:\")\n",
    "print(\"-\"*20)\n",
    "for key, value in data_dict.items():\n",
    "    if key == \"RRI\" or key == \"MAD\" or key == \"SLP\":\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "print(\"\\nNew dictionaries:\")\n",
    "print(\"-\"*20)\n",
    "for new_dict in new_dictionaries:\n",
    "    for key, value in new_dict.items():\n",
    "        if key == \"RRI\" or key == \"MAD\" or key == \"SLP\":\n",
    "            print(f\"{key}: {value.shape}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fusing signals back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted Signals:\n",
      " [array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15]), array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21])]\n",
      "\n",
      "Fused signal:\n",
      " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n"
     ]
    }
   ],
   "source": [
    "signals_from_splitting, shift_length = split_long_signal(\n",
    "        signal = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], # type: ignore\n",
    "        sampling_frequency = 1,\n",
    "        target_frequency = 1,\n",
    "        nn_signal_duration_seconds = 10,\n",
    "        wanted_shift_length_seconds = 5,\n",
    "        absolute_shift_deviation_seconds = 1,\n",
    "        all_signal_frequencies = [1]\n",
    "        )\n",
    "\n",
    "print(\"Splitted Signals:\\n\", signals_from_splitting)\n",
    "\n",
    "fused_signal = fuse_splitted_signals(\n",
    "    signals = signals_from_splitting, # type: ignore\n",
    "    shift_length = int(shift_length), # type: ignore\n",
    "    signal_type = \"feature\"\n",
    ")\n",
    "\n",
    "print(\"\\nFused signal:\\n\", fused_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fusing splitted dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dictionary:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: (46,)\n",
      "RRI_frequency: 2\n",
      "MAD: (23,)\n",
      "MAD_frequency: 1\n",
      "SLP: (12,)\n",
      "SLP_frequency: 0.5\n"
     ]
    }
   ],
   "source": [
    "data_dict = {\n",
    "    \"ID\": \"1\",\n",
    "    \"RRI\": np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22]),\n",
    "    \"RRI_frequency\": 2,\n",
    "    \"MAD\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]),\n",
    "    \"MAD_frequency\": 1,\n",
    "    \"SLP\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n",
    "    \"SLP_frequency\": 0.5\n",
    "}\n",
    "\n",
    "new_dictionaries = split_signals_within_dictionary(\n",
    "    data_dict = data_dict,\n",
    "    id_key = \"ID\",\n",
    "    valid_signal_keys = [\"RRI\", \"MAD\", \"SLP\"],\n",
    "    signal_frequencies = [2, 1, 0.5],\n",
    "    signal_target_frequencies = [2, 1, 0.5],\n",
    "    nn_signal_duration_seconds = 10,\n",
    "    wanted_shift_length_seconds = 5,\n",
    "    absolute_shift_deviation_seconds = 2,\n",
    "    all_signal_frequencies = [2, 1, 0.5]\n",
    ")\n",
    "\n",
    "print(\"Original dictionary:\")\n",
    "print(\"-\"*20)\n",
    "for key, value in data_dict.items():\n",
    "    if key == \"RRI\" or key == \"MAD\" or key == \"SLP\":\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New dictionaries:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: [0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9]\n",
      "RRI_frequency: 2\n",
      "MAD: [0 1 2 3 4 5 6 7 8 9]\n",
      "MAD_frequency: 1\n",
      "SLP: [0 1 2 3 4]\n",
      "SLP_frequency: 0.5\n",
      "shift_length_seconds: 6\n",
      "\n",
      "ID: 1_shift_x1\n",
      "RRI: [ 6  6  7  7  8  8  9  9 10 10 11 11 12 12 13 13 14 14 15 15]\n",
      "RRI_frequency: 2\n",
      "MAD: [ 6  7  8  9 10 11 12 13 14 15]\n",
      "MAD_frequency: 1\n",
      "SLP: [3 4 5 6 7]\n",
      "SLP_frequency: 0.5\n",
      "shift_length_seconds: 6\n",
      "\n",
      "ID: 1_shift_x2\n",
      "RRI: [12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21]\n",
      "RRI_frequency: 2\n",
      "MAD: [12 13 14 15 16 17 18 19 20 21]\n",
      "MAD_frequency: 1\n",
      "SLP: [ 6  7  8  9 10]\n",
      "SLP_frequency: 0.5\n",
      "shift_length_seconds: 6\n",
      "\n",
      "ID: 1_shift_x3\n",
      "RRI: [18 18 19 19 20 20 21 21 22 22]\n",
      "RRI_frequency: 2\n",
      "MAD: [18 19 20 21 22]\n",
      "MAD_frequency: 1\n",
      "SLP: [ 9 10 11]\n",
      "SLP_frequency: 0.5\n",
      "shift_length_seconds: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNew dictionaries:\")\n",
    "print(\"-\"*20)\n",
    "for new_dict in new_dictionaries:\n",
    "    for key, value in new_dict.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1\n",
      "RRI: [ 0  0  1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10 11 11\n",
      " 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22]\n",
      "MAD: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]\n",
      "SLP: [ 0  1  2  3  4  5  6  7  8  9 10  9 10 11]\n",
      "RRI_frequency: 2\n",
      "MAD_frequency: 1\n",
      "SLP_frequency: 0.5\n"
     ]
    }
   ],
   "source": [
    "fused_dictionary = fuse_splitted_signals_within_dictionaries(\n",
    "    data_dictionaries = new_dictionaries,\n",
    "    valid_signal_keys = [\"RRI\", \"MAD\", \"SLP\"],\n",
    "    valid_signal_frequencies = [2, 1, 0.5],\n",
    ")\n",
    "\n",
    "for key, value in fused_dictionary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a .h5 - file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the available training datasets for our neural network model are stored in a .h5 file. So we need\n",
    "to be able to read it. These are the important operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random patient: 201966_1\n",
      "[0 1 2 3 5]\n",
      "\n",
      "key: slp\n",
      "Data shape: (747,)\n",
      "Data frequency: 0.03333333333333333\n",
      "Inverse data frequency: 30.0\n",
      "Data length: 22410.0 s\n",
      "\n",
      "key: rri\n",
      "Data shape: (89640,)\n",
      "Data frequency: 4\n",
      "Inverse data frequency: 0.25\n",
      "Data length: 22410.0 s\n"
     ]
    }
   ],
   "source": [
    "shhs_dataset = h5py.File(\"Raw_Data/SHHS_dataset.h5\", 'r')\n",
    "patients = list(shhs_dataset['slp'].keys()) # type: ignore\n",
    "\n",
    "random_patient = patients[np.random.randint(0, len(patients))]\n",
    "print(f\"Random patient: {random_patient}\")\n",
    "\n",
    "print(np.unique(shhs_dataset[\"slp\"][random_patient][:])) # type: ignore\n",
    "\n",
    "for key in [\"slp\", \"rri\"]:\n",
    "    print(f\"\\nkey: {key}\")\n",
    "\n",
    "    patient_data = shhs_dataset[key][random_patient][:] # type: ignore\n",
    "    print(f\"Data shape: {patient_data.shape}\") # type: ignore\n",
    "\n",
    "    data_freq = shhs_dataset[key].attrs[\"freq\"] # type: ignore\n",
    "    print(f\"Data frequency: {data_freq}\")\n",
    "    print(f\"Inverse data frequency: {1/data_freq}\") # type: ignore\n",
    "\n",
    "    print(f\"Data length: {patient_data.shape[0]/data_freq} s\") # type: ignore\n",
    "\n",
    "del shhs_dataset, patients, random_patient, key, patient_data, data_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide up a signal into overlapping windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardest thing about this is, that 'window_overlap' and 'datapoints_per_window' must be chosen so that\n",
    "the whole signal fits perfectly into n windows. \n",
    "\n",
    "Additionally, those values must be integers. This means that 'window_duration_seconds' and 'overlap_seconds'\n",
    "multiplied with 'target_fequency' as well as 'sampling_frequency' must be integers. (The features and the target labels\n",
    "must fit equally well into the windows, so that we can find the correlation between a feature- and target- window.)\n",
    "\n",
    "We have the RRI and MAD values as features and the sleep phase as target classification. As we will see,\n",
    "RRI and MAD values were recorded with an integer sampling frequency. While the sampling frequency of the \n",
    "sleep classification is 1/30. \n",
    "\n",
    "Finding window parameters that fullfill the conditions mentioned is easier than it sounds. We will always pass data\n",
    "to the neural network that is 10 hours long. Now, we just need to think in seconds and find integer values\n",
    "for 'window_duration_seconds' and 'overlap_seconds' that are a multiple of 30:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding optimal window_parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suitable window parameters for signal of length: 36000:\n",
      "-------------------------------------------------------\n",
      "Number of windows: 1025, Window size: 160, Overlap: 125.0\n",
      "Number of windows: 1026, Window size: 125, Overlap: 90.0\n",
      "Number of windows: 1055, Window size: 164, Overlap: 130.0\n",
      "Number of windows: 1056, Window size: 130, Overlap: 96.0\n",
      "Number of windows: 1087, Window size: 162, Overlap: 129.0\n",
      "Number of windows: 1088, Window size: 129, Overlap: 96.0\n",
      "Number of windows: 1121, Window size: 160, Overlap: 128.0\n",
      "Number of windows: 1122, Window size: 128, Overlap: 96.0\n",
      "Number of windows: 1157, Window size: 164, Overlap: 133.0\n",
      "Number of windows: 1158, Window size: 133, Overlap: 102.0\n",
      "Number of windows: 1196, Window size: 150, Overlap: 120.0\n",
      "Number of windows: 1197, Window size: 120, Overlap: 90.0\n"
     ]
    }
   ],
   "source": [
    "find_suitable_window_parameters(\n",
    "        signal_length = 10 * 3600,\n",
    "        number_windows_range = (1000, 1400),\n",
    "        window_size_range = (120, 180),\n",
    "        minimum_window_size_overlap_difference = 30\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our options are:\n",
    "\n",
    "Number of windows: 1196, Window size: 150, Overlap: 120.0 \\\n",
    "Number of windows: 1197, Window size: 120, Overlap: 90.0\n",
    "\n",
    "We will choose the latter, because we don't want the window_size to be too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When transforming a classification signal into windows, which is supposed to be the target in the neural \n",
    "network, then each window will only be represented by the most common sleep stage. If there is a tie\n",
    "between the labels, then the one with the highest priority will be chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal shape: (1200,)\n",
      "Signal in windows shape: (1197,)\n"
     ]
    }
   ],
   "source": [
    "signal_length_seconds = 10 * 3600\n",
    "frequency = 1/30\n",
    "signal_length = int(signal_length_seconds * frequency)\n",
    "\n",
    "signal = np.array([random.randint(0, 5) for _ in range(signal_length)])\n",
    "\n",
    "signal_in_windows = signal_to_windows(\n",
    "    signal = signal, # type: ignore\n",
    "    datapoints_per_window = int(120 * frequency),\n",
    "    window_overlap = int(90 * frequency),\n",
    "    signal_type = \"target\",\n",
    "    priority_order = [0, 1, 2, 3, 4, 5, -1]\n",
    "    )\n",
    "\n",
    "print(f\"Signal shape: {signal.shape}\")\n",
    "print(f\"Signal in windows shape: {signal_in_windows.shape}\")\n",
    "\n",
    "del signal, signal_in_windows, signal_length_seconds, frequency, signal_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal shape: (36000,)\n",
      "Signal in windows shape: (1197, 120)\n"
     ]
    }
   ],
   "source": [
    "signal = np.random.rand(36000)\n",
    "\n",
    "signal_in_windows = signal_to_windows(\n",
    "    signal = signal, # type: ignore\n",
    "    datapoints_per_window = 120,\n",
    "    window_overlap = 90,\n",
    "    signal_type = \"feature\"\n",
    "    )\n",
    "\n",
    "print(f\"Signal shape: {signal.shape}\")\n",
    "print(f\"Signal in windows shape: {signal_in_windows.shape}\")\n",
    "\n",
    "del signal, signal_in_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will be applied to transform a signal into overlapping windows. It will make sure\n",
    "that the data is passed correctly to the function mentioned above. \n",
    "\n",
    "This means it will:\n",
    "- check if 'number_nn_datapoints', 'datapoints_per_window' and 'window_overlap' are integers\n",
    "- check if 'datapoints_per_window' and 'window_overlap' perfectly fit into 'number_nn_datapoints'\n",
    "- compare length of provided signal to length of signal in nn ('number_nn_datapoints')\n",
    "    - if smaller: Pad with Zeros\n",
    "    - if bigger: Print warning, but continue by cropping last datapoints\n",
    "- check if signal transformed to windows has the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random array shape: (36000,)\n",
      "Reshaped array shape: (1197, 480)\n",
      "Random array shape: (1200,)\n",
      "Reshaped array shape: (1197,)\n"
     ]
    }
   ],
   "source": [
    "random_array = np.random.rand(36000)\n",
    "reshaped_array = reshape_signal_to_overlapping_windows(\n",
    "    signal = random_array, # type: ignore\n",
    "    target_frequency = 4, \n",
    "    number_windows = 1197, \n",
    "    window_duration_seconds = 120, \n",
    "    overlap_seconds = 90,\n",
    "    signal_type = \"feature\",\n",
    "    nn_signal_duration_seconds = 10*3600,\n",
    "    )\n",
    "\n",
    "print(f\"Random array shape: {random_array.shape}\")\n",
    "print(f\"Reshaped array shape: {reshaped_array.shape}\")\n",
    "\n",
    "random_array = np.array([random.randint(0, 3) for _ in range(int(36000/30))])\n",
    "reshaped_array = reshape_signal_to_overlapping_windows(\n",
    "    signal = random_array, # type: ignore\n",
    "    target_frequency = 1/30, \n",
    "    number_windows = 1197, \n",
    "    window_duration_seconds = 120, \n",
    "    overlap_seconds = 90,\n",
    "    signal_type = \"target\",\n",
    "    nn_signal_duration_seconds = 10*3600,\n",
    "    )\n",
    "\n",
    "print(f\"Random array shape: {random_array.shape}\")\n",
    "print(f\"Reshaped array shape: {reshaped_array.shape}\")\n",
    "\n",
    "del random_array, reshaped_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse Reshape\n",
    "\n",
    "Reversing Reshape of feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original signal:\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "Signal reshaped to overlapping windows:\n",
      "[[ 1  2  3  4  5]\n",
      " [ 2  3  4  5  6]\n",
      " [ 3  4  5  6  7]\n",
      " [ 4  5  6  7  8]\n",
      " [ 5  6  7  8  9]\n",
      " [ 6  7  8  9 10]\n",
      " [ 7  8  9 10  0]\n",
      " [ 8  9 10  0  0]\n",
      " [ 9 10  0  0  0]\n",
      " [10  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]]\n",
      "\n",
      "Last window when padding was cropped:\n",
      "[10  0  0  0  0]\n",
      "\n",
      "Signal reshaped back to original:\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original signal:\")\n",
    "test = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "print(test)\n",
    "\n",
    "print(\"\\nSignal reshaped to overlapping windows:\")\n",
    "reshaped_test = reshape_signal_to_overlapping_windows(\n",
    "    signal = test,\n",
    "    target_frequency = 1,\n",
    "    nn_signal_duration_seconds = 16,\n",
    "    number_windows = 12,\n",
    "    window_duration_seconds = 5,\n",
    "    overlap_seconds = 4,\n",
    "    signal_type = \"feature\"\n",
    "    )\n",
    "print(reshaped_test)\n",
    "\n",
    "print(\"\\nLast window when padding was cropped:\")\n",
    "cropped_padding = remove_padding_from_windows(\n",
    "    signal_in_windows = copy.deepcopy(reshaped_test), # type: ignore\n",
    "    target_frequency = 1,\n",
    "    original_signal_length = 10,\n",
    "    window_duration_seconds = 5, \n",
    "    overlap_seconds = 4,\n",
    "    )\n",
    "print(cropped_padding[-1])\n",
    "\n",
    "print(\"\\nSignal reshaped back to original:\")\n",
    "reversed_test = reverse_signal_to_windows_reshape(\n",
    "    signal_in_windows = reshaped_test, # type: ignore\n",
    "    target_frequency = 1,\n",
    "    original_signal_length = 10,\n",
    "    number_windows = 12,\n",
    "    window_duration_seconds = 5,\n",
    "    overlap_seconds = 4\n",
    "    )\n",
    "print(reversed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sleep stage labels were reshaped differently, as we only keep one label for each window and therefore won't\n",
    "create a 2d array. \n",
    "\n",
    "After predicting the sleep stage labels, we will transform them into a 2d array, that is computable by our \n",
    "reverse reshape function. Effectively, we will create an array from each label, containing only the label as\n",
    "elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original signal:\n",
      "[1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
      "\n",
      "Signal reshaped to overlapping windows:\n",
      "[1 1 1 2 2 2 3 3 3]\n",
      "\n",
      "Expanded signal:\n",
      "[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [2, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2], [3, 3, 3, 3], [3, 3, 3, 3], [3, 3, 3, 3]]\n",
      "\n",
      "Expanded signal reshaped to original:\n",
      "[1.   1.   1.   1.25 1.5  1.75 2.25 2.5  2.75 3.   3.   3.  ]\n",
      "[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original signal:\")\n",
    "test = [1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
    "print(test)\n",
    "reshaped_test = reshape_signal_to_overlapping_windows(\n",
    "    signal = test,\n",
    "    target_frequency = 1/3,\n",
    "    nn_signal_duration_seconds = 36,\n",
    "    number_windows = 9,\n",
    "    window_duration_seconds = 12,\n",
    "    overlap_seconds = 9,\n",
    "    signal_type = \"target\"\n",
    "    )\n",
    "\n",
    "print(\"\\nSignal reshaped to overlapping windows:\")\n",
    "print(reshaped_test)\n",
    "\n",
    "expanded_reshaped_test = []\n",
    "for slp_stg in reshaped_test:\n",
    "    expanded_reshaped_test.append([slp_stg for _ in range(int(12 * 1/3))])\n",
    "\n",
    "print(\"\\nExpanded signal:\")\n",
    "print(expanded_reshaped_test)\n",
    "\n",
    "reversed_test = reverse_signal_to_windows_reshape(\n",
    "    signal_in_windows = expanded_reshaped_test, # type: ignore\n",
    "    target_frequency = 1/3, # type: ignore\n",
    "    original_signal_length = 12,\n",
    "    number_windows = 9,\n",
    "    window_duration_seconds = 12,\n",
    "    overlap_seconds = 9\n",
    "    )\n",
    "\n",
    "print(\"\\nExpanded signal reshaped to original:\")\n",
    "print(reversed_test)\n",
    "print([round(i) for i in reversed_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Signal\n",
    "\n",
    "The implemented unity normalization function can either normalize a multi-dimensional array across all\n",
    "arrays (global) or normalize each array indivudally (local)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dimensional = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "two_dimensional = np.array([[0, 2, 4], [4, 5, 6], [6, 8, 10]])\n",
    "three_dimensional = np.array([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [8, 9, 10]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization_Mode: 'global'\n",
      "----------------------------\n",
      "\n",
      "Normalized One dimensional array:\n",
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
      "\n",
      "Normalized Two dimensional array:\n",
      "[[0.  0.2 0.4]\n",
      " [0.4 0.5 0.6]\n",
      " [0.6 0.8 1. ]]\n",
      "\n",
      "Normalized Three dimensional array:\n",
      "[[[0.  0.1 0.2]\n",
      "  [0.3 0.4 0.5]]\n",
      "\n",
      " [[0.6 0.7 0.8]\n",
      "  [0.8 0.9 1. ]]]\n"
     ]
    }
   ],
   "source": [
    "message = \"Normalization_Mode: \\'global\\'\"\n",
    "print(message)\n",
    "print(\"-\"*len(message))\n",
    "print(\"\\nNormalized One dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = one_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"global\"\n",
    "    ))\n",
    "print(\"\\nNormalized Two dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = two_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"global\"\n",
    "    ))\n",
    "print(\"\\nNormalized Three dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = three_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"global\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization_Mode: 'local'\n",
      "---------------------------\n",
      "\n",
      "Normalized One dimensional array:\n",
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
      "\n",
      "Normalized Two dimensional array:\n",
      "[[0.  0.5 1. ]\n",
      " [0.  0.5 1. ]\n",
      " [0.  0.5 1. ]]\n",
      "\n",
      "Normalized Three dimensional array:\n",
      "[[[0.  0.5 1. ]\n",
      "  [0.  0.5 1. ]]\n",
      "\n",
      " [[0.  0.5 1. ]\n",
      "  [0.  0.5 1. ]]]\n"
     ]
    }
   ],
   "source": [
    "message = \"Normalization_Mode: \\'local\\'\"\n",
    "print(message)\n",
    "print(\"-\"*len(message))\n",
    "print(\"\\nNormalized One dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = one_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"local\"\n",
    "    ))\n",
    "print(\"\\nNormalized Two dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = two_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"local\"\n",
    "    ))\n",
    "print(\"\\nNormalized Three dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = three_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"local\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alter Sleep Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function makes sure to keep labels unfiform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2 -1  0  1  2  3  4  5  6  7]\n",
      "[-1 -1  0  0  1  2 -1  3 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "slp = np.array([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7])\n",
    "print(slp)\n",
    "\n",
    "current_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "desired_labels = {\"wake\": 0, \"LS\": 1, \"DS\": 2, \"REM\": 3, \"artifect\": -1}\n",
    "\n",
    "print(alter_slp_labels(\n",
    "        slp_labels = slp, # type: ignore\n",
    "        current_labels = current_labels,\n",
    "        desired_labels = desired_labels,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['light_sleep' 'deep_sleep' 'deep_sleep_2' 'WAKE' 'REM' 'bla' 'blub']\n",
      "['1' '2' '2' '0' '3' '-1' '-1']\n"
     ]
    }
   ],
   "source": [
    "slp = np.array([\"light_sleep\", \"deep_sleep\", \"deep_sleep_2\", \"WAKE\", \"REM\", \"bla\", \"blub\"])\n",
    "print(slp)\n",
    "\n",
    "current_labels = {\"wake\": [\"WAKE\"], \"LS\": [\"light_sleep\"], \"DS\": [\"deep_sleep\", \"deep_sleep_2\"], \"REM\": [\"REM\"], \"artifect\": [\"other\"]}\n",
    "desired_labels = {\"wake\": 0, \"LS\": 1, \"DS\": 2, \"REM\": 3, \"artifect\": -1}\n",
    "\n",
    "print(alter_slp_labels(\n",
    "        slp_labels = slp, # type: ignore\n",
    "        current_labels = current_labels,\n",
    "        desired_labels = desired_labels,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Transformation from previous (not mine) Sleep Stage Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2 -1  0  1  2  3  4  5  6  7]\n",
      "[-2  0  0  0  1  2  3  3  0  6]\n"
     ]
    }
   ],
   "source": [
    "slp = np.array([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7])\n",
    "print(slp)\n",
    "\n",
    "slp[slp>=1] = slp[slp>=1] - 1\n",
    "slp[slp==4] = 3\n",
    "slp[slp==5] = 0\n",
    "slp[slp==-1] = 0 # set artifact as wake stage\n",
    "\n",
    "print(slp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
