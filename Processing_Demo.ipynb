{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Author: Johannes Peter Knoll\n",
    "\n",
    "Within this notebook you will learn and test everything that was implemented to preprocess the data\n",
    "for the neural network.\n",
    "\n",
    "Note:   This notebook is rather for those who want to make sure everything works correctly. It is very thorough\n",
    "        and therefore unnecessary if you only want to get a quick start into the predictions. If that is the case, head\n",
    "        to 'Classification_Demo.ipynb'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thorough Demonstration of 'dataset_processing.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# The autoreload extension allows you to tweak the code in the imported modules\n",
    "# and rerun cells to reflect the changes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we demonstrate the implemented class that helps you to manage the data you want to pass to the\n",
    "neural network model.\n",
    "\n",
    "The usage of this class is not required, you could also just use the implemented functions on your data, which\n",
    "are explained in the next section.\n",
    "\n",
    "I still would highly recommend using this class, as it is able to handle large data in a memory saving way\n",
    "and makes it very easy to check and process your data, so that it can be passed to the model easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_processing import *\n",
    "\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Database itself is just a .pkl file that contains multiple dictionaries. The first dictionary is always\n",
    "the file information, while the following will be the Database's datapoints. Each datapoint needs a unique ID\n",
    "(key: \"ID\") and can contain the following data: RRI-Signal (key: \"RRI\"), MAD-Signal  (key: \"MAD\"), Sleep-Labels \n",
    "(key: \"SLP\").\n",
    "\n",
    "You might wonder about the individual sampling frequencies... \\\n",
    "The point of the database is to store the data in the same format, so that it can be processed by the neural network\n",
    "correctly. Therefore the sampling frequency for each of the signals is uniform across all datapoints.\n",
    "(For our network we need: RRI frequency = 4, Mad frequency = 1, and SLP frequency = 1/30 for all datapoints.)\n",
    "\n",
    "The frequency for each signal is saved in the file information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not know if this needs to be said, but NEVER MANUALLY CHANGE THE CLASS ATTRIBUTES when data was already\n",
    "added.\n",
    "\n",
    "If you have different requirements for the unfiform frequency, signal length or any of the other parameters,\n",
    "that's fine. But change them before you save data to it. Even better: Alter the hard coded default values in\n",
    "the class, so that you do not forget to update the file information each time you create a new instance of \n",
    "the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Database (.pkl - file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initializing a database (calling 'SleepDataManager' on non-existent path to .pkl file) the class will\n",
    "automatically write the first dictionary to it, which functions as an information on the data properties\n",
    "within the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRI_frequency: 4\n",
      "MAD_frequency: 1\n",
      "SLP_frequency: 0.03333333333333333\n",
      "sleep_stage_label: {'wake': 0, 'LS': 1, 'DS': 2, 'REM': 3, 'artifect': -1}\n",
      "signal_length_seconds: 36000\n",
      "wanted_shift_length_seconds: 3600\n",
      "absolute_shift_deviation_seconds: 1800\n",
      "train_val_test_split_applied: False\n",
      "main_file_path: Sleep_Data/messing_around.pkl\n",
      "train_file_path: Sleep_Data/messing_around_training_pid.pkl\n",
      "validation_file_path: Sleep_Data/messing_around_validation_pid.pkl\n",
      "test_file_path: Sleep_Data/messing_around_test_pid.pkl\n"
     ]
    }
   ],
   "source": [
    "some_data_manager = SleepDataManager(file_path = \"Sleep_Data/messing_around.pkl\")\n",
    "file_information = some_data_manager.file_info\n",
    "\n",
    "for key in file_information.keys():\n",
    "    print(f\"{key}: {file_information[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already mentioned the first three keys, don't mind the others yet. Most of them will be explained below.\n",
    "For the few others, take a look at the function: 'split_long_signal'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Data is harder than it sounds. As mentioned above: we want to store uniform data. Therefore we need to \n",
    "alter the data so that it fits to the file data parameters (file info).\n",
    "\n",
    "By \"we\" I of course mean the simple function that you call to save data. I just wanted to point out that \n",
    "it does not trust you on the data you pass to it. So, if you want to add a signal, you must also pass its\n",
    "sampling frequency (keys: \"RRI_frequency\", \"MAD_frequency\", \"SLP_frequency\").\n",
    "\n",
    "The only keys you are allowed to provide are the following: \"ID\", \"RRI\", \"MAD\", \"SLP\", \"RRI_frequency\", \n",
    "\"MAD_frequency\", \"SLP_frequency\" and \"sleep_stage_label\".\n",
    "\n",
    "The save() function takes the new data as dictionary.\n",
    "\n",
    "Operations that might be happening to the data you try to save:\n",
    "- scale number of datapoints in signal so that signal frequency matches uniform database signal frequency\n",
    "- alter sleep labels\n",
    "- split signal into multiple signals if signal is longer than the uniform maximum signal length: 'signal_length_seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altering the sleep stage labels is highly important and must therefore be dealt with caution, that's why I\n",
    "will provide an example here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleep stage labels in shhs dataset:\n",
    "# \"wake\": 0,    \"N1\": 1,    \"N2\": 2,    \"N3\": 3,    \"REM\": 5,   \"artifect\": \"other integers\"\n",
    "\n",
    "# in the nn we only divide between wake, LS, DS, REM, and artifect. Above, N1 must be redeclared as \"wake\", \n",
    "# N2 as \"LS\" and N3 as \"DS\":\n",
    "shhs_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for the keys must be lists, this was implemented for the special case above, in which the sleep\n",
    "stages were more subdivided than in the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will pass the worst possible data to the database: differing sampling frequencies and overlength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First datapoints of RRI signal: [4. 4. 3. 1. 4. 3. 4. 4. 4. 3.] (shape: (261360,))\n",
      "First datapoints of RRI signal scaled: [4.  3.5 1.  3.5 4.  4.  3.  3.  2.  4.5] (shape: (174240,))\n",
      "First datapoints of MAD signal: [4, 3, 3, 4, 4, 1, 4, 2, 5, 5] (shape: 87120)\n",
      "First datapoints of MAD signal scaled: [4 3 4 4 5 2 3 2 2 3] (shape: 43560)\n",
      "First datapoints of SLP signal: [5, 5, 1, 1, 5, 5, 5, 3, 4, 1] (shape: 2178)\n",
      "First datapoints of SLP signal scaled: [5 5 1 5 5 3 1 5 3 1] (shape: 1452)\n",
      "First datapoints of scaled SLP signal altered: [3 3 0 3 3 2 0 3 2 0] (shape: 1452)\n"
     ]
    }
   ],
   "source": [
    "# creating signal with different sampling frequencies and overlength:\n",
    "signal_time_in_seconds = 12.1 * 3600\n",
    "rri_frequency = 6\n",
    "mad_frequency = 2\n",
    "slp_frequency = 1/20\n",
    "\n",
    "# creating signals and printing manually scaled versions\n",
    "rri_signal = np.array([random.randint(1, 5) for i in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "print(f\"First datapoints of RRI signal: {rri_signal[:10]} (shape: {rri_signal.shape})\")\n",
    "interpolate_rri = interpolate_signal(rri_signal, rri_frequency, 4) # type: ignore\n",
    "print(f\"First datapoints of RRI signal scaled: {interpolate_rri[:10]} (shape: {interpolate_rri.shape})\")\n",
    "mad_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * mad_frequency))]\n",
    "print(f\"First datapoints of MAD signal: {mad_signal[:10]} (shape: {len(mad_signal)})\")\n",
    "interpolate_mad = interpolate_signal(mad_signal, mad_frequency, 1) # type: ignore\n",
    "print(f\"First datapoints of MAD signal scaled: {interpolate_mad[:10]} (shape: {len(interpolate_mad)})\")\n",
    "slp_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "print(f\"First datapoints of SLP signal: {slp_signal[:10]} (shape: {len(slp_signal)})\")\n",
    "scaled_slp = scale_classification_signal(slp_signal, slp_frequency, 1/30) # type: ignore\n",
    "print(f\"First datapoints of SLP signal scaled: {scaled_slp[:10]} (shape: {len(scaled_slp)})\")\n",
    "\n",
    "random_sleep_stage_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "altered_scaled_slp = alter_slp_labels(scaled_slp, random_sleep_stage_labels) # type: ignore\n",
    "print(f\"First datapoints of scaled SLP signal altered: {altered_scaled_slp[:10]} (shape: {len(altered_scaled_slp)})\")\n",
    "\n",
    "new_datapoint = {\n",
    "    \"ID\": \"4\",\n",
    "    \"RRI\": rri_signal,\n",
    "    \"RRI_frequency\": rri_frequency,\n",
    "    \"MAD\": mad_signal,\n",
    "    \"MAD_frequency\": mad_frequency,\n",
    "    \"SLP\": slp_signal,\n",
    "    \"SLP_frequency\": slp_frequency,\n",
    "    \"sleep_stage_label\": random_sleep_stage_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the new datapoints\n",
    "some_data_manager.save(copy.deepcopy(new_datapoint), overwrite_id=True, unique_id=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "ID 4\n",
      "RRI [4.  3.5 1.  3.5 4.  4.  3.  3.  2.  4.5] (144000,)\n",
      "MAD [4 3 4 4 5 2 3 2 2 3] (36000,)\n",
      "SLP [3 3 0 3 3 2 0 3 2 0] (1200,)\n",
      "----------------------------------------------------------------------\n",
      "ID 4_shift_3780s_x1\n",
      "RRI [2.  3.  3.  4.5 4.  2.5 4.  3.  3.  3.5] (144000,)\n",
      "MAD [5 5 2 5 1 3 2 2 1 3] (36000,)\n",
      "SLP [ 1  2  3  0 -1  1  3  0  0 -1] (1200,)\n",
      "----------------------------------------------------------------------\n",
      "ID 4_shift_3780s_x2\n",
      "RRI [2.  3.5 1.  5.  5.  4.  2.  3.  1.  4.5] (144000,)\n",
      "MAD [3 3 4 4 3 4 5 3 3 4] (36000,)\n",
      "SLP [ 1 -1 -1 -1  1  0  3  1  3  1] (1200,)\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print the data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*70)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "            print(key, dict[key][:10], dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry at this point too much about how the scaling and splitting works. The responsible functions are\n",
    "explained in more detail (far) below. \n",
    "\n",
    "Just notice that the signal length now matches the uniform file length of 10 hours (36 000 seconds).\n",
    "(Remember: number datapoints in signal = signal length in seconds * frequency).\n",
    "\n",
    "You might also see that the signals in the first splitted signal (\"ID\"=4) match the manually rescaled\n",
    "signals in the code above where we initialized the new datapoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding information to already existing datapoints\n",
    "\n",
    "To add or overwrite an exisiting signal in the database, just set the optional argument \"overwrite_id\"\n",
    "to \"True\" when saving.\n",
    "\n",
    "If set to \"False\" and the ID already exists in the database, it will discard the data you are trying to save\n",
    "and raise an error.\n",
    "\n",
    "Attention: For this argument to have an effect, the optional argument \"unique_id\" must be set to \"False\". \n",
    "Further information below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed up data saving\n",
    "\n",
    "Because every id needs to be checked, it takes a while to compute if you want to add many datapoints.\n",
    "\n",
    "To speed up the process, you can check once if every id is unique and afterwards skip the checking when\n",
    "adding the datapoints to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All IDs are unique.\n"
     ]
    }
   ],
   "source": [
    "list_of_ids = [\"101\", \"102\", \"103\"]\n",
    "\n",
    "some_data_manager.check_if_ids_are_unique(list_of_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all ids are unique you can use: some_data_manager(..., unique_id=True) and the saving will be much faster!\n",
    "(Making \"overwrite_id\" obsolete.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be loaded in multiple ways. First we will look at the load() function.\n",
    "The argument you pass to it ('key_id_index') can be a string or an integer. \n",
    "\n",
    "If it's an integer, it will treat it as position in the database, analogue to a list. It will return the\n",
    "whole data dictionary. \\\n",
    "If it's a string that equals a data key it will return all entities of that specific key in the database. \\\n",
    "If it's a different string, then it will treat it as ID and look for a match. Equal to index, it will return\n",
    "the whole dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': '4_shift_3780s_x1', 'RRI': array([2. , 3. , 3. , ..., 3.5, 2. , 3.5]), 'MAD': array([5, 5, 2, ..., 3, 4, 4]), 'SLP': array([ 1,  2,  3, ..., -1,  3,  2])}\n"
     ]
    }
   ],
   "source": [
    "loaded_data = some_data_manager.load(1)\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': '4_shift_3780s_x1', 'RRI': array([2. , 3. , 3. , ..., 3.5, 2. , 3.5]), 'MAD': array([5, 5, 2, ..., 3, 4, 4]), 'SLP': array([ 1,  2,  3, ..., -1,  3,  2])}\n"
     ]
    }
   ],
   "source": [
    "loaded_data = some_data_manager.load(\"4_shift_3780s_x1\")\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([4. , 3.5, 1. , ..., 2.5, 4. , 4.5]), array([2. , 3. , 3. , ..., 3.5, 2. , 3.5]), array([2. , 3.5, 1. , ..., 3.5, 1. , 3.5])]\n"
     ]
    }
   ],
   "source": [
    "loaded_data = some_data_manager.load(\"RRI\")\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second way of achieving the last result is by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([4, 3, 4, ..., 1, 5, 4]), array([5, 5, 2, ..., 3, 4, 4]), array([3, 3, 4, ..., 1, 2, 1])]\n"
     ]
    }
   ],
   "source": [
    "loaded_data = some_data_manager[\"MAD\"]\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove() takes the same key as load() and works analogue, but instead of loading it will remove from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "ID 4\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "--------------------\n",
      "ID 4_shift_3780s_x1\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "--------------------\n",
      "ID 4_shift_3780s_x2\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "some_data_manager.remove(\"RRI\")\n",
    "\n",
    "# print all data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*20)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "            print(key, dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "ID 4\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "--------------------\n",
      "ID 4_shift_3780s_x2\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "some_data_manager.remove(\"4_shift_3780s_x1\")\n",
    "\n",
    "# print all data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*20)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "            print(key, dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "ID 4_shift_3780s_x2\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "some_data_manager.remove(0)\n",
    "\n",
    "# print all data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*20)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "            print(key, dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's restore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data_manager.save(copy.deepcopy(new_datapoint), overwrite_id=True, unique_id=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other 'smaller' operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over Database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_shift_3780s_x2\n",
      "4\n",
      "4_shift_3780s_x1\n"
     ]
    }
   ],
   "source": [
    "for datapoint in some_data_manager:\n",
    "    print(datapoint[\"ID\"])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if datapoint with certain ID is in database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoint with \"ID\" = 4 is in the data manager\n"
     ]
    }
   ],
   "source": [
    "if \"4\" in some_data_manager:\n",
    "    print(\"Datapoint with \\\"ID\\\" = 4 is in the data manager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: Sleep_Data/messing_around.pkl\n",
      "file_info: {'RRI_frequency': 4, 'MAD_frequency': 1, 'SLP_frequency': 0.03333333333333333, 'sleep_stage_label': {'wake': '0', 'LS': '1', 'DS': '2', 'REM': '3', 'artifect': '-1'}, 'signal_length_seconds': 36000, 'wanted_shift_length_seconds': 3600, 'absolute_shift_deviation_seconds': 1800, 'train_val_test_split_applied': False, 'main_file_path': 'Sleep_Data/messing_around.pkl', 'train_file_path': 'Sleep_Data/messing_around_training_pid.pkl', 'validation_file_path': 'Sleep_Data/messing_around_validation_pid.pkl', 'test_file_path': 'Sleep_Data/messing_around_test_pid.pkl'}\n"
     ]
    }
   ],
   "source": [
    "print(some_data_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-, Validation-, Test- Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we aim to train a machine learning model with the data handled by this class. So, we want to\n",
    "be able to separate the data into training-, validation- and test- pids.\n",
    "\n",
    "First, let's create a new file and add some more data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints in file: 100\n"
     ]
    }
   ],
   "source": [
    "many_files_data_manager = SleepDataManager(file_path = \"Sleep_Data/Data.pkl\")\n",
    "\n",
    "add_number_datapoints = 100\n",
    "\n",
    "# optimal signal (fitting sampling frequencies and length):\n",
    "signal_time_in_seconds = 10 * 3600\n",
    "rri_frequency = 4\n",
    "mad_frequency = 1\n",
    "slp_frequency = 1/30\n",
    "\n",
    "random_sleep_stage_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "\n",
    "for i in range(add_number_datapoints):\n",
    "    rri_signal = np.array([random.randint(1, 5) for i in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "    mad_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * mad_frequency))]\n",
    "    slp_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "\n",
    "    decide_what_data_to_add = random.randint(0, 2)\n",
    "\n",
    "    if decide_what_data_to_add == 0:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"MAD\": mad_signal,\n",
    "            \"MAD_frequency\": mad_frequency,\n",
    "            \"SLP\": slp_signal,\n",
    "            \"SLP_frequency\": slp_frequency,\n",
    "            \"sleep_stage_label\": random_sleep_stage_labels\n",
    "        } # optimal data (rri and mad to slp)\n",
    "    elif decide_what_data_to_add == 1:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"MAD\": mad_signal,\n",
    "            \"MAD_frequency\": mad_frequency,\n",
    "        } # invalid data (no target: slp)\n",
    "    else:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"SLP\": slp_signal,\n",
    "            \"SLP_frequency\": slp_frequency,\n",
    "            \"sleep_stage_label\": random_sleep_stage_labels\n",
    "        } # only rri to slp\n",
    "    \n",
    "    many_files_data_manager.save(new_datapoint, overwrite_id=False)\n",
    "\n",
    "print(f\"Number of datapoints in file: {len(many_files_data_manager)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending whether test_size is provided or None we can create separate files where training-, validation- and \n",
    "test- data or just training- and validation- data is stored:\n",
    "\n",
    "Data that can not be used to train the network (i.e. missing \"RRI\" and \"SLP\") will be left in the main file. \n",
    "        \n",
    "As we can manage data with \"RRI\" and \"MAD\" and data with \"RRI\" only, the algorithm makes sure\n",
    "that only one of the two types of data is used (the one with more samples). The other type will \n",
    "be left in the main file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention: 31 datapoints without MAD signal will be left in the main file.\n"
     ]
    }
   ],
   "source": [
    "many_files_data_manager.separate_train_test_validation(\n",
    "    train_size = 0.8, \n",
    "    validation_size = 0.1, \n",
    "    test_size = 0.1, \n",
    "    random_state = None, \n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual files can be accessed by another instance of this class. \n",
    "\n",
    "ATTENTION:  \n",
    "\n",
    "-   The instances on all files will have reduced functionality from now on. As the data should\n",
    "    be fully prepared for the network now, the instances are designed to only load data and\n",
    "    not save or edit it.\n",
    "\n",
    "-   The functionality of the instance on the main file is not as restricted as the ones on the\n",
    "    training, validation, and test files. The main file instance can additionally save data\n",
    "    (only to main file, won't be forwarded to training, validation, or test files), reshuffle \n",
    "    the data in the secondary files or pull them back into the main file for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's access our training-, validation- and test- data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_file_info = many_files_data_manager.file_info\n",
    "\n",
    "train_data_manager = SleepDataManager(file_path = main_file_info[\"train_file_path\"])\n",
    "validation_data_manager = SleepDataManager(file_path = main_file_info[\"validation_file_path\"])\n",
    "test_data_manager = SleepDataManager(file_path = main_file_info[\"test_file_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the individual file info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main file info is the same as train, validation and test file info!\n",
      "\n",
      "Uniform file information:\n",
      "------------------------------------------------------------\n",
      "RRI_frequency: 4\n",
      "MAD_frequency: 1\n",
      "SLP_frequency: 0.03333333333333333\n",
      "sleep_stage_label: {'wake': '0', 'LS': '1', 'DS': '2', 'REM': '3', 'artifect': '-1'}\n",
      "signal_length_seconds: 36000\n",
      "wanted_shift_length_seconds: 3600\n",
      "absolute_shift_deviation_seconds: 1800\n",
      "train_val_test_split_applied: True\n",
      "main_file_path: Sleep_Data/Data.pkl\n",
      "train_file_path: Sleep_Data/Data_training_pid.pkl\n",
      "validation_file_path: Sleep_Data/Data_validation_pid.pkl\n",
      "test_file_path: Sleep_Data/Data_test_pid.pkl\n"
     ]
    }
   ],
   "source": [
    "train_file_info = train_data_manager.file_info\n",
    "validation_file_info = validation_data_manager.file_info\n",
    "test_file_info = test_data_manager.file_info\n",
    "\n",
    "equal = True\n",
    "for key in main_file_info.keys():\n",
    "    if main_file_info[key] != train_file_info[key] or main_file_info[key] != validation_file_info[key] or main_file_info[key] != test_file_info[key]:\n",
    "        equal = False\n",
    "        break\n",
    "\n",
    "if equal:\n",
    "    print(\"Main file info is the same as train, validation and test file info!\")\n",
    "\n",
    "print(\"\\nUniform file information:\")\n",
    "print(\"-\"*60)\n",
    "for key in main_file_info.keys():\n",
    "    print(f\"{key}: {main_file_info[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each dataset:\n",
      "------------------------------\n",
      "Main: 64\n",
      "Train: 28\n",
      "Validation: 4\n",
      "Test: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of each dataset:\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Main: {len(many_files_data_manager)}\")\n",
    "print(f\"Train: {len(train_data_manager)}\")\n",
    "print(f\"Validation: {len(validation_data_manager)}\")\n",
    "print(f\"Test: {len(test_data_manager)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ID', 'RRI', 'MAD', 'SLP'])\n",
      "dict_keys(['ID', 'RRI', 'MAD', 'SLP'])\n",
      "dict_keys(['ID', 'RRI', 'MAD', 'SLP'])\n",
      "dict_keys(['ID', 'RRI', 'MAD', 'SLP'])\n"
     ]
    }
   ],
   "source": [
    "for datapoint in validation_data_manager:\n",
    "    print(datapoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ID', 'RRI', 'MAD', 'SLP'])\n",
      "dict_keys(['ID', 'RRI', 'MAD', 'SLP'])\n",
      "dict_keys(['ID', 'RRI', 'MAD', 'SLP'])\n",
      "dict_keys(['ID', 'RRI', 'MAD', 'SLP'])\n"
     ]
    }
   ],
   "source": [
    "for datapoint in test_data_manager:\n",
    "    print(datapoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 65\n",
      "RRI: [1. 5. 5. ... 1. 1. 2.]\n",
      "MAD: [5 4 3 ... 4 2 5]\n",
      "SLP: [ 2  2  2 ... -1  3  0]\n"
     ]
    }
   ],
   "source": [
    "some_datapoint = train_data_manager.load(random.randint(0, len(train_data_manager)))\n",
    "\n",
    "for key in some_datapoint.keys(): # type: ignore\n",
    "    print(f\"{key}: {some_datapoint[key]}\") # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can always reshuffle the data again from the main file manager (let's assign different \n",
    "arguments to see that something happened):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention: 31 datapoints without MAD signal will be left in the main file.\n"
     ]
    }
   ],
   "source": [
    "many_files_data_manager.separate_train_test_validation(\n",
    "    train_size = 0.5, \n",
    "    validation_size = 0.5, \n",
    "    test_size = None, \n",
    "    random_state = None, \n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each dataset:\n",
      "------------------------------\n",
      "Main: 64\n",
      "Train: 18\n",
      "Validation: 18\n",
      "No test data manager\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of each dataset:\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Main: {len(many_files_data_manager)}\")\n",
    "print(f\"Train: {len(train_data_manager)}\")\n",
    "print(f\"Validation: {len(validation_data_manager)}\")\n",
    "try:\n",
    "    print(f\"Test: {len(test_data_manager)}\")\n",
    "except:\n",
    "    print(\"No test data manager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fuse the data again (do not forget to close your active data managers!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_files_data_manager.fuse_train_test_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data_manager\n",
    "del validation_data_manager\n",
    "del test_data_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(many_files_data_manager))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the implemented functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # type: ignore\n",
    "import random\n",
    "import h5py # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you can check whether the implemented functions in this project work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling number of datapoints from signal- to target- frequency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would highly suggest to provide data where the signals don't need to be scaled to the frequencies of the data\n",
    "used to train the neural network.\n",
    "\n",
    "If there is no other option, then so be it. Here is a demonstration of the functions that will be applied to \n",
    "your data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Classification Frequency: 0.05 -> Target Frequency: 0.03333333333333333\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "Classification array:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "Classification array shape:  (15,)\n",
      "\n",
      "Scaled array:  [ 0  1  3  4  6  7  9 10 12 13]\n",
      "Scaled array shape:  (10,)\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Classification Frequency: 0.02 -> Target Frequency: 0.03333333333333333\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "Classification array:  [0 1 2 3 4 5 6 7 8]\n",
      "Classification array shape:  (9,)\n",
      "\n",
      "Scaled array:  [0 1 1 2 2 3 4 4 5 5 6 7 7 8 8]\n",
      "Scaled array shape:  (15,)\n"
     ]
    }
   ],
   "source": [
    "classification_array = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
    "classification_frequency = 1/20\n",
    "target_frequency = 1/30\n",
    "\n",
    "print(\"-\"*71)\n",
    "print(f\"Classification Frequency: {classification_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*71)\n",
    "print(\"\\nClassification array: \", classification_array)\n",
    "print(\"Classification array shape: \", classification_array.shape)\n",
    "\n",
    "reshaped_array = scale_classification_signal(\n",
    "        signal = classification_array, # type: ignore\n",
    "        signal_frequency = classification_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(\"\\nScaled array: \", reshaped_array)\n",
    "print(\"Scaled array shape: \", reshaped_array.shape)\n",
    "\n",
    "classification_array = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "classification_frequency = 1/50\n",
    "target_frequency = 1/30\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-\"*71)\n",
    "print(f\"Classification Frequency: {classification_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*71)\n",
    "print(\"\\nClassification array: \", classification_array)\n",
    "print(\"Classification array shape: \", classification_array.shape)\n",
    "\n",
    "reshaped_array = scale_classification_signal(\n",
    "        signal = classification_array, # type: ignore\n",
    "        signal_frequency = classification_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(\"\\nScaled array: \", reshaped_array)\n",
    "print(\"Scaled array shape: \", reshaped_array.shape)\n",
    "\n",
    "del reshaped_array, classification_array, classification_frequency, target_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Continuous Frequency: 3 -> Target Frequency: 4\n",
      "---------------------------------------------------------------------------\n",
      "Continuous array: [0 1 2 3 4 5] / [0. 1. 2. 3. 4. 5.]\n",
      "Continuous array shape:  (6,)\n",
      "\n",
      "Scaled array: [0 1 2 2 3 4 4 5] / [0.   0.75 1.5  2.25 3.   3.75 4.5  5.  ]\n",
      "Scaled array shape:  (8,)\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Continuous Frequency: 5 -> Target Frequency: 4\n",
      "---------------------------------------------------------------------------\n",
      "Continuous array: [0 1 2 3 4 5 6 7 8 9] / [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "Continuous array shape:  (10,)\n",
      "\n",
      "Scaled array: [0 1 2 4 5 6 8 9] / [0.   1.25 2.5  3.75 5.   6.25 7.5  8.75]\n",
      "Scaled array shape:  (8,)\n"
     ]
    }
   ],
   "source": [
    "continuous_array_int = np.array([0, 1, 2, 3, 4, 5])\n",
    "continuous_array_float = np.array([0, 1, 2, 3, 4, 5], dtype = float)\n",
    "continuous_frequency = 3\n",
    "target_frequency = 4\n",
    "\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous Frequency: {continuous_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous array: {continuous_array_int} / {continuous_array_float}\")\n",
    "print(\"Continuous array shape: \", continuous_array_int.shape)\n",
    "\n",
    "reshaped_array_int = interpolate_signal(\n",
    "        signal = continuous_array_int, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "reshaped_array_float = interpolate_signal(\n",
    "        signal = continuous_array_float, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(f\"\\nScaled array: {reshaped_array_int} / {reshaped_array_float}\")\n",
    "print(\"Scaled array shape: \", reshaped_array_int.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "continuous_array_int = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "continuous_array_float = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype = float)\n",
    "continuous_frequency = 5\n",
    "target_frequency = 4\n",
    "\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous Frequency: {continuous_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous array: {continuous_array_int} / {continuous_array_float}\")\n",
    "print(\"Continuous array shape: \", continuous_array_int.shape)\n",
    "\n",
    "reshaped_array_int = interpolate_signal(\n",
    "        signal = continuous_array_int, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "reshaped_array_float = interpolate_signal(\n",
    "        signal = continuous_array_float, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(f\"\\nScaled array: {reshaped_array_int} / {reshaped_array_float}\")\n",
    "print(\"Scaled array shape: \", reshaped_array_int.shape)\n",
    "\n",
    "del reshaped_array_int, reshaped_array_float, continuous_array_int, continuous_array_float, continuous_frequency, target_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting a signal which is too long for the neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A signal which is too short will be padded with zeros. No big deal. On the other hand: A signal which is too \n",
    "long will be splitted into multiple signals. To create more data, the 10 hour range will be shifted along the\n",
    "signal.\n",
    "\n",
    "This shift should not be too small, to create redundant data but also not too big, because the more data the \n",
    "better. So we try to find a shift size close to 1 hour, which lets us shift an integer amount of times\n",
    "easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding optimal shift size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal shift length for signal which is 2.5 hours longer than desired length of 10 hours: 0.833 hours\n"
     ]
    }
   ],
   "source": [
    "signal_length_addition_hours = 2.5\n",
    "desired_length_hours = 10\n",
    "\n",
    "optimal_shift_length = calculate_optimal_shift_length(\n",
    "        signal_length = (desired_length_hours + signal_length_addition_hours) * 3600, # type: ignore\n",
    "        desired_length = desired_length_hours*3600, \n",
    "        wanted_shift_length = 3600,\n",
    "        absolute_shift_deviation = 1800,\n",
    ")\n",
    "\n",
    "print(f\"Optimal shift length for signal which is {signal_length_addition_hours} hours longer than desired length of {desired_length_hours} hours: {round(optimal_shift_length/3600, 3)} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function to find optimal shift length is embedded in the following split funtion. The optimal\n",
    "shift size will be estimated for every signal individually.\n",
    "\n",
    "If there is no integer shift size in range, that lets you shift the signal so, that you perfectly enclose the\n",
    "last datapoints of the long signal, then the last shift will be altered so that it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift length: 15120\n",
      "Shift length: 3780.0 seconds\n",
      "Signal shape:  (174240,)\n",
      "Datapoints in NN: 144000\n",
      "Signals from splitting shape:  (3, 144000)\n"
     ]
    }
   ],
   "source": [
    "# Create random signal\n",
    "frequency = 4\n",
    "length_signal_seconds = 12.1 * 3600\n",
    "signal = np.random.rand(int(length_signal_seconds * frequency))\n",
    "\n",
    "# Only important parameters here:\n",
    "nn_signal_seconds = 10 * 3600\n",
    "shift_length_seconds = 3600\n",
    "absolute_shift_deviation_seconds = 1800\n",
    "\n",
    "signals_from_splitting, shift_length = split_long_signal(\n",
    "        signal = signal, # type: ignore\n",
    "        sampling_frequency = frequency,\n",
    "        target_frequency = frequency,\n",
    "        nn_signal_duration_seconds = nn_signal_seconds,\n",
    "        wanted_shift_length_seconds = shift_length_seconds,\n",
    "        absolute_shift_deviation_seconds = absolute_shift_deviation_seconds\n",
    "        )\n",
    "\n",
    "print(\"Shift length:\", shift_length)\n",
    "print(f\"Shift length: {shift_length / frequency} seconds\")\n",
    "print(\"Signal shape: \", signal.shape)\n",
    "print(f\"Datapoints in NN: {nn_signal_seconds * frequency}\")\n",
    "print(\"Signals from splitting shape: \", signals_from_splitting.shape)\n",
    "\n",
    "del signals_from_splitting, signal, shift_length, frequency, nn_signal_seconds, shift_length_seconds, absolute_shift_deviation_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting signals within dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dictionary:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: (174240,)\n",
      "RRI_frequency: 4\n",
      "MAD: (43560,)\n",
      "MAD_frequency: 1\n",
      "\n",
      "New dictionaries:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: (144000,)\n",
      "RRI_frequency: 4\n",
      "MAD: (36000,)\n",
      "MAD_frequency: 1\n",
      "\n",
      "ID: 1_shift_3780s_x1\n",
      "RRI: (144000,)\n",
      "RRI_frequency: 4\n",
      "MAD: (36000,)\n",
      "MAD_frequency: 1\n",
      "\n",
      "ID: 1_shift_3780s_x2\n",
      "RRI: (144000,)\n",
      "RRI_frequency: 4\n",
      "MAD: (36000,)\n",
      "MAD_frequency: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create random signal\n",
    "length_signal_seconds = 12.1 * 3600\n",
    "rri_frequency = 4\n",
    "mad_frequency = 1\n",
    "rri_signal = np.random.rand(int(length_signal_seconds * rri_frequency))\n",
    "mad_signal = np.random.rand(int(length_signal_seconds * mad_frequency))\n",
    "\n",
    "data_dict = {\n",
    "    \"ID\": \"1\",\n",
    "    \"RRI\": rri_signal,\n",
    "    \"RRI_frequency\": rri_frequency,\n",
    "    \"MAD\": mad_signal,\n",
    "    \"MAD_frequency\": mad_frequency,\n",
    "}\n",
    "\n",
    "new_dictionaries = split_signals_within_dictionary(\n",
    "    data_dict = data_dict,\n",
    "    id_key = \"ID\",\n",
    "    valid_signal_keys = [\"RRI\", \"MAD\"],\n",
    "    signal_frequencies = [rri_frequency, mad_frequency],\n",
    "    signal_target_frequencies = [rri_frequency, mad_frequency],\n",
    "    nn_signal_duration_seconds = 10 * 3600,\n",
    "    wanted_shift_length_seconds = 3600,\n",
    "    absolute_shift_deviation_seconds = 1800\n",
    ")\n",
    "\n",
    "print(\"Original dictionary:\")\n",
    "print(\"-\"*20)\n",
    "for key, value in data_dict.items():\n",
    "    if key == \"RRI\" or key == \"MAD\" or key == \"SLP\":\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "print(\"\\nNew dictionaries:\")\n",
    "print(\"-\"*20)\n",
    "for new_dict in new_dictionaries:\n",
    "    for key, value in new_dict.items():\n",
    "        if key == \"RRI\" or key == \"MAD\" or key == \"SLP\":\n",
    "            print(f\"{key}: {value.shape}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a .h5 - file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the available training datasets for our neural network model are stored in a .h5 file. So we need\n",
    "to be able to read it. These are the important operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random patient: 201949_1\n",
      "[0 1 2 3 5]\n",
      "\n",
      "key: slp\n",
      "Data shape: (865,)\n",
      "Data frequency: 0.03333333333333333\n",
      "Inverse data frequency: 30.0\n",
      "Data length: 25950.0 s\n",
      "\n",
      "key: rri\n",
      "Data shape: (103800,)\n",
      "Data frequency: 4\n",
      "Inverse data frequency: 0.25\n",
      "Data length: 25950.0 s\n"
     ]
    }
   ],
   "source": [
    "shhs_dataset = h5py.File(\"Raw_Data/SHHS_dataset.h5\", 'r')\n",
    "patients = list(shhs_dataset['slp'].keys()) # type: ignore\n",
    "\n",
    "random_patient = patients[np.random.randint(0, len(patients))]\n",
    "print(f\"Random patient: {random_patient}\")\n",
    "\n",
    "print(np.unique(shhs_dataset[\"slp\"][random_patient][:])) # type: ignore\n",
    "\n",
    "for key in [\"slp\", \"rri\"]:\n",
    "    print(f\"\\nkey: {key}\")\n",
    "\n",
    "    patient_data = shhs_dataset[key][random_patient][:] # type: ignore\n",
    "    print(f\"Data shape: {patient_data.shape}\") # type: ignore\n",
    "\n",
    "    data_freq = shhs_dataset[key].attrs[\"freq\"] # type: ignore\n",
    "    print(f\"Data frequency: {data_freq}\")\n",
    "    print(f\"Inverse data frequency: {1/data_freq}\") # type: ignore\n",
    "\n",
    "    print(f\"Data length: {patient_data.shape[0]/data_freq} s\") # type: ignore\n",
    "\n",
    "del shhs_dataset, patients, random_patient, key, patient_data, data_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide up a signal into overlapping windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardest thing about this is, that 'window_overlap' and 'datapoints_per_window' must be chosen so that\n",
    "the whole signal fits perfectly into n windows. \n",
    "\n",
    "Additionally, those values must be integers. This means that 'window_duration_seconds' and 'overlap_seconds'\n",
    "multiplied with 'target_fequency' as well as 'sampling_frequency' must be integers. (The features and the target labels\n",
    "must fit equally well into the windows, so that we can find the correlation between a feature- and target- window.)\n",
    "\n",
    "We have the RRI and MAD values as features and the sleep phase as target classification. As we will see,\n",
    "RRI and MAD values were recorded with an integer sampling frequency. While the sampling frequency of the \n",
    "sleep classification is 1/30. \n",
    "\n",
    "Finding window parameters that fullfill the conditions mentioned is easier than it sounds. We will always pass data\n",
    "to the neural network that is 10 hours long. Now, we just need to think in seconds and find integer values\n",
    "for 'window_duration_seconds' and 'overlap_seconds' that are a multiple of 30:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding optimal window_parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suitable window parameters for signal of length: 36000:\n",
      "-------------------------------------------------------\n",
      "Number of windows: 1025, Window size: 160, Overlap: 125.0\n",
      "Number of windows: 1026, Window size: 125, Overlap: 90.0\n",
      "Number of windows: 1055, Window size: 164, Overlap: 130.0\n",
      "Number of windows: 1056, Window size: 130, Overlap: 96.0\n",
      "Number of windows: 1087, Window size: 162, Overlap: 129.0\n",
      "Number of windows: 1088, Window size: 129, Overlap: 96.0\n",
      "Number of windows: 1121, Window size: 160, Overlap: 128.0\n",
      "Number of windows: 1122, Window size: 128, Overlap: 96.0\n",
      "Number of windows: 1157, Window size: 164, Overlap: 133.0\n",
      "Number of windows: 1158, Window size: 133, Overlap: 102.0\n",
      "Number of windows: 1196, Window size: 150, Overlap: 120.0\n",
      "Number of windows: 1197, Window size: 120, Overlap: 90.0\n"
     ]
    }
   ],
   "source": [
    "find_suitable_window_parameters(\n",
    "        signal_length = 10 * 3600,\n",
    "        number_windows_range = (1000, 1400),\n",
    "        window_size_range = (120, 180),\n",
    "        minimum_window_size_overlap_difference = 30\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our options are:\n",
    "\n",
    "Number of windows: 1196, Window size: 150, Overlap: 120.0 \\\n",
    "Number of windows: 1197, Window size: 120, Overlap: 90.0\n",
    "\n",
    "We will choose the latter, because we don't want the window_size to be too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When transforming a classification signal into windows, which is supposed to be the target in the neural \n",
    "network, then each window will only be represented by the most common sleep stage. If there is a tie\n",
    "between the labels, then the one with the highest priority will be chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "\n",
      "WARNING: No label found in priority order. Appending first label. Better terminate and recheck priority_order.\n",
      " Labels: [4]\n",
      "Signal shape: (1200,)\n",
      "Signal in windows shape: (1197,)\n"
     ]
    }
   ],
   "source": [
    "signal_length_seconds = 10 * 3600\n",
    "frequency = 1/30\n",
    "signal_length = int(signal_length_seconds * frequency)\n",
    "\n",
    "signal = np.array([random.randint(0, 5) for _ in range(signal_length)])\n",
    "\n",
    "signal_in_windows = signal_to_windows(\n",
    "    signal = signal, # type: ignore\n",
    "    datapoints_per_window = int(120 * frequency),\n",
    "    window_overlap = int(90 * frequency),\n",
    "    signal_type = \"target\",\n",
    "    priority_order = [0, 1, 2, 3, 5, -1]\n",
    "    )\n",
    "\n",
    "print(f\"Signal shape: {signal.shape}\")\n",
    "print(f\"Signal in windows shape: {signal_in_windows.shape}\")\n",
    "\n",
    "del signal, signal_in_windows, signal_length_seconds, frequency, signal_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal shape: (36000,)\n",
      "Signal in windows shape: (1197, 120)\n"
     ]
    }
   ],
   "source": [
    "signal = np.random.rand(36000)\n",
    "\n",
    "signal_in_windows = signal_to_windows(\n",
    "    signal = signal, # type: ignore\n",
    "    datapoints_per_window = 120,\n",
    "    window_overlap = 90,\n",
    "    signal_type = \"feature\"\n",
    "    )\n",
    "\n",
    "print(f\"Signal shape: {signal.shape}\")\n",
    "print(f\"Signal in windows shape: {signal_in_windows.shape}\")\n",
    "\n",
    "del signal, signal_in_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will actually be applied the transform a signal into windows. It will make sure\n",
    "that the data is passed correctly to the function mentioned above. \n",
    "\n",
    "This means it will:\n",
    "- check if 'number_nn_datapoints', 'datapoints_per_window' and 'window_overlap' are integers\n",
    "- check if 'datapoints_per_window' and 'window_overlap' perfectly fit into 'number_nn_datapoints'\n",
    "- compare length of provided signal to length of signal in nn ('number_nn_datapoints')\n",
    "    - if smaller: Pad with Zeros\n",
    "    - if bigger: Print warning, but continue by cropping last datapoints\n",
    "- check if signal transformed to windows has the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random array shape: (36000,)\n",
      "Reshaped array shape: (1197, 480)\n",
      "Random array shape: (1200,)\n",
      "Reshaped array shape: (1197,)\n"
     ]
    }
   ],
   "source": [
    "random_array = np.random.rand(36000)\n",
    "reshaped_array = reshape_signal_to_overlapping_windows(\n",
    "    signal = random_array, # type: ignore\n",
    "    sampling_frequency = 4,\n",
    "    target_frequency = 4, \n",
    "    number_windows = 1197, \n",
    "    window_duration_seconds = 120, \n",
    "    overlap_seconds = 90,\n",
    "    signal_type = \"feature\",\n",
    "    nn_signal_duration_seconds = 10*3600,\n",
    "    )\n",
    "\n",
    "print(f\"Random array shape: {random_array.shape}\")\n",
    "print(f\"Reshaped array shape: {reshaped_array.shape}\")\n",
    "\n",
    "random_array = np.array([random.randint(0, 3) for _ in range(int(36000/30))])\n",
    "reshaped_array = reshape_signal_to_overlapping_windows(\n",
    "    signal = random_array, # type: ignore\n",
    "    sampling_frequency = 1/30,\n",
    "    target_frequency = 1/30, \n",
    "    number_windows = 1197, \n",
    "    window_duration_seconds = 120, \n",
    "    overlap_seconds = 90,\n",
    "    signal_type = \"target\",\n",
    "    nn_signal_duration_seconds = 10*3600,\n",
    "    )\n",
    "\n",
    "print(f\"Random array shape: {random_array.shape}\")\n",
    "print(f\"Reshaped array shape: {reshaped_array.shape}\")\n",
    "\n",
    "del random_array, reshaped_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alter Sleep Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function makes sure to keep labels unfiform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2 -1  0  1  2  3  4  5  6  7]\n",
      "[-1 -1  0  0  1  2 -1  3 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "slp = np.array([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7])\n",
    "print(slp)\n",
    "\n",
    "current_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "desired_labels = {\"wake\": 0, \"LS\": 1, \"DS\": 2, \"REM\": 3, \"artifect\": -1}\n",
    "\n",
    "print(alter_slp_labels(\n",
    "        slp_labels = slp, # type: ignore\n",
    "        current_labels = current_labels,\n",
    "        desired_labels = desired_labels,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['light_sleep' 'deep_sleep' 'deep_sleep_2' 'WAKE' 'REM' 'bla' 'blub']\n",
      "['1' '2' '2' '0' '3' '-1' '-1']\n"
     ]
    }
   ],
   "source": [
    "slp = np.array([\"light_sleep\", \"deep_sleep\", \"deep_sleep_2\", \"WAKE\", \"REM\", \"bla\", \"blub\"])\n",
    "print(slp)\n",
    "\n",
    "current_labels = {\"wake\": [\"WAKE\"], \"LS\": [\"light_sleep\"], \"DS\": [\"deep_sleep\", \"deep_sleep_2\"], \"REM\": [\"REM\"], \"artifect\": [\"other\"]}\n",
    "desired_labels = {\"wake\": 0, \"LS\": 1, \"DS\": 2, \"REM\": 3, \"artifect\": -1}\n",
    "\n",
    "print(alter_slp_labels(\n",
    "        slp_labels = slp, # type: ignore\n",
    "        current_labels = current_labels,\n",
    "        desired_labels = desired_labels,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2 -1  0  1  2  3  4  5  6  7]\n",
      "[-2  0  0  0  1  2  3  3  0  6]\n"
     ]
    }
   ],
   "source": [
    "slp = np.array([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7])\n",
    "print(slp)\n",
    "\n",
    "slp[slp>=1] = slp[slp>=1] - 1\n",
    "slp[slp==4] = 3\n",
    "slp[slp==5] = 0\n",
    "slp[slp==-1] = 0 # set artifact as wake stage\n",
    "\n",
    "print(slp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
