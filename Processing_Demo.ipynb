{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Johannes Peter Knoll\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Within this notebook you will learn and test everything that was implemented to preprocess the data\n",
    "for the neural network.\n",
    "\n",
    "Note:   This notebook is rather for those who want to make sure everything works correctly. It is very thorough\n",
    "        and therefore unnecessary if you only want to get a quick start into the predictions. If that is the case, head\n",
    "        to 'Classification_Demo.ipynb'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thorough Demonstration of 'dataset_processing.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The autoreload extension allows you to tweak the code in the imported modules\n",
    "# and rerun cells to reflect the changes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we demonstrate the implemented class that helps you to manage the data you want to pass to the\n",
    "neural network model.\n",
    "\n",
    "The usage of this class is not required, you could also just use the implemented functions on your data, which\n",
    "are explained in the next section.\n",
    "\n",
    "I still would highly recommend using this class, as it is able to handle large data in a memory saving way\n",
    "and makes it very easy to check and process your data, so that it can be passed to the model easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_processing import *\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Database itself is just a .pkl file that contains multiple dictionaries. The first dictionary is always\n",
    "the file information, while the following will be the Database's datapoints. Each datapoint needs a unique ID\n",
    "(key: \"ID\") and can contain the following signals: \n",
    "- RRI (key: \"RRI\")\n",
    "- MAD (key: \"MAD\")\n",
    "- Sleep-Labels (key: \"SLP\")\n",
    "- predicted Sleep-Labels (key: \"SLP_predicted\")\n",
    "- predicted individual probabilities for every sleep stage (key: \"SLP_predicted_probability\")\n",
    "\n",
    "The file information dictionary holds parameters that apply to every datapoint. This can be parameters that\n",
    "affect how the data is processed or uniform informations like the sampling frequencies for each signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not know if this needs to be said, but NEVER MANUALLY CHANGE THE CLASS ATTRIBUTES when data was already\n",
    "added.\n",
    "\n",
    "If you have different requirements for the uniform frequency, signal length or any of the other parameters,\n",
    "that's fine. But change them before you save data to it: See section: \"Change File Information\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Database (.pkl - file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initializing a database (calling 'SleepDataManager' on non-existent path to .pkl file) the class will\n",
    "automatically write the first dictionary to it, which functions as an information on the data properties\n",
    "within the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRI_frequency: 4\n",
      "MAD_frequency: 1\n",
      "SLP_frequency: 0.03333333333333333\n",
      "SLP_predicted_frequency: 0.03333333333333333\n",
      "RRI_inlier_interval: [0.3, 2.0]\n",
      "MAD_inlier_interval: [None, None]\n",
      "sleep_stage_label: {'wake': 0, 'LS': 1, 'DS': 2, 'REM': 3, 'artifect': 0}\n",
      "signal_length_seconds: 36000\n",
      "wanted_shift_length_seconds: 5400\n",
      "absolute_shift_deviation_seconds: 1800\n",
      "signal_split_reversed: False\n",
      "train_val_test_split_applied: False\n",
      "main_file_path: Processing_Demonstration/demo_file_info_change.pkl\n",
      "train_file_path: Processing_Demonstration/demo_file_info_change_training_pid.pkl\n",
      "validation_file_path: Processing_Demonstration/demo_file_info_change_validation_pid.pkl\n",
      "test_file_path: Processing_Demonstration/demo_file_info_change_test_pid.pkl\n"
     ]
    }
   ],
   "source": [
    "a_data_manager = SleepDataManager(file_path = \"Processing_Demonstration/demo_file_info_change.pkl\")\n",
    "file_information = a_data_manager.file_info\n",
    "\n",
    "for key in file_information.keys():\n",
    "    print(f\"{key}: {file_information[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't mind all the different keys yet. Necessary ones will be explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing File Information\n",
    "\n",
    "File information can be easily changed but is only possible as long as no data was added to the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file information:\n",
      "\n",
      "RRI_frequency: 2\n",
      "SLP_frequency: 2\n",
      "\n",
      "File information in new instance on same path:\n",
      "\n",
      "RRI_frequency: 2\n",
      "SLP_frequency: 2\n",
      "\n",
      "File information in new instance on different path:\n",
      "\n",
      "RRI_frequency: 4\n",
      "SLP_frequency: 0.03333333333333333\n"
     ]
    }
   ],
   "source": [
    "# change the file information\n",
    "new_file_info = {\"RRI_frequency\": 2, \"SLP_frequency\": 2}\n",
    "a_data_manager.change_file_information(new_file_info)\n",
    "\n",
    "print(\"Updated file information:\\n\")\n",
    "file_information = a_data_manager.file_info\n",
    "for key in new_file_info.keys():\n",
    "    print(f\"{key}: {file_information[key]}\")\n",
    "\n",
    "del a_data_manager, file_information\n",
    "\n",
    "# the change in file information is saved and can be accessed by another instance of SleepDataManager\n",
    "another_data_manager = SleepDataManager(file_path = \"Processing_Demonstration/demo_file_info_change.pkl\")\n",
    "file_information = another_data_manager.file_info\n",
    "\n",
    "print(\"\\nFile information in new instance on same path:\\n\")\n",
    "for key in new_file_info.keys():\n",
    "    print(f\"{key}: {file_information[key]}\")\n",
    "\n",
    "del another_data_manager, file_information\n",
    "os.remove(\"Processing_Demonstration/demo_file_info_change.pkl\")\n",
    "\n",
    "some_data_manager = SleepDataManager(file_path = \"Processing_Demonstration/messing_around.pkl\")\n",
    "file_information = some_data_manager.file_info\n",
    "\n",
    "print(\"\\nFile information in new instance on different path:\\n\")\n",
    "for key in new_file_info.keys():\n",
    "    print(f\"{key}: {file_information[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data\n",
    "\n",
    "The most processing is already done during saving. To keep the data uniform you must always provide\n",
    "the sampling frequency for each signal \n",
    "(keys: \"RRI_frequency\", \"MAD_frequency\", \"SLP_frequency\", \"SLP_predicted_frequency\").\n",
    "\n",
    "Operations that might be happening to the data you try to save:\n",
    "- scale number of datapoints in signal so that signal frequency matches uniform database signal frequency\n",
    "- alter sleep labels\n",
    "- remove RRI and/or MAD outliers\n",
    "- split signal into multiple signals if signal is longer than the uniform maximum signal length: 'signal_length_seconds'\n",
    "\n",
    "To add a SLP signal, you must additionally provide the key: \"sleep_stage_label\".\n",
    "This dictionary is supposed to tell which entries correspond to which sleep stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleep stage labels in shhs dataset:\n",
    "# \"wake\": 0,    \"N1\": 1,    \"N2\": 2,    \"N3\": 3,    \"REM\": 5,   \"artifect\": \"other integers\"\n",
    "\n",
    "# in the nn we only divide between wake, LS, DS, REM, and artifect. Above, N1 must be redeclared as \"wake\", \n",
    "# N2 as \"LS\" and N3 as \"DS\":\n",
    "shhs_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will pass the worst possible data to the database: differing sampling frequencies and overlength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First datapoints of RRI signal: [2. 5. 5. 4. 2. 3. 1. 5. 3. 4.] (shape: (261360,))\n",
      "First datapoints of RRI signal scaled: [2.  5.  4.  2.5 1.  4.  4.  3.  3.  4. ] (shape: (174240,))\n",
      "First datapoints of MAD signal: [4 3 5 5 3 5 2 4 5 5] (shape: 87120)\n",
      "First datapoints of MAD signal scaled: [4 5 3 2 5 3 4 2 3 5] (shape: 43560)\n",
      "First datapoints of SLP signal: [3, 1, 5, 3, 4, 5, 1, 3, 4, 4] (shape: 2178)\n",
      "First datapoints of SLP signal scaled: [3 1 3 4 1 3 4 5 5 1] (shape: 1452)\n",
      "First datapoints of scaled SLP signal altered: [2 0 2 0 0 2 0 3 3 0] (shape: 1452)\n"
     ]
    }
   ],
   "source": [
    "# creating signal with different sampling frequencies and overlength:\n",
    "signal_time_in_seconds = 12.1 * 3600\n",
    "rri_frequency = 6\n",
    "mad_frequency = 2\n",
    "slp_frequency = 1/20\n",
    "\n",
    "# creating signals and printing manually scaled versions\n",
    "rri_signal = np.array([random.randint(1, 5) for i in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "print(f\"First datapoints of RRI signal: {rri_signal[:10]} (shape: {rri_signal.shape})\")\n",
    "interpolate_rri = interpolate_signal(rri_signal, rri_frequency, 4) # type: ignore\n",
    "print(f\"First datapoints of RRI signal scaled: {interpolate_rri[:10]} (shape: {interpolate_rri.shape})\")\n",
    "mad_signal = np.array([random.randint(1, 5) for i in range(int(signal_time_in_seconds * mad_frequency))])\n",
    "print(f\"First datapoints of MAD signal: {mad_signal[:10]} (shape: {len(mad_signal)})\")\n",
    "interpolate_mad = interpolate_signal(mad_signal, mad_frequency, 1) # type: ignore\n",
    "print(f\"First datapoints of MAD signal scaled: {interpolate_mad[:10]} (shape: {len(interpolate_mad)})\")\n",
    "slp_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "print(f\"First datapoints of SLP signal: {slp_signal[:10]} (shape: {len(slp_signal)})\")\n",
    "scaled_slp = scale_classification_signal(slp_signal, slp_frequency, 1/30) # type: ignore\n",
    "print(f\"First datapoints of SLP signal scaled: {scaled_slp[:10]} (shape: {len(scaled_slp)})\")\n",
    "\n",
    "random_sleep_stage_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "altered_scaled_slp = alter_slp_labels(scaled_slp, random_sleep_stage_labels, desired_labels = {\"wake\": 0, \"LS\": 1, \"DS\": 2, \"REM\": 3, \"artifect\": 0}) # type: ignore\n",
    "print(f\"First datapoints of scaled SLP signal altered: {altered_scaled_slp[:10]} (shape: {len(altered_scaled_slp)})\")\n",
    "\n",
    "new_datapoint = {\n",
    "    \"ID\": \"4\",\n",
    "    \"RRI\": rri_signal,\n",
    "    \"RRI_frequency\": rri_frequency,\n",
    "    \"MAD\": mad_signal,\n",
    "    \"MAD_frequency\": mad_frequency,\n",
    "    \"SLP\": slp_signal,\n",
    "    \"SLP_frequency\": slp_frequency,\n",
    "    \"sleep_stage_label\": random_sleep_stage_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the new datapoint\n",
    "some_data_manager.save(copy.deepcopy(new_datapoint), overwrite_id=True, unique_id=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "ID 4\n",
      "RRI [2.  2.  2.  2.  1.  2.  2.  1.5 2.  2. ] (144000,)\n",
      "MAD [4 5 3 2 5 3 4 2 3 5] (36000,)\n",
      "SLP [2 0 2 0 0 2 0 3 3 0] (1200,)\n",
      "shift_length_seconds 3780\n",
      "----------------------------------------------------------------------\n",
      "ID 4_shift_x1\n",
      "RRI [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.] (144000,)\n",
      "MAD [1 4 2 2 3 4 3 1 2 5] (36000,)\n",
      "SLP [3 0 3 1 3 3 1 2 0 0] (1200,)\n",
      "shift_length_seconds 3780\n",
      "----------------------------------------------------------------------\n",
      "ID 4_shift_x2\n",
      "RRI [2.  2.  2.  2.  1.  2.  1.  1.5 1.  2. ] (144000,)\n",
      "MAD [1 1 1 1 4 5 2 5 4 3] (36000,)\n",
      "SLP [3 3 3 3 2 1 1 0 2 2] (1200,)\n",
      "shift_length_seconds 3780\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print the data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*70)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "            print(key, dict[key][:10], dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided datapoint was too long and its signal sampling frequencies did not match. Therefore, the\n",
    "signals were extrapolated and the datapoint (\"4\") was split into multiple (\"4\", \"4_shift_x1\", \"4_shift_x2\"),\n",
    "by shifting the wanted length (file_info: signal_length_seconds) by \"shift_length_seconds\" along the\n",
    "datapoint.\n",
    "\n",
    "Additionally, some of the RRI values were outside of the 'RRI_inlier_interval' (see file information) and were\n",
    "therefore adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding information to already existing datapoints\n",
    "\n",
    "The ultimate goal is either using the data to train the neural network or use the neural network on the data\n",
    "to predict the sleep stages. For the second case you might want to store the predictions in your database \n",
    "after passing the data to the neural network (key: \"SLP_predicted\", \"SLP_predicted_probability\"):\n",
    "\n",
    "To add or overwrite an exisiting signal in the database, just set the optional argument \"overwrite_id\"\n",
    "to \"True\" when saving. If set to \"False\" and the ID already exists in the database, it will discard the data \n",
    "you are trying to save and raise an error.\n",
    "\n",
    "Attention: For this argument to have an effect, the optional argument \"unique_id\" must be set to \"False\". \n",
    "Further information below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform frequency of predicted sleep stages (before saving the first): 0.03333333333333333\n",
      "-------------------------------------------------------------------------------------\n",
      "ID 4\n",
      "RRI [2.  2.  2.  2.  1.  2.  2.  1.5 2.  2. ] (144000,)\n",
      "MAD [4 5 3 2 5 3 4 2 3 5] (36000,)\n",
      "SLP [2 0 2 0 0 2 0 3 3 0] (1200,)\n",
      "shift_length_seconds 3780\n",
      "SLP_predicted [1 0 1 2 1 2 3 1 1 1] (300,)\n",
      "SLP_predicted_probability [1 2 2 3 2 0 2 2 1 2] (300,)\n",
      "-------------------------------------------------------------------------------------\n",
      "ID 4_shift_x1\n",
      "RRI [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.] (144000,)\n",
      "MAD [1 4 2 2 3 4 3 1 2 5] (36000,)\n",
      "SLP [3 0 3 1 3 3 1 2 0 0] (1200,)\n",
      "shift_length_seconds 3780\n",
      "-------------------------------------------------------------------------------------\n",
      "ID 4_shift_x2\n",
      "RRI [2.  2.  2.  2.  1.  2.  1.  1.5 1.  2. ] (144000,)\n",
      "MAD [1 1 1 1 4 5 2 5 4 3] (36000,)\n",
      "SLP [3 3 3 3 2 1 1 0 2 2] (1200,)\n",
      "shift_length_seconds 3780\n",
      "-------------------------------------------------------------------------------------\n",
      "Uniform frequency of predicted sleep stages (after saving): 0.03333333333333333\n"
     ]
    }
   ],
   "source": [
    "datapoint_additions = {\n",
    "    \"ID\": \"4\",\n",
    "    \"SLP_predicted\": np.array([random.randint(0, 3) for _ in range(int(36000 * 1/120))]),\n",
    "    \"SLP_predicted_probability\": np.array([random.randint(0, 3) for _ in range(int(36000 * 1/120))]),\n",
    "    \"SLP_predicted_frequency\": 1/120\n",
    "}\n",
    "\n",
    "print(\"Uniform frequency of predicted sleep stages (before saving the first):\", some_data_manager.file_info[\"SLP_predicted_frequency\"])\n",
    "\n",
    "# saving the additional signals to datapoint\n",
    "some_data_manager.save(copy.deepcopy(datapoint_additions), overwrite_id=True, unique_id=False)\n",
    "\n",
    "# print the data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*85)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "            print(key, dict[key][:10], dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*85)\n",
    "\n",
    "print(\"Uniform frequency of predicted sleep stages (after saving):\", some_data_manager.file_info[\"SLP_predicted_frequency\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed up data saving\n",
    "\n",
    "During the saving process, every id is checked to ensure every id is unique. This takes a while to compute if \n",
    "you want to add many datapoints. To speed up the process, you can check once if every id is unique and \n",
    "afterwards skip the checking when adding the datapoints to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All IDs are unique.\n"
     ]
    }
   ],
   "source": [
    "list_of_ids = [\"101\", \"102\", \"103\"]\n",
    "\n",
    "some_data_manager.check_if_ids_are_unique(list_of_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all ids are unique you can use: some_data_manager(..., unique_id=True) and the saving will be much faster!\n",
    "(Making \"overwrite_id\" obsolete.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "ID 4\n",
      "RRI [2. 2. 2. 2. 1.] (144000,)\n",
      "MAD [4 5 3 2 5] (36000,)\n",
      "SLP [2 0 2 0 0] (1200,)\n",
      "shift_length_seconds 3780\n",
      "SLP_predicted [1 0 1 2 1] (300,)\n",
      "SLP_predicted_probability [1 2 2 3 2] (300,)\n",
      "----------------------------------------------------------------------\n",
      "ID 4_shift_x1\n",
      "RRI [2. 2. 2. 2. 2.] (144000,)\n",
      "MAD [1 4 2 2 3] (36000,)\n",
      "SLP [3 0 3 1 3] (1200,)\n",
      "shift_length_seconds 3780\n",
      "----------------------------------------------------------------------\n",
      "ID 4_shift_x2\n",
      "RRI [2. 2. 2. 2. 1.] (144000,)\n",
      "MAD [1 1 1 1 4] (36000,)\n",
      "SLP [3 3 3 3 2] (1200,)\n",
      "shift_length_seconds 3780\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print the data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*70)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\", \"SLP_windows\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "            print(key, dict[key][0:5], dict[key].shape)\n",
    "        elif key in [\"RRI_windows\", \"MAD_windows\"]:\n",
    "            print(key, dict[key][0][0:5], dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Data can be loaded in multiple ways using a string or an integer. \n",
    "\n",
    "If it's an integer, it will treat it as position in the database and return the whole data dictionary. \\\n",
    "If it's a string that equals a key in the data dictionaries, it will return all entities of that specific key in the database. \\\n",
    "If it's a different string, then it will treat it as ID and look for a match. Equal to index, it will return\n",
    "the whole dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': '4_shift_x1', 'RRI': array([2., 2., 2., ..., 2., 2., 1.]), 'MAD': array([1, 4, 2, ..., 1, 1, 2]), 'SLP': array([3, 0, 3, ..., 2, 3, 0]), 'shift_length_seconds': 3780}\n"
     ]
    }
   ],
   "source": [
    "loaded_data = some_data_manager.load(1)\n",
    "# loaded_data = some_data_manager[1] # same as above\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': '4_shift_x1', 'RRI': array([2., 2., 2., ..., 2., 2., 1.]), 'MAD': array([1, 4, 2, ..., 1, 1, 2]), 'SLP': array([3, 0, 3, ..., 2, 3, 0]), 'shift_length_seconds': 3780}\n"
     ]
    }
   ],
   "source": [
    "loaded_data = some_data_manager.load(\"4_shift_x1\")\n",
    "# loaded_data = some_data_manager[\"4_shift_x1\"] # same as above\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2. , 2. , 2. , ..., 2. , 2. , 1.5]), array([2., 2., 2., ..., 2., 2., 1.]), array([2. , 2. , 2. , ..., 1.5, 1. , 2. ])]\n"
     ]
    }
   ],
   "source": [
    "loaded_data = some_data_manager.load(\"RRI\")\n",
    "# loaded_data = some_data_manager[\"RRI\"] # same as above\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Data\n",
    "\n",
    "Removing takes the same argument as loading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting a signal from all entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "ID 4\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "shift_length_seconds 3780\n",
      "SLP_predicted (300,)\n",
      "SLP_predicted_probability (300,)\n",
      "--------------------\n",
      "ID 4_shift_x1\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "shift_length_seconds 3780\n",
      "--------------------\n",
      "ID 4_shift_x2\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "shift_length_seconds 3780\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "some_data_manager.remove(\"RRI\")\n",
    "\n",
    "# print all data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*20)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "            print(key, dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting an entry by ID (If a signal was splitted and one of the ID's is being removed, all other will be \n",
    "removed as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some data\n",
    "new_datapoint_2 = copy.deepcopy(new_datapoint)\n",
    "new_datapoint_2[\"ID\"] = \"5\"\n",
    "some_data_manager.save(new_datapoint_2, overwrite_id=True, unique_id=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "ID 5\n",
      "RRI (144000,)\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "shift_length_seconds 3780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "ID 5_shift_x1\n",
      "RRI (144000,)\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "shift_length_seconds 3780\n",
      "--------------------\n",
      "ID 5_shift_x2\n",
      "RRI (144000,)\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "shift_length_seconds 3780\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "some_data_manager.remove(\"4_shift_x1\")\n",
    "\n",
    "# print all data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*20)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "            print(key, dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing by index works analogous to removing by ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some data\n",
    "new_datapoint_2 = copy.deepcopy(new_datapoint)\n",
    "new_datapoint_2[\"ID\"] = \"6\"\n",
    "some_data_manager.save(new_datapoint_2, overwrite_id=True, unique_id=False)\n",
    "del new_datapoint_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "ID 6\n",
      "RRI (144000,)\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "shift_length_seconds 3780\n",
      "------------------------------\n",
      "ID 6_shift_x1\n",
      "RRI (144000,)\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "shift_length_seconds 3780\n",
      "------------------------------\n",
      "ID 6_shift_x2\n",
      "RRI (144000,)\n",
      "MAD (36000,)\n",
      "SLP (1200,)\n",
      "shift_length_seconds 3780\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "some_data_manager.remove(0)\n",
    "\n",
    "# print all data\n",
    "for dict in some_data_manager:\n",
    "    print(\"-\"*30)\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP\"]:\n",
    "            print(key, dict[key].shape)\n",
    "        else:\n",
    "            print(key, dict[key])\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's restore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data_manager.remove(0)\n",
    "some_data_manager.save(copy.deepcopy(new_datapoint), overwrite_id=True, unique_id=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over Database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4_shift_x1\n",
      "4_shift_x2\n"
     ]
    }
   ],
   "source": [
    "for datapoint in some_data_manager:\n",
    "    print(datapoint[\"ID\"])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if datapoint with certain ID is in database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoint with \"ID\" = 4 is in the data manager\n"
     ]
    }
   ],
   "source": [
    "if \"4\" in some_data_manager:\n",
    "    print(\"Datapoint with \\\"ID\\\" = 4 is in the data manager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: Processing_Demonstration/messing_around.pkl\n",
      "file_info: {'RRI_frequency': 4, 'MAD_frequency': 1, 'SLP_frequency': 0.03333333333333333, 'SLP_predicted_frequency': 0.03333333333333333, 'RRI_inlier_interval': [0.3, 2.0], 'MAD_inlier_interval': [None, None], 'sleep_stage_label': {'wake': '0', 'LS': '1', 'DS': '2', 'REM': '3', 'artifect': '0'}, 'signal_length_seconds': 36000, 'wanted_shift_length_seconds': 5400, 'absolute_shift_deviation_seconds': 1800, 'signal_split_reversed': False, 'train_val_test_split_applied': False, 'main_file_path': 'Processing_Demonstration/messing_around.pkl', 'train_file_path': 'Processing_Demonstration/messing_around_training_pid.pkl', 'validation_file_path': 'Processing_Demonstration/messing_around_validation_pid.pkl', 'test_file_path': 'Processing_Demonstration/messing_around_test_pid.pkl'}\n"
     ]
    }
   ],
   "source": [
    "print(some_data_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-, Validation-, Test- Split\n",
    "\n",
    "Of course, we aim to train a machine learning model with the data handled by this class. So, we want to\n",
    "be able to separate the data into training-, validation- and test- pids.\n",
    "\n",
    "First, let's create a new file and add some more data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints in file: 100\n"
     ]
    }
   ],
   "source": [
    "many_files_data_manager = SleepDataManager(file_path = \"Processing_Demonstration/Data.pkl\")\n",
    "\n",
    "add_number_datapoints = 100\n",
    "\n",
    "# optimal signal (fitting sampling frequencies and length):\n",
    "signal_time_in_seconds = 10 * 3600\n",
    "rri_frequency = 4\n",
    "mad_frequency = 1\n",
    "slp_frequency = 1/30\n",
    "\n",
    "random_sleep_stage_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "\n",
    "for i in range(add_number_datapoints):\n",
    "    rri_signal = np.array([random.randint(1, 5) for i in range(int(signal_time_in_seconds * rri_frequency))], dtype=np.float64)\n",
    "    mad_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * mad_frequency))]\n",
    "    slp_signal = [random.randint(1, 5) for i in range(int(signal_time_in_seconds * slp_frequency))]\n",
    "\n",
    "    decide_what_data_to_add = random.randint(0, 2)\n",
    "\n",
    "    if decide_what_data_to_add == 0:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"MAD\": mad_signal,\n",
    "            \"MAD_frequency\": mad_frequency,\n",
    "            \"SLP\": slp_signal,\n",
    "            \"SLP_frequency\": slp_frequency,\n",
    "            \"sleep_stage_label\": random_sleep_stage_labels\n",
    "        } # optimal data (rri and mad to slp)\n",
    "    elif decide_what_data_to_add == 1:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"MAD\": mad_signal,\n",
    "            \"MAD_frequency\": mad_frequency,\n",
    "        } # invalid data (no target: slp)\n",
    "    else:\n",
    "        new_datapoint = {\n",
    "            \"ID\": str(i),\n",
    "            \"RRI\": rri_signal,\n",
    "            \"RRI_frequency\": rri_frequency,\n",
    "            \"SLP\": slp_signal,\n",
    "            \"SLP_frequency\": slp_frequency,\n",
    "            \"sleep_stage_label\": random_sleep_stage_labels\n",
    "        } # only rri to slp\n",
    "    \n",
    "    many_files_data_manager.save(new_datapoint, overwrite_id=False)\n",
    "\n",
    "print(f\"Number of datapoints in file: {len(many_files_data_manager)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending whether test_size is provided or None we can create separate files where training-, validation- and \n",
    "test- data or just training- and validation- data is stored:\n",
    "\n",
    "Data that can not be used to train the network (i.e. missing \"RRI\" and \"SLP\") will be left in the main file. \n",
    "        \n",
    "As we can manage data with \"RRI\" and \"MAD\" and data with \"RRI\" only, the algorithm makes sure\n",
    "that only one of the two types of data is used (the one with more samples). The other type will \n",
    "be left in the main file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention: 26 datapoints with MAD signal will be left in the main file.\n",
      "\n",
      "Distributing 80.0% / 10.0% / 10.0% of datapoints into training / validation / test pids, respectively:\n",
      "   ✅: 100.0% [█████████████████████████] 100 / 100 | 0.0s / 0.0s (0.0s/it) |"
     ]
    }
   ],
   "source": [
    "many_files_data_manager.separate_train_test_validation(\n",
    "    train_size = 0.8, \n",
    "    validation_size = 0.1, \n",
    "    test_size = 0.1, \n",
    "    random_state = None, \n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual files can be accessed by another instance of this class. \n",
    "\n",
    "ATTENTION:  \n",
    "\n",
    "-   The instances on all files will have reduced functionality from now on. As the data should\n",
    "    be fully prepared for the network now, the instances are designed to only load data and\n",
    "    not save or edit it.\n",
    "\n",
    "-   The functionality of the instance on the main file is not as restricted as the ones on the\n",
    "    training, validation, and test files. The main file instance can additionally save data\n",
    "    (only to main file, won't be forwarded to training, validation, or test files), reshuffle \n",
    "    the data in the secondary files or pull them back into the main file for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing training-, validation- and test- data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_file_info = many_files_data_manager.file_info\n",
    "\n",
    "train_data_manager = SleepDataManager(file_path = main_file_info[\"train_file_path\"])\n",
    "validation_data_manager = SleepDataManager(file_path = main_file_info[\"validation_file_path\"])\n",
    "test_data_manager = SleepDataManager(file_path = main_file_info[\"test_file_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each dataset:\n",
      "------------------------------\n",
      "Main: 64\n",
      "Train: 28\n",
      "Validation: 4\n",
      "Test: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of each dataset:\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Main: {len(many_files_data_manager)}\")\n",
    "print(f\"Train: {len(train_data_manager)}\")\n",
    "print(f\"Validation: {len(validation_data_manager)}\")\n",
    "print(f\"Test: {len(test_data_manager)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main file info is the same as train, validation and test file info!\n"
     ]
    }
   ],
   "source": [
    "train_file_info = train_data_manager.file_info\n",
    "validation_file_info = validation_data_manager.file_info\n",
    "test_file_info = test_data_manager.file_info\n",
    "\n",
    "equal = True\n",
    "for key in main_file_info.keys():\n",
    "    if main_file_info[key] != train_file_info[key] or main_file_info[key] != validation_file_info[key] or main_file_info[key] != test_file_info[key]:\n",
    "        equal = False\n",
    "        break\n",
    "\n",
    "if equal:\n",
    "    print(\"Main file info is the same as train, validation and test file info!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can always reshuffle the data again from the main file manager (let's assign different \n",
    "arguments to see that something happened):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention: 26 datapoints with MAD signal will be left in the main file.\n",
      "\n",
      "Distributing 50.0% / 50.0% of datapoints into training / validation pids, respectively:\n",
      "   ✅: 100.0% [█████████████████████████] 100 / 100 | 0.0s / 0.0s (0.0s/it) |"
     ]
    }
   ],
   "source": [
    "many_files_data_manager.separate_train_test_validation(\n",
    "    train_size = 0.5, \n",
    "    validation_size = 0.5, \n",
    "    test_size = None, \n",
    "    random_state = None, \n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each dataset:\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main: 64\n",
      "Train: 18\n",
      "Validation: 18\n",
      "No test data manager\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of each dataset:\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Main: {len(many_files_data_manager)}\")\n",
    "print(f\"Train: {len(train_data_manager)}\")\n",
    "print(f\"Validation: {len(validation_data_manager)}\")\n",
    "try:\n",
    "    print(f\"Test: {len(test_data_manager)}\")\n",
    "except:\n",
    "    print(\"No test data manager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also fuse the data again (do not forget to close your active data managers!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_files_data_manager.fuse_train_test_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data_manager\n",
    "del validation_data_manager\n",
    "del test_data_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(many_files_data_manager))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversing Signal Split\n",
    "\n",
    "After you added predicted sleep stages to the database, you might want to reverse the signal split that was \n",
    "applied to the data during the saving process:\n",
    "\n",
    "Calling the function will combine all signals, including the predicted sleep stages, providing you with\n",
    "multiple results for the sleep stage of the overlapping parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "ID: 0, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 0_shift_x1, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 0_shift_x2, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 1, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 1_shift_x1, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 1_shift_x2, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 2, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 2_shift_x1, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 2_shift_x2, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 3, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 3_shift_x1, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 3_shift_x2, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "ID: 4, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "RRI [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19.]\n",
      "MAD [0 1 2 3 4 5 6 7 8 9]\n",
      "SLP_predicted [39 40 41 42 43]\n",
      "SLP_predicted_probability [[0.82 0.94 0.29 0.03]\n",
      " [0.95 0.09 0.09 0.65]\n",
      " [0.69 0.45 0.34 0.75]\n",
      " [0.68 0.39 0.65 0.27]\n",
      " [0.24 0.1  0.26 0.59]]\n",
      "------------------------------------------------------------------------------------------\n",
      "ID: 4_shift_x1, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "RRI [12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29.\n",
      " 30. 31.]\n",
      "MAD [ 6  7  8  9 10 11 12 13 14 15]\n",
      "SLP_predicted [42 43 44 45 46]\n",
      "SLP_predicted_probability [[0.68 0.39 0.65 0.27]\n",
      " [0.24 0.1  0.26 0.59]\n",
      " [0.42 0.89 0.32 0.65]\n",
      " [0.35 0.72 0.08 0.46]\n",
      " [0.98 0.37 0.34 0.46]]\n",
      "------------------------------------------------------------------------------------------\n",
      "ID: 4_shift_x2, RRI: (20,), MAD: (10,), SLP_predicted: (5,), SLP_predicted_probability: (5, 4)\n",
      "RRI [24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41.\n",
      " 42. 43.]\n",
      "MAD [12 13 14 15 16 17 18 19 20 21]\n",
      "SLP_predicted [45 46 47 48 49]\n",
      "SLP_predicted_probability [[0.35 0.72 0.08 0.46]\n",
      " [0.98 0.37 0.34 0.46]\n",
      " [0.59 0.2  0.06 0.77]\n",
      " [0.11 0.78 0.67 0.79]\n",
      " [0.91 0.03 0.9  0.09]]\n",
      "------------------------------------------------------------------------------------------\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# initialize the data manager\n",
    "splitting_data_manager = SleepDataManager(file_path = \"Processing_Demonstration/Reverse_Splitting.pkl\")\n",
    "\n",
    "# change the file information\n",
    "new_file_info = {\"RRI_frequency\": 2, \"MAD_frequency\": 1, \"signal_length_seconds\": 10, \"wanted_shift_length_seconds\": 5, \"absolute_shift_deviation_seconds\": 2, \"SLP_frequency\": 1, \"SLP_predicted_frequency\": 0.5, \"RRI_inlier_interval\": [None, None]}\n",
    "splitting_data_manager.change_file_information(new_file_info)\n",
    "\n",
    "data_dict = {\n",
    "    \"RRI\": np.array([i for i in range(44)], dtype=np.float64),\n",
    "    \"RRI_frequency\": 2,\n",
    "    \"MAD\": np.array([i for i in range(22)], dtype=np.int64),\n",
    "    \"MAD_frequency\": 1,\n",
    "}\n",
    "\n",
    "# add data that will be splitted\n",
    "for i in range(5):\n",
    "    data_dict[\"ID\"] = str(i)\n",
    "    splitting_data_manager.save(data_dict, overwrite_id=False)\n",
    "\n",
    "file_generator = load_from_pickle(\"Processing_Demonstration/Reverse_Splitting.pkl\")\n",
    "next(file_generator)\n",
    "\n",
    "# add \"predicted\" sleep stages\n",
    "count = 1\n",
    "old_slp_pred_prob = np.array([np.round(np.random.rand(4), 2) for _ in range(2)], dtype=np.float64)\n",
    "for file in file_generator:\n",
    "    new_slp_pred_prob = np.array([np.round(np.random.rand(4), 2) for _ in range(3)], dtype=np.float64)\n",
    "    new_slp_pred_prob = np.append(old_slp_pred_prob, new_slp_pred_prob, axis=0)\n",
    "    old_slp_pred_prob = new_slp_pred_prob[3:]\n",
    "    additional_info = {\n",
    "        \"SLP_predicted\": np.array([i for i in range(5)], dtype=np.int64)+3*count,\n",
    "        \"SLP_predicted_probability\": new_slp_pred_prob,\n",
    "        \"SLP_predicted_frequency\": 0.5\n",
    "    }\n",
    "    additional_info[\"ID\"] = file[\"ID\"]\n",
    "    splitting_data_manager.save(additional_info, overwrite_id=True)\n",
    "    count += 1\n",
    "\n",
    "del file_generator\n",
    "\n",
    "# print the data\n",
    "print(\"=\"*90)\n",
    "for dict in splitting_data_manager:\n",
    "    message = f\"ID: {dict['ID']}\"\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "            message += f\", {key}: {dict[key].shape}\"\n",
    "    print(message)\n",
    "\n",
    "    if \"4\" in dict['ID']:\n",
    "        for key in dict.keys():\n",
    "            if key in [\"RRI\", \"MAD\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "                print(key, dict[key])\n",
    "        print(\"-\"*90)\n",
    "\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_shape(list):\n",
    "    shape = \"(\"\n",
    "    while True:\n",
    "        try:\n",
    "            shape += str(len(list))\n",
    "            list = list[0]\n",
    "            shape += \", \"\n",
    "        except:\n",
    "            break\n",
    "    shape += \")\"\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "ID: 0, RRI: (44,), MAD: (22,), SLP_predicted: (11, 1, ), SLP_predicted_probability: (11, 4)\n",
      "ID: 1, RRI: (44,), MAD: (22,), SLP_predicted: (11, 1, ), SLP_predicted_probability: (11, 4)\n",
      "ID: 2, RRI: (44,), MAD: (22,), SLP_predicted: (11, 1, ), SLP_predicted_probability: (11, 4)\n",
      "ID: 3, RRI: (44,), MAD: (22,), SLP_predicted: (11, 1, ), SLP_predicted_probability: (11, 4)\n",
      "ID: 4, RRI: (44,), MAD: (22,), SLP_predicted: (11, 1, ), SLP_predicted_probability: (11, 4)\n",
      "------------------------------------------------------------------------------------------\n",
      "RRI [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43.]\n",
      "MAD [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n",
      "SLP_predicted [[39], [40], [41], [42, 42], [43, 43], [44], [45, 45], [46, 46], [47], [48], [49]]\n",
      "SLP_predicted_probability [[0.82 0.94 0.29 0.03]\n",
      " [0.95 0.09 0.09 0.65]\n",
      " [0.69 0.45 0.34 0.75]\n",
      " [0.68 0.39 0.65 0.27]\n",
      " [0.24 0.1  0.26 0.59]\n",
      " [0.42 0.89 0.32 0.65]\n",
      " [0.35 0.72 0.08 0.46]\n",
      " [0.98 0.37 0.34 0.46]\n",
      " [0.59 0.2  0.06 0.77]\n",
      " [0.11 0.78 0.67 0.79]\n",
      " [0.91 0.03 0.9  0.09]]\n"
     ]
    }
   ],
   "source": [
    "# apply the reverse splitting\n",
    "splitting_data_manager.reverse_signal_split()\n",
    "\n",
    "# print the data\n",
    "print(\"-\"*90)\n",
    "for dict in splitting_data_manager:\n",
    "    message = f\"ID: {dict['ID']}\"\n",
    "    for key in dict.keys():\n",
    "        if key in [\"RRI\", \"MAD\", \"SLP_predicted_probability\"]:\n",
    "            message += f\", {key}: {dict[key].shape}\"\n",
    "        if key in [\"SLP_predicted\"]:\n",
    "            message += f\", {key}: {list_shape(dict[key])}\"\n",
    "    print(message)\n",
    "print(\"-\"*90)\n",
    "\n",
    "for key in dict.keys():\n",
    "    if key in [\"RRI\", \"MAD\", \"SLP_predicted\", \"SLP_predicted_probability\"]:\n",
    "        print(key, dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_files = os.listdir(\"Processing_Demonstration\")\n",
    "for file in created_files:\n",
    "    try:\n",
    "        os.remove(f\"Processing_Demonstration/{file}\")\n",
    "    except:\n",
    "        pass\n",
    "os.rmdir(\"Processing_Demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the implemented functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # type: ignore\n",
    "import random\n",
    "import h5py # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you can check whether the implemented functions in this project work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling number of datapoints from signal- to target- frequency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would highly suggest to provide data where the signals don't need to be scaled to the frequencies of the data\n",
    "used to train the neural network.\n",
    "\n",
    "If there is no other option, then so be it. Here is a demonstration of the functions that will be applied to \n",
    "your data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Classification Frequency: 0.05 -> Target Frequency: 0.03333333333333333\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "Classification array:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "Classification array shape:  (15,)\n",
      "\n",
      "Scaled array:  [ 0  1  3  4  6  7  9 10 12 13]\n",
      "Scaled array shape:  (10,)\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "Classification Frequency: 0.02 -> Target Frequency: 0.03333333333333333\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "Classification array:  [0 1 2 3 4 5 6 7 8]\n",
      "Classification array shape:  (9,)\n",
      "\n",
      "Scaled array:  [0 1 1 2 2 3 4 4 5 5 6 7 7 8 8]\n",
      "Scaled array shape:  (15,)\n"
     ]
    }
   ],
   "source": [
    "classification_array = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
    "classification_frequency = 1/20\n",
    "target_frequency = 1/30\n",
    "\n",
    "print(\"-\"*71)\n",
    "print(f\"Classification Frequency: {classification_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*71)\n",
    "print(\"\\nClassification array: \", classification_array)\n",
    "print(\"Classification array shape: \", classification_array.shape)\n",
    "\n",
    "reshaped_array = scale_classification_signal(\n",
    "        signal = classification_array, # type: ignore\n",
    "        signal_frequency = classification_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(\"\\nScaled array: \", reshaped_array)\n",
    "print(\"Scaled array shape: \", reshaped_array.shape)\n",
    "\n",
    "classification_array = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "classification_frequency = 1/50\n",
    "target_frequency = 1/30\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"-\"*71)\n",
    "print(f\"Classification Frequency: {classification_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*71)\n",
    "print(\"\\nClassification array: \", classification_array)\n",
    "print(\"Classification array shape: \", classification_array.shape)\n",
    "\n",
    "reshaped_array = scale_classification_signal(\n",
    "        signal = classification_array, # type: ignore\n",
    "        signal_frequency = classification_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(\"\\nScaled array: \", reshaped_array)\n",
    "print(\"Scaled array shape: \", reshaped_array.shape)\n",
    "\n",
    "del reshaped_array, classification_array, classification_frequency, target_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Continuous Frequency: 3 -> Target Frequency: 4\n",
      "---------------------------------------------------------------------------\n",
      "Continuous array: [0 1 2 3 4 5] / [0. 1. 2. 3. 4. 5.]\n",
      "Continuous array shape:  (6,)\n",
      "\n",
      "Scaled array: [0 1 2 2 3 4 4 5] / [0.   0.75 1.5  2.25 3.   3.75 4.5  5.  ]\n",
      "Scaled array shape:  (8,)\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Continuous Frequency: 5 -> Target Frequency: 4\n",
      "---------------------------------------------------------------------------\n",
      "Continuous array: [0 1 2 3 4 5 6 7 8 9] / [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "Continuous array shape:  (10,)\n",
      "\n",
      "Scaled array: [0 1 2 4 5 6 8 9] / [0.   1.25 2.5  3.75 5.   6.25 7.5  8.75]\n",
      "Scaled array shape:  (8,)\n"
     ]
    }
   ],
   "source": [
    "continuous_array_int = np.array([0, 1, 2, 3, 4, 5])\n",
    "continuous_array_float = np.array([0, 1, 2, 3, 4, 5], dtype = float)\n",
    "continuous_frequency = 3\n",
    "target_frequency = 4\n",
    "\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous Frequency: {continuous_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous array: {continuous_array_int} / {continuous_array_float}\")\n",
    "print(\"Continuous array shape: \", continuous_array_int.shape)\n",
    "\n",
    "reshaped_array_int = interpolate_signal(\n",
    "        signal = continuous_array_int, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "reshaped_array_float = interpolate_signal(\n",
    "        signal = continuous_array_float, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(f\"\\nScaled array: {reshaped_array_int} / {reshaped_array_float}\")\n",
    "print(\"Scaled array shape: \", reshaped_array_int.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "continuous_array_int = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "continuous_array_float = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype = float)\n",
    "continuous_frequency = 5\n",
    "target_frequency = 4\n",
    "\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous Frequency: {continuous_frequency} -> Target Frequency: {target_frequency}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"Continuous array: {continuous_array_int} / {continuous_array_float}\")\n",
    "print(\"Continuous array shape: \", continuous_array_int.shape)\n",
    "\n",
    "reshaped_array_int = interpolate_signal(\n",
    "        signal = continuous_array_int, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "reshaped_array_float = interpolate_signal(\n",
    "        signal = continuous_array_float, # type: ignore\n",
    "        signal_frequency = continuous_frequency,\n",
    "        target_frequency = target_frequency\n",
    "        )\n",
    "\n",
    "print(f\"\\nScaled array: {reshaped_array_int} / {reshaped_array_float}\")\n",
    "print(\"Scaled array shape: \", reshaped_array_int.shape)\n",
    "\n",
    "del reshaped_array_int, reshaped_array_float, continuous_array_int, continuous_array_float, continuous_frequency, target_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting a signal which is too long for the neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A signal which is too short will be padded with zeros. No big deal. On the other hand: A signal which is too \n",
    "long will be splitted into multiple signals. To create more data, the 10 hour range will be shifted along the\n",
    "signal.\n",
    "\n",
    "This shift should not be too small, to create redundant data but also not too big, because the more data the \n",
    "better. So we try to find a shift size close to 1 hour, which lets us shift an integer amount of times\n",
    "easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding optimal shift size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "Optimal shift length for signal which is 2.5 hours longer than desired length of 10 hours: 0.833 hours\n"
     ]
    }
   ],
   "source": [
    "signal_length_addition_hours = 2.5\n",
    "desired_length_hours = 10\n",
    "\n",
    "optimal_shift_length = calculate_optimal_shift_length(\n",
    "        signal_length_seconds = (desired_length_hours + signal_length_addition_hours) * 3600, # type: ignore\n",
    "        desired_length_seconds = desired_length_hours*3600, \n",
    "        wanted_shift_length_seconds = 3600,\n",
    "        absolute_shift_deviation_seconds = 1800,\n",
    "        all_signal_frequencies = [4, 1, 1/30, 1/120]\n",
    ")\n",
    "print(optimal_shift_length)\n",
    "\n",
    "print(f\"Optimal shift length for signal which is {signal_length_addition_hours} hours longer than desired length of {desired_length_hours} hours: {round(optimal_shift_length/3600, 3)} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function to find optimal shift length is embedded in the following split funtion. The optimal\n",
    "shift size will be estimated for every signal individually.\n",
    "\n",
    "If there is no integer shift size in range, that lets you shift the signal so, that you perfectly enclose the\n",
    "last datapoints of the long signal, then the last shift will be altered so that it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift length: 3780\n",
      "Shift length: 945.0 seconds\n",
      "Signal shape:  (174240,)\n",
      "Datapoints in NN: 144000\n",
      "Signals from splitting shape:  (3, 144000, )\n"
     ]
    }
   ],
   "source": [
    "# Create random signal\n",
    "frequency = 4\n",
    "length_signal_seconds = 12.1 * 3600\n",
    "signal = np.random.rand(int(length_signal_seconds * frequency))\n",
    "\n",
    "# Only important parameters here:\n",
    "nn_signal_seconds = 10 * 3600\n",
    "shift_length_seconds = 3600\n",
    "absolute_shift_deviation_seconds = 1800\n",
    "\n",
    "signals_from_splitting, shift_length = split_long_signal(\n",
    "        signal = signal, # type: ignore\n",
    "        sampling_frequency = frequency,\n",
    "        target_frequency = frequency,\n",
    "        nn_signal_duration_seconds = nn_signal_seconds,\n",
    "        wanted_shift_length_seconds = shift_length_seconds,\n",
    "        absolute_shift_deviation_seconds = absolute_shift_deviation_seconds\n",
    "        )\n",
    "\n",
    "print(\"Shift length:\", shift_length)\n",
    "print(f\"Shift length: {shift_length / frequency} seconds\")\n",
    "print(\"Signal shape: \", signal.shape)\n",
    "print(f\"Datapoints in NN: {nn_signal_seconds * frequency}\")\n",
    "print(\"Signals from splitting shape: \", list_shape(signals_from_splitting))\n",
    "\n",
    "del signals_from_splitting, signal, shift_length, frequency, nn_signal_seconds, shift_length_seconds, absolute_shift_deviation_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting signals within dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dictionary:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: (174240,)\n",
      "RRI_frequency: 4\n",
      "MAD: (43560,)\n",
      "MAD_frequency: 1\n",
      "\n",
      "New dictionaries:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: (144000,)\n",
      "RRI_frequency: 4\n",
      "MAD: (36000,)\n",
      "MAD_frequency: 1\n",
      "shift_length_seconds: 3780\n",
      "\n",
      "ID: 1_shift_x1\n",
      "RRI: (144000,)\n",
      "RRI_frequency: 4\n",
      "MAD: (36000,)\n",
      "MAD_frequency: 1\n",
      "shift_length_seconds: 3780\n",
      "\n",
      "ID: 1_shift_x2\n",
      "RRI: (144000,)\n",
      "RRI_frequency: 4\n",
      "MAD: (36000,)\n",
      "MAD_frequency: 1\n",
      "shift_length_seconds: 3780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create random signal\n",
    "length_signal_seconds = 12.1 * 3600\n",
    "rri_frequency = 4\n",
    "mad_frequency = 1\n",
    "rri_signal = np.random.rand(int(length_signal_seconds * rri_frequency))\n",
    "mad_signal = np.random.rand(int(length_signal_seconds * mad_frequency))\n",
    "\n",
    "data_dict = {\n",
    "    \"ID\": \"1\",\n",
    "    \"RRI\": rri_signal,\n",
    "    \"RRI_frequency\": rri_frequency,\n",
    "    \"MAD\": mad_signal,\n",
    "    \"MAD_frequency\": mad_frequency,\n",
    "}\n",
    "\n",
    "new_dictionaries = split_signals_within_dictionary(\n",
    "    data_dict = data_dict,\n",
    "    id_key = \"ID\",\n",
    "    valid_signal_keys = [\"RRI\", \"MAD\"],\n",
    "    signal_frequencies = [rri_frequency, mad_frequency],\n",
    "    signal_target_frequencies = [rri_frequency, mad_frequency],\n",
    "    nn_signal_duration_seconds = 10 * 3600,\n",
    "    wanted_shift_length_seconds = 3600,\n",
    "    absolute_shift_deviation_seconds = 1800,\n",
    "    all_signal_frequencies = [rri_frequency, mad_frequency]\n",
    ")\n",
    "\n",
    "print(\"Original dictionary:\")\n",
    "print(\"-\"*20)\n",
    "for key, value in data_dict.items():\n",
    "    if key == \"RRI\" or key == \"MAD\" or key == \"SLP\":\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "print(\"\\nNew dictionaries:\")\n",
    "print(\"-\"*20)\n",
    "for new_dict in new_dictionaries:\n",
    "    for key, value in new_dict.items():\n",
    "        if key == \"RRI\" or key == \"MAD\" or key == \"SLP\":\n",
    "            print(f\"{key}: {value.shape}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fusing signals back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted Signals:\n",
      " [array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15]), array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21])]\n",
      "\n",
      "Fused signal:\n",
      " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n"
     ]
    }
   ],
   "source": [
    "signals_from_splitting, shift_length = split_long_signal(\n",
    "        signal = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], # type: ignore\n",
    "        sampling_frequency = 1,\n",
    "        target_frequency = 1,\n",
    "        nn_signal_duration_seconds = 10,\n",
    "        wanted_shift_length_seconds = 5,\n",
    "        absolute_shift_deviation_seconds = 1,\n",
    "        all_signal_frequencies = [1]\n",
    "        )\n",
    "\n",
    "print(\"Splitted Signals:\\n\", signals_from_splitting)\n",
    "\n",
    "fused_signal = fuse_splitted_signals(\n",
    "    signals = signals_from_splitting, # type: ignore\n",
    "    shift_length = int(shift_length), # type: ignore\n",
    "    signal_type = \"feature\"\n",
    ")\n",
    "\n",
    "print(\"\\nFused signal:\\n\", fused_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fusing splitted dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dictionary:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: (46,)\n",
      "RRI_frequency: 2\n",
      "MAD: (23,)\n",
      "MAD_frequency: 1\n",
      "SLP: (12,)\n",
      "SLP_frequency: 0.5\n"
     ]
    }
   ],
   "source": [
    "data_dict = {\n",
    "    \"ID\": \"1\",\n",
    "    \"RRI\": np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22]),\n",
    "    \"RRI_frequency\": 2,\n",
    "    \"MAD\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]),\n",
    "    \"MAD_frequency\": 1,\n",
    "    \"SLP\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n",
    "    \"SLP_frequency\": 0.5\n",
    "}\n",
    "\n",
    "new_dictionaries = split_signals_within_dictionary(\n",
    "    data_dict = data_dict,\n",
    "    id_key = \"ID\",\n",
    "    valid_signal_keys = [\"RRI\", \"MAD\", \"SLP\"],\n",
    "    signal_frequencies = [2, 1, 0.5],\n",
    "    signal_target_frequencies = [2, 1, 0.5],\n",
    "    nn_signal_duration_seconds = 10,\n",
    "    wanted_shift_length_seconds = 5,\n",
    "    absolute_shift_deviation_seconds = 2,\n",
    "    all_signal_frequencies = [2, 1, 0.5]\n",
    ")\n",
    "\n",
    "print(\"Original dictionary:\")\n",
    "print(\"-\"*20)\n",
    "for key, value in data_dict.items():\n",
    "    if key == \"RRI\" or key == \"MAD\" or key == \"SLP\":\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New dictionaries:\n",
      "--------------------\n",
      "ID: 1\n",
      "RRI: [0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9]\n",
      "RRI_frequency: 2\n",
      "MAD: [0 1 2 3 4 5 6 7 8 9]\n",
      "MAD_frequency: 1\n",
      "SLP: [0 1 2 3 4]\n",
      "SLP_frequency: 0.5\n",
      "shift_length_seconds: 6\n",
      "\n",
      "ID: 1_shift_x1\n",
      "RRI: [ 6  6  7  7  8  8  9  9 10 10 11 11 12 12 13 13 14 14 15 15]\n",
      "RRI_frequency: 2\n",
      "MAD: [ 6  7  8  9 10 11 12 13 14 15]\n",
      "MAD_frequency: 1\n",
      "SLP: [3 4 5 6 7]\n",
      "SLP_frequency: 0.5\n",
      "shift_length_seconds: 6\n",
      "\n",
      "ID: 1_shift_x2\n",
      "RRI: [12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21]\n",
      "RRI_frequency: 2\n",
      "MAD: [12 13 14 15 16 17 18 19 20 21]\n",
      "MAD_frequency: 1\n",
      "SLP: [ 6  7  8  9 10]\n",
      "SLP_frequency: 0.5\n",
      "shift_length_seconds: 6\n",
      "\n",
      "ID: 1_shift_x3\n",
      "RRI: [18 18 19 19 20 20 21 21 22 22]\n",
      "RRI_frequency: 2\n",
      "MAD: [18 19 20 21 22]\n",
      "MAD_frequency: 1\n",
      "SLP: [ 9 10 11]\n",
      "SLP_frequency: 0.5\n",
      "shift_length_seconds: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNew dictionaries:\")\n",
    "print(\"-\"*20)\n",
    "for new_dict in new_dictionaries:\n",
    "    for key, value in new_dict.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1\n",
      "RRI: [ 0  0  1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10 11 11\n",
      " 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22]\n",
      "MAD: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]\n",
      "SLP: [ 0  1  2  3  4  5  6  7  8  9 10  9 10 11]\n",
      "RRI_frequency: 2\n",
      "MAD_frequency: 1\n",
      "SLP_frequency: 0.5\n"
     ]
    }
   ],
   "source": [
    "fused_dictionary = fuse_splitted_signals_within_dictionaries(\n",
    "    data_dictionaries = new_dictionaries,\n",
    "    valid_signal_keys = [\"RRI\", \"MAD\", \"SLP\"],\n",
    "    valid_signal_frequencies = [2, 1, 0.5],\n",
    ")\n",
    "\n",
    "for key, value in fused_dictionary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a .h5 - file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the available training datasets for our neural network model are stored in a .h5 file. So we need\n",
    "to be able to read it. These are the important operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random patient: 201966_1\n",
      "[0 1 2 3 5]\n",
      "\n",
      "key: slp\n",
      "Data shape: (747,)\n",
      "Data frequency: 0.03333333333333333\n",
      "Inverse data frequency: 30.0\n",
      "Data length: 22410.0 s\n",
      "\n",
      "key: rri\n",
      "Data shape: (89640,)\n",
      "Data frequency: 4\n",
      "Inverse data frequency: 0.25\n",
      "Data length: 22410.0 s\n"
     ]
    }
   ],
   "source": [
    "shhs_dataset = h5py.File(\"Raw_Data/SHHS_dataset.h5\", 'r')\n",
    "patients = list(shhs_dataset['slp'].keys()) # type: ignore\n",
    "\n",
    "random_patient = patients[np.random.randint(0, len(patients))]\n",
    "print(f\"Random patient: {random_patient}\")\n",
    "\n",
    "print(np.unique(shhs_dataset[\"slp\"][random_patient][:])) # type: ignore\n",
    "\n",
    "for key in [\"slp\", \"rri\"]:\n",
    "    print(f\"\\nkey: {key}\")\n",
    "\n",
    "    patient_data = shhs_dataset[key][random_patient][:] # type: ignore\n",
    "    print(f\"Data shape: {patient_data.shape}\") # type: ignore\n",
    "\n",
    "    data_freq = shhs_dataset[key].attrs[\"freq\"] # type: ignore\n",
    "    print(f\"Data frequency: {data_freq}\")\n",
    "    print(f\"Inverse data frequency: {1/data_freq}\") # type: ignore\n",
    "\n",
    "    print(f\"Data length: {patient_data.shape[0]/data_freq} s\") # type: ignore\n",
    "\n",
    "del shhs_dataset, patients, random_patient, key, patient_data, data_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide up a signal into overlapping windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardest thing about this is, that 'window_overlap' and 'datapoints_per_window' must be chosen so that\n",
    "the whole signal fits perfectly into n windows. \n",
    "\n",
    "Additionally, those values must be integers. This means that 'window_duration_seconds' and 'overlap_seconds'\n",
    "multiplied with 'target_fequency' as well as 'sampling_frequency' must be integers. (The features and the target labels\n",
    "must fit equally well into the windows, so that we can find the correlation between a feature- and target- window.)\n",
    "\n",
    "We have the RRI and MAD values as features and the sleep phase as target classification. As we will see,\n",
    "RRI and MAD values were recorded with an integer sampling frequency. While the sampling frequency of the \n",
    "sleep classification is 1/30. \n",
    "\n",
    "Finding window parameters that fullfill the conditions mentioned is easier than it sounds. We will always pass data\n",
    "to the neural network that is 10 hours long. Now, we just need to think in seconds and find integer values\n",
    "for 'window_duration_seconds' and 'overlap_seconds' that are a multiple of 30:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding optimal window_parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suitable window parameters for signal of length: 36000:\n",
      "-------------------------------------------------------\n",
      "Number of windows: 1025, Window size: 160, Overlap: 125.0\n",
      "Number of windows: 1026, Window size: 125, Overlap: 90.0\n",
      "Number of windows: 1055, Window size: 164, Overlap: 130.0\n",
      "Number of windows: 1056, Window size: 130, Overlap: 96.0\n",
      "Number of windows: 1087, Window size: 162, Overlap: 129.0\n",
      "Number of windows: 1088, Window size: 129, Overlap: 96.0\n",
      "Number of windows: 1121, Window size: 160, Overlap: 128.0\n",
      "Number of windows: 1122, Window size: 128, Overlap: 96.0\n",
      "Number of windows: 1157, Window size: 164, Overlap: 133.0\n",
      "Number of windows: 1158, Window size: 133, Overlap: 102.0\n",
      "Number of windows: 1196, Window size: 150, Overlap: 120.0\n",
      "Number of windows: 1197, Window size: 120, Overlap: 90.0\n"
     ]
    }
   ],
   "source": [
    "find_suitable_window_parameters(\n",
    "        signal_length = 10 * 3600,\n",
    "        number_windows_range = (1000, 1400),\n",
    "        window_size_range = (120, 180),\n",
    "        minimum_window_size_overlap_difference = 30\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our options are:\n",
    "\n",
    "Number of windows: 1196, Window size: 150, Overlap: 120.0 \\\n",
    "Number of windows: 1197, Window size: 120, Overlap: 90.0\n",
    "\n",
    "We will choose the latter, because we don't want the window_size to be too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When transforming a classification signal into windows, which is supposed to be the target in the neural \n",
    "network, then each window will only be represented by the most common sleep stage. If there is a tie\n",
    "between the labels, then the one with the highest priority will be chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal shape: (1200,)\n",
      "Signal in windows shape: (1197,)\n"
     ]
    }
   ],
   "source": [
    "signal_length_seconds = 10 * 3600\n",
    "frequency = 1/30\n",
    "signal_length = int(signal_length_seconds * frequency)\n",
    "\n",
    "signal = np.array([random.randint(0, 5) for _ in range(signal_length)])\n",
    "\n",
    "signal_in_windows = signal_to_windows(\n",
    "    signal = signal, # type: ignore\n",
    "    datapoints_per_window = int(120 * frequency),\n",
    "    window_overlap = int(90 * frequency),\n",
    "    signal_type = \"target\",\n",
    "    priority_order = [0, 1, 2, 3, 4, 5, -1]\n",
    "    )\n",
    "\n",
    "print(f\"Signal shape: {signal.shape}\")\n",
    "print(f\"Signal in windows shape: {signal_in_windows.shape}\")\n",
    "\n",
    "del signal, signal_in_windows, signal_length_seconds, frequency, signal_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal shape: (36000,)\n",
      "Signal in windows shape: (1197, 120)\n"
     ]
    }
   ],
   "source": [
    "signal = np.random.rand(36000)\n",
    "\n",
    "signal_in_windows = signal_to_windows(\n",
    "    signal = signal, # type: ignore\n",
    "    datapoints_per_window = 120,\n",
    "    window_overlap = 90,\n",
    "    signal_type = \"feature\"\n",
    "    )\n",
    "\n",
    "print(f\"Signal shape: {signal.shape}\")\n",
    "print(f\"Signal in windows shape: {signal_in_windows.shape}\")\n",
    "\n",
    "del signal, signal_in_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will be applied to transform a signal into overlapping windows. It will make sure\n",
    "that the data is passed correctly to the function mentioned above. \n",
    "\n",
    "This means it will:\n",
    "- check if 'number_nn_datapoints', 'datapoints_per_window' and 'window_overlap' are integers\n",
    "- check if 'datapoints_per_window' and 'window_overlap' perfectly fit into 'number_nn_datapoints'\n",
    "- compare length of provided signal to length of signal in nn ('number_nn_datapoints')\n",
    "    - if smaller: Pad with Zeros\n",
    "    - if bigger: Print warning, but continue by cropping last datapoints\n",
    "- check if signal transformed to windows has the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random array shape: (36000,)\n",
      "Reshaped array shape: (1197, 480)\n",
      "Random array shape: (1200,)\n",
      "Reshaped array shape: (1197,)\n"
     ]
    }
   ],
   "source": [
    "random_array = np.random.rand(36000)\n",
    "reshaped_array = reshape_signal_to_overlapping_windows(\n",
    "    signal = random_array, # type: ignore\n",
    "    target_frequency = 4, \n",
    "    number_windows = 1197, \n",
    "    window_duration_seconds = 120, \n",
    "    overlap_seconds = 90,\n",
    "    signal_type = \"feature\",\n",
    "    nn_signal_duration_seconds = 10*3600,\n",
    "    )\n",
    "\n",
    "print(f\"Random array shape: {random_array.shape}\")\n",
    "print(f\"Reshaped array shape: {reshaped_array.shape}\")\n",
    "\n",
    "random_array = np.array([random.randint(0, 3) for _ in range(int(36000/30))])\n",
    "reshaped_array = reshape_signal_to_overlapping_windows(\n",
    "    signal = random_array, # type: ignore\n",
    "    target_frequency = 1/30, \n",
    "    number_windows = 1197, \n",
    "    window_duration_seconds = 120, \n",
    "    overlap_seconds = 90,\n",
    "    signal_type = \"target\",\n",
    "    nn_signal_duration_seconds = 10*3600,\n",
    "    )\n",
    "\n",
    "print(f\"Random array shape: {random_array.shape}\")\n",
    "print(f\"Reshaped array shape: {reshaped_array.shape}\")\n",
    "\n",
    "del random_array, reshaped_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse Reshape\n",
    "\n",
    "Reversing Reshape of feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original signal:\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "Signal reshaped to overlapping windows:\n",
      "[[ 1  2  3  4  5]\n",
      " [ 2  3  4  5  6]\n",
      " [ 3  4  5  6  7]\n",
      " [ 4  5  6  7  8]\n",
      " [ 5  6  7  8  9]\n",
      " [ 6  7  8  9 10]\n",
      " [ 7  8  9 10  0]\n",
      " [ 8  9 10  0  0]\n",
      " [ 9 10  0  0  0]\n",
      " [10  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]]\n",
      "\n",
      "Last window when padding was cropped:\n",
      "[10  0  0  0  0]\n",
      "\n",
      "Signal reshaped back to original:\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original signal:\")\n",
    "test = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "print(test)\n",
    "\n",
    "print(\"\\nSignal reshaped to overlapping windows:\")\n",
    "reshaped_test = reshape_signal_to_overlapping_windows(\n",
    "    signal = test,\n",
    "    target_frequency = 1,\n",
    "    nn_signal_duration_seconds = 16,\n",
    "    number_windows = 12,\n",
    "    window_duration_seconds = 5,\n",
    "    overlap_seconds = 4,\n",
    "    signal_type = \"feature\"\n",
    "    )\n",
    "print(reshaped_test)\n",
    "\n",
    "print(\"\\nLast window when padding was cropped:\")\n",
    "cropped_padding = remove_padding_from_windows(\n",
    "    signal_in_windows = copy.deepcopy(reshaped_test), # type: ignore\n",
    "    target_frequency = 1,\n",
    "    original_signal_length = 10,\n",
    "    window_duration_seconds = 5, \n",
    "    overlap_seconds = 4,\n",
    "    )\n",
    "print(cropped_padding[-1])\n",
    "\n",
    "print(\"\\nSignal reshaped back to original:\")\n",
    "reversed_test = reverse_signal_to_windows_reshape(\n",
    "    signal_in_windows = reshaped_test, # type: ignore\n",
    "    target_frequency = 1,\n",
    "    original_signal_length = 10,\n",
    "    number_windows = 12,\n",
    "    window_duration_seconds = 5,\n",
    "    overlap_seconds = 4\n",
    "    )\n",
    "print(reversed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sleep stage labels were reshaped differently, as we only keep one label for each window and therefore won't\n",
    "create a 2d array. \n",
    "\n",
    "After predicting the sleep stage labels, we will transform them into a 2d array, that is computable by our \n",
    "reverse reshape function. Effectively, we will create an array from each label, containing only the label as\n",
    "elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original signal:\n",
      "[1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
      "\n",
      "Signal reshaped to overlapping windows:\n",
      "[1 1 1 2 2 2 3 3 3]\n",
      "\n",
      "Expanded signal:\n",
      "[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [2, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2], [3, 3, 3, 3], [3, 3, 3, 3], [3, 3, 3, 3]]\n",
      "\n",
      "Expanded signal reshaped to original:\n",
      "[1.   1.   1.   1.25 1.5  1.75 2.25 2.5  2.75 3.   3.   3.  ]\n",
      "[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original signal:\")\n",
    "test = [1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n",
    "print(test)\n",
    "reshaped_test = reshape_signal_to_overlapping_windows(\n",
    "    signal = test,\n",
    "    target_frequency = 1/3,\n",
    "    nn_signal_duration_seconds = 36,\n",
    "    number_windows = 9,\n",
    "    window_duration_seconds = 12,\n",
    "    overlap_seconds = 9,\n",
    "    signal_type = \"target\"\n",
    "    )\n",
    "\n",
    "print(\"\\nSignal reshaped to overlapping windows:\")\n",
    "print(reshaped_test)\n",
    "\n",
    "expanded_reshaped_test = []\n",
    "for slp_stg in reshaped_test:\n",
    "    expanded_reshaped_test.append([slp_stg for _ in range(int(12 * 1/3))])\n",
    "\n",
    "print(\"\\nExpanded signal:\")\n",
    "print(expanded_reshaped_test)\n",
    "\n",
    "reversed_test = reverse_signal_to_windows_reshape(\n",
    "    signal_in_windows = expanded_reshaped_test, # type: ignore\n",
    "    target_frequency = 1/3, # type: ignore\n",
    "    original_signal_length = 12,\n",
    "    number_windows = 9,\n",
    "    window_duration_seconds = 12,\n",
    "    overlap_seconds = 9\n",
    "    )\n",
    "\n",
    "print(\"\\nExpanded signal reshaped to original:\")\n",
    "print(reversed_test)\n",
    "print([round(i) for i in reversed_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Signal\n",
    "\n",
    "The implemented unity normalization function can either normalize a multi-dimensional array across all\n",
    "arrays (global) or normalize each array indivudally (local)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_dimensional = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "two_dimensional = np.array([[0, 2, 4], [4, 5, 6], [6, 8, 10]])\n",
    "three_dimensional = np.array([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [8, 9, 10]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization_Mode: 'global'\n",
      "----------------------------\n",
      "\n",
      "Normalized One dimensional array:\n",
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
      "\n",
      "Normalized Two dimensional array:\n",
      "[[0.  0.2 0.4]\n",
      " [0.4 0.5 0.6]\n",
      " [0.6 0.8 1. ]]\n",
      "\n",
      "Normalized Three dimensional array:\n",
      "[[[0.  0.1 0.2]\n",
      "  [0.3 0.4 0.5]]\n",
      "\n",
      " [[0.6 0.7 0.8]\n",
      "  [0.8 0.9 1. ]]]\n"
     ]
    }
   ],
   "source": [
    "message = \"Normalization_Mode: \\'global\\'\"\n",
    "print(message)\n",
    "print(\"-\"*len(message))\n",
    "print(\"\\nNormalized One dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = one_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"global\"\n",
    "    ))\n",
    "print(\"\\nNormalized Two dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = two_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"global\"\n",
    "    ))\n",
    "print(\"\\nNormalized Three dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = three_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"global\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization_Mode: 'local'\n",
      "---------------------------\n",
      "\n",
      "Normalized One dimensional array:\n",
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
      "\n",
      "Normalized Two dimensional array:\n",
      "[[0.  0.5 1. ]\n",
      " [0.  0.5 1. ]\n",
      " [0.  0.5 1. ]]\n",
      "\n",
      "Normalized Three dimensional array:\n",
      "[[[0.  0.5 1. ]\n",
      "  [0.  0.5 1. ]]\n",
      "\n",
      " [[0.  0.5 1. ]\n",
      "  [0.  0.5 1. ]]]\n"
     ]
    }
   ],
   "source": [
    "message = \"Normalization_Mode: \\'local\\'\"\n",
    "print(message)\n",
    "print(\"-\"*len(message))\n",
    "print(\"\\nNormalized One dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = one_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"local\"\n",
    "    ))\n",
    "print(\"\\nNormalized Two dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = two_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"local\"\n",
    "    ))\n",
    "print(\"\\nNormalized Three dimensional array:\")\n",
    "print(unity_based_normalization(\n",
    "        signal = three_dimensional, # type: ignore\n",
    "        normalization_max = 1,\n",
    "        normalization_min = 0,\n",
    "        normalization_mode = \"local\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alter Sleep Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function makes sure to keep labels unfiform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2 -1  0  1  2  3  4  5  6  7]\n",
      "[-1 -1  0  0  1  2 -1  3 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "slp = np.array([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7])\n",
    "print(slp)\n",
    "\n",
    "current_labels = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "desired_labels = {\"wake\": 0, \"LS\": 1, \"DS\": 2, \"REM\": 3, \"artifect\": -1}\n",
    "\n",
    "print(alter_slp_labels(\n",
    "        slp_labels = slp, # type: ignore\n",
    "        current_labels = current_labels,\n",
    "        desired_labels = desired_labels,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['light_sleep' 'deep_sleep' 'deep_sleep_2' 'WAKE' 'REM' 'bla' 'blub']\n",
      "['1' '2' '2' '0' '3' '-1' '-1']\n"
     ]
    }
   ],
   "source": [
    "slp = np.array([\"light_sleep\", \"deep_sleep\", \"deep_sleep_2\", \"WAKE\", \"REM\", \"bla\", \"blub\"])\n",
    "print(slp)\n",
    "\n",
    "current_labels = {\"wake\": [\"WAKE\"], \"LS\": [\"light_sleep\"], \"DS\": [\"deep_sleep\", \"deep_sleep_2\"], \"REM\": [\"REM\"], \"artifect\": [\"other\"]}\n",
    "desired_labels = {\"wake\": 0, \"LS\": 1, \"DS\": 2, \"REM\": 3, \"artifect\": -1}\n",
    "\n",
    "print(alter_slp_labels(\n",
    "        slp_labels = slp, # type: ignore\n",
    "        current_labels = current_labels,\n",
    "        desired_labels = desired_labels,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Transformation from previous (not mine) Sleep Stage Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2 -1  0  1  2  3  4  5  6  7]\n",
      "[-2  0  0  0  1  2  3  3  0  6]\n"
     ]
    }
   ],
   "source": [
    "slp = np.array([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7])\n",
    "print(slp)\n",
    "\n",
    "slp[slp>=1] = slp[slp>=1] - 1\n",
    "slp[slp==4] = 3\n",
    "slp[slp==5] = 0\n",
    "slp[slp==-1] = 0 # set artifact as wake stage\n",
    "\n",
    "print(slp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
