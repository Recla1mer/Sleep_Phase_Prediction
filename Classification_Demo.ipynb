{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** *Johannes Peter Knoll*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Within this notebook you will:\n",
    "- Preprocess raw data\n",
    "- Train Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The autoreload extension allows you to tweak the code in the imported modules\n",
    "# and rerun cells to reflect the changes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL IMPORTS\n",
    "from dataset_processing import *\n",
    "from neural_network_model import *\n",
    "\n",
    "# IMPORTS\n",
    "import numpy as np # type: ignore\n",
    "import random\n",
    "import h5py # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Dataset for Neural Network\n",
    "\n",
    "The class SleepDataManager handles the data we want to pass to the network. It makes the data accessible in\n",
    "a memory saving way, but needs to save it (again) into a pickle file. Of course, you can delete the .h5\n",
    "file afterwards if you want to.\n",
    "\n",
    "We unfortunately have multiple sources (besides the SHHS Dataset) with data that we can train the network on.\n",
    "We need to make sure that the data is uniform in sampling frequency, signal length, etc., which is why we will\n",
    "check and transform each datapoint before (and afterwards saving it) using the SleepDataManager class.\n",
    "\n",
    "During the saving process, the SleepDataManager makes sure that the data is uniform in every way and might\n",
    "perform following actions:\n",
    "- Scale number of datapoints in signal if sampling frequency does not match\n",
    "- Alter sleep stage labels if they do not refer to the same context\n",
    "- Split datapoint into multiple if signal duration is longer than required for the neural network\n",
    "\n",
    "To do all of this, we need to provide more information than the signal itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saveable Datapoint:\n",
    "\"\"\"\n",
    "{\n",
    "    \"ID\": str,                  # always required\n",
    "    \"RRI\": np.ndarray,\n",
    "    \"MAD\": np.ndarray,\n",
    "    \"SLP\": np.ndarray,\n",
    "    \"RRI_frequency\": int,       # required if RRI signal is provided\n",
    "    \"MAD_frequency\": int,       # required if MAD signal is provided\n",
    "    \"SLP_frequency\": int,       # required if SLP signal is provided\n",
    "    \"sleep_stage_label\": list   # required if SLP signal is provided\n",
    "} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the keys are save explaining, except for the last one:\n",
    "\n",
    "We want to assign different sleep stage labels in our network (SSM in the following):\n",
    "\n",
    "|number|SHHS stage|GIF stage|SSM stage|\n",
    "|------|----------|---------|---------|\n",
    "|  0   | wake     |         | wake    |\n",
    "|  1   | N1       |         | LS      |\n",
    "|  2   | N2       |         | DS      |\n",
    "|  3   | N3       |         | REM     |\n",
    "|  5   | REM      |         |         |\n",
    "| other| artifact |         |         |\n",
    "| -1   |          |         | artifact|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see: N1 needs to be classified as wake, N2 as LS (light sleep), and N3 as DS (deep sleep).\n",
    "To do this, we effectively need to change: \\\n",
    "0 -> 0, 1 -> 0, 2 -> 1, 3 -> 2, 5 -> 3, other -> -1\n",
    "\n",
    "To make this achievable by the algorithm, we just need to say which labels correspond to which stage\n",
    "in the \"sleep_stage_label\" key as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shhs_sleep_stage_label = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "gif_sleep_stage_label = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHHS Dataset\n",
    "\n",
    "The [Sleep Heart Health Study (SHHS)](https://sleepdata.org/datasets/shhs) is a multi-center cohort study implemented by the National Heart Lung & Blood Institute to determine the cardiovascular and other consequences of sleep-disordered breathing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget \"https://onedrive.live.com/download?cid=45D5A10F94E33861&resid=45D5A10F94E33861%21248707&authkey=AKRa5kb3XFj4G-o\" -O shhs_dataset.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_shhs_dataset = \"../Training_Data/SHHS_dataset.h5\"\n",
    "path_to_shhs_dataset = \"Raw_Data/SHHS_dataset.h5\"\n",
    "\n",
    "shhs_dataset = h5py.File(path_to_shhs_dataset, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfering Data from .h5 into .pkl file using SleepDataManager\n",
    "\n",
    "It is wise to check if all ID's are unique prior to saving. Then we can skip\n",
    "checking every ID in the database when saving each datapoint, which will speed up the saving process greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All IDs are unique.\n"
     ]
    }
   ],
   "source": [
    "# initializing the database\n",
    "file_path_to_sleep_data = \"Processed_Data/shhs_data.pkl\"\n",
    "shhs_data_manager = SleepDataManager(file_path = file_path_to_sleep_data)\n",
    "\n",
    "# accessing patient ids:\n",
    "patients = list(shhs_dataset['slp'].keys()) # type: ignore\n",
    "\n",
    "# check if patient ids are unique:\n",
    "shhs_data_manager.check_if_ids_are_unique(patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all ID's are unique, you can continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving all data from SHHS dataset to the shhs_data.pkl file\n",
    "for patient_id in patients:\n",
    "    new_datapoint = {\n",
    "        \"ID\": patient_id,\n",
    "        \"RRI\": shhs_dataset[\"rri\"][patient_id][:], # type: ignore\n",
    "        \"SLP\": shhs_dataset[\"slp\"][patient_id][:], # type: ignore\n",
    "        \"RRI_frequency\": shhs_dataset[\"rri\"].attrs[\"freq\"], # type: ignore\n",
    "        \"SLP_frequency\": shhs_dataset[\"slp\"].attrs[\"freq\"], # type: ignore\n",
    "        \"sleep_stage_label\": copy.deepcopy(shhs_sleep_stage_label)\n",
    "    }\n",
    "\n",
    "    shhs_data_manager.save(new_datapoint, unique_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200178_2_shift_4440s_x1\n",
      "200333_2_shift_0s_x1\n",
      "200373_2_shift_0s_x1\n",
      "200575_2_shift_0s_x1\n",
      "200662_2_shift_0s_x1\n",
      "200750_2_shift_2220s_x1\n",
      "200841_2_shift_0s_x1\n",
      "200853_2_shift_0s_x1\n",
      "200866_2_shift_1920s_x1\n",
      "200932_2_shift_0s_x1\n",
      "200939_2_shift_3360s_x1\n",
      "200956_2_shift_0s_x1\n",
      "200990_2_shift_2910s_x1\n",
      "200990_2_shift_2910s_x2\n",
      "201072_2_shift_0s_x1\n",
      "201196_2_shift_0s_x1\n",
      "201206_2_shift_2340s_x1\n",
      "201216_2_shift_1920s_x1\n",
      "201268_2_shift_2010s_x1\n",
      "201317_2_shift_2100s_x1\n",
      "201340_2_shift_1800s_x1\n",
      "201353_2_shift_2220s_x1\n",
      "201359_2_shift_0s_x1\n",
      "201377_2_shift_0s_x1\n",
      "201477_2_shift_0s_x1\n",
      "201517_2_shift_3060s_x1\n",
      "201526_2_shift_0s_x1\n",
      "201544_2_shift_0s_x1\n",
      "201550_2_shift_0s_x1\n",
      "201557_2_shift_4800s_x1\n",
      "201558_2_shift_2280s_x1\n",
      "201606_2_shift_3540s_x1\n",
      "201784_2_shift_2220s_x1\n",
      "201790_2_shift_0s_x1\n",
      "201856_2_shift_3180s_x1\n",
      "202376_2_shift_2280s_x1\n",
      "202408_2_shift_2940s_x1\n",
      "202426_2_shift_0s_x1\n",
      "202510_2_shift_0s_x1\n",
      "202517_2_shift_0s_x1\n",
      "202540_2_shift_0s_x1\n",
      "202617_2_shift_0s_x1\n",
      "202630_2_shift_0s_x1\n",
      "202826_2_shift_4800s_x1\n",
      "202845_2_shift_0s_x1\n",
      "202941_2_shift_0s_x1\n",
      "203010_2_shift_0s_x1\n",
      "203139_2_shift_0s_x1\n",
      "203240_2_shift_0s_x1\n",
      "203268_2_shift_2880s_x1\n",
      "203275_2_shift_4380s_x1\n",
      "203286_2_shift_0s_x1\n",
      "203334_2_shift_2280s_x1\n",
      "203430_2_shift_0s_x1\n",
      "203448_2_shift_0s_x1\n",
      "203456_2_shift_4230s_x1\n",
      "203519_2_shift_2280s_x1\n",
      "203542_2_shift_0s_x1\n",
      "203555_2_shift_0s_x1\n",
      "203574_2_shift_0s_x1\n",
      "203587_2_shift_0s_x1\n",
      "203636_2_shift_2400s_x1\n",
      "203709_2_shift_0s_x1\n",
      "203716_2_shift_0s_x1\n",
      "203746_2_shift_1860s_x1\n",
      "203830_2_shift_0s_x1\n",
      "203881_2_shift_2160s_x1\n",
      "203912_2_shift_3240s_x1\n",
      "203932_2_shift_3600s_x1\n",
      "203934_2_shift_0s_x1\n",
      "203937_2_shift_0s_x1\n",
      "204186_2_shift_0s_x1\n",
      "204216_2_shift_0s_x1\n",
      "204222_2_shift_0s_x1\n",
      "204275_2_shift_2400s_x1\n",
      "204300_2_shift_0s_x1\n",
      "204314_2_shift_0s_x1\n",
      "204455_2_shift_0s_x1\n",
      "204465_2_shift_2520s_x1\n",
      "204474_2_shift_0s_x1\n",
      "204518_2_shift_2850s_x1\n",
      "204518_2_shift_2850s_x2\n",
      "204533_2_shift_0s_x1\n",
      "204667_2_shift_2490s_x1\n",
      "204690_2_shift_0s_x1\n",
      "204708_2_shift_0s_x1\n",
      "204730_2_shift_0s_x1\n",
      "204825_2_shift_0s_x1\n",
      "204868_2_shift_2940s_x1\n",
      "204883_2_shift_0s_x1\n",
      "204895_2_shift_0s_x1\n",
      "204941_2_shift_0s_x1\n",
      "204948_2_shift_0s_x1\n",
      "205150_2_shift_0s_x1\n",
      "205157_2_shift_0s_x1\n",
      "205172_2_shift_2910s_x1\n",
      "205204_2_shift_0s_x1\n",
      "205246_2_shift_2460s_x1\n",
      "205248_2_shift_2340s_x1\n",
      "205362_2_shift_0s_x1\n",
      "205398_2_shift_0s_x1\n",
      "205468_2_shift_0s_x1\n",
      "205798_2_shift_3060s_x1\n"
     ]
    }
   ],
   "source": [
    "for key in sleep_data_manager[\"ID\"]: # type: ignore\n",
    "    if \"shift\" in key:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data to overlapping windows\n",
    "\n",
    "We want to pass the signal in overlapping windows to the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shhs_data_manager.transform_signals_to_windows(\n",
    "    number_windows = 1197, \n",
    "    window_duration_seconds = 120, \n",
    "    overlap_seconds = 90, \n",
    "    priority_order = [0, 1, 2, 3, 5, -1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GIF Dataset\n",
    "\n",
    "Analogue to the SHHS Dataset, we will save the data to our SleepDataManager and transform it into windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_keys = [\"file_name\", \"RRI\", \"RRI_frequency\", \"MAD\", \"MAD_frequency\", \"SLP\"]\n",
    "results_generator = load_from_pickle(\"Processed_GIF/GIF_Results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_gif_dataset = \"Raw_Data/GIF_dataset.h5\"\n",
    "\n",
    "gif_dataset = h5py.File(path_to_gif_dataset, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfering Data from .h5 into .pkl file using SleepDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All IDs are unique.\n"
     ]
    }
   ],
   "source": [
    "# initializing the database\n",
    "file_path_to_sleep_data = \"Processed_Data/gif_data.pkl\"\n",
    "gif_data_manager = SleepDataManager(file_path = file_path_to_sleep_data)\n",
    "\n",
    "# accessing patient ids:\n",
    "patients = list(gif_dataset['stage'].keys()) # type: ignore\n",
    "\n",
    "# check if patient ids are unique:\n",
    "gif_data_manager.check_if_ids_are_unique(patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293 ['SL003', 'SL005', 'SL006', 'SL008', 'SL009', 'SL010', 'SL012', 'SL013', 'SL014', 'SL015', 'SL017', 'SL018', 'SL019', 'SL020', 'SL021', 'SL022', 'SL024', 'SL026', 'SL028', 'SL029', 'SL030', 'SL031', 'SL033', 'SL035', 'SL036', 'SL038', 'SL039', 'SL041', 'SL042', 'SL043', 'SL044', 'SL045', 'SL046', 'SL047', 'SL048', 'SL049', 'SL050', 'SL051', 'SL052', 'SL053', 'SL054', 'SL056', 'SL058', 'SL059', 'SL060', 'SL062', 'SL063', 'SL064', 'SL065', 'SL067', 'SL068', 'SL069', 'SL070', 'SL071', 'SL072', 'SL074', 'SL077', 'SL078', 'SL080', 'SL081', 'SL082', 'SL084', 'SL086', 'SL092', 'SL093', 'SL094', 'SL095', 'SL097', 'SL099', 'SL102', 'SL103', 'SL104', 'SL106', 'SL107', 'SL108', 'SL109', 'SL110', 'SL112', 'SL113', 'SL115', 'SL117', 'SL118', 'SL119', 'SL120', 'SL121', 'SL122', 'SL123', 'SL124', 'SL125', 'SL127', 'SL128', 'SL129', 'SL130', 'SL131', 'SL134', 'SL135', 'SL136', 'SL137', 'SL139', 'SL140', 'SL142', 'SL143', 'SL144', 'SL146', 'SL147', 'SL148', 'SL149', 'SL150', 'SL152', 'SL153', 'SL154', 'SL155', 'SL156', 'SL158', 'SL160', 'SL161', 'SL162', 'SL163', 'SL164', 'SL167', 'SL168', 'SL169', 'SL170', 'SL171', 'SL172', 'SL175', 'SL177', 'SL180', 'SL181', 'SL182', 'SL183', 'SL184', 'SL185', 'SL187', 'SL188', 'SL189', 'SL190', 'SL191', 'SL192', 'SL194', 'SL195', 'SL196', 'SL197', 'SL198', 'SL200', 'SL201', 'SL206', 'SL212', 'SL213', 'SL215', 'SL220', 'SL222', 'SL223', 'SL224', 'SL225', 'SL226', 'SL230', 'SL231', 'SL232', 'SL233', 'SL234', 'SL235', 'SL238', 'SL239', 'SL241', 'SL242', 'SL243', 'SL244', 'SL251', 'SL253', 'SL254', 'SL256', 'SL258', 'SL259', 'SL260', 'SL261', 'SL262', 'SL263', 'SL264', 'SL265', 'SL266', 'SL267', 'SL270', 'SL271', 'SL272', 'SL273', 'SL274', 'SL275', 'SL276', 'SL277', 'SL278', 'SL279', 'SL280', 'SL281', 'SL282', 'SL283', 'SL284', 'SL285', 'SL287', 'SL288', 'SL289', 'SL291', 'SL292', 'SL295', 'SL296', 'SL297', 'SL298', 'SL299', 'SL300', 'SL301', 'SL302', 'SL303', 'SL304', 'SL306', 'SL307', 'SL308', 'SL309', 'SL310', 'SL311', 'SL312', 'SL313', 'SL314', 'SL317', 'SL318', 'SL319', 'SL320', 'SL321', 'SL323', 'SL324', 'SL325', 'SL326', 'SL327', 'SL329', 'SL330', 'SL331', 'SL334', 'SL335', 'SL337', 'SL338', 'SL339', 'SL340', 'SL343', 'SL345', 'SL346', 'SL347', 'SL350', 'SL351', 'SL352', 'SL353', 'SL354', 'SL355', 'SL356', 'SL357', 'SL358', 'SL360', 'SL361', 'SL362', 'SL363', 'SL364', 'SL366', 'SL367', 'SL368', 'SL369', 'SL370', 'SL371', 'SL372', 'SL373', 'SL374', 'SL375', 'SL376', 'SL377', 'SL378', 'SL379', 'SL381', 'SL383', 'SL385', 'SL387', 'SL388', 'SL390', 'SL391', 'SL393', 'SL394', 'SL395', 'SL396', 'SL398', 'SL400', 'SL401', 'SL402', 'SL403', 'SL404', 'SL405', 'SL407', 'SL409']\n",
      "1\n",
      "4\n",
      "1\n",
      "1439 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3.]\n",
      "172680\n",
      "43170\n"
     ]
    }
   ],
   "source": [
    "print(len(patients), patients)\n",
    "print(gif_dataset[\"stage\"].attrs[\"freq\"])\n",
    "print(gif_dataset[\"rri\"].attrs[\"freq\"])\n",
    "print(gif_dataset[\"mad\"].attrs[\"freq\"])\n",
    "sleep = gif_dataset[\"stage\"][\"SL003\"][:]\n",
    "rri = gif_dataset[\"rri\"][\"SL003\"][:]\n",
    "mad = gif_dataset[\"mad\"][\"SL003\"][:]\n",
    "print(len(sleep), sleep[400:500])\n",
    "print(len(rri))\n",
    "print(len(mad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all ID's are unique, you can continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving all data from GIF dataset to the gif_data.pkl file\n",
    "for patient_id in patients:\n",
    "    new_datapoint = {\n",
    "        \"ID\": patient_id,\n",
    "        \"RRI\": gif_dataset[\"rri\"][patient_id][:], # type: ignore\n",
    "        \"MAD\": gif_dataset[\"mad\"][patient_id][:], # type: ignore\n",
    "        \"SLP\": gif_dataset[\"stage\"][patient_id][:], # type: ignore\n",
    "        \"RRI_frequency\": gif_dataset[\"rri\"].attrs[\"freq\"], # type: ignore\n",
    "        \"MAD_frequency\": gif_dataset[\"mad\"].attrs[\"freq\"], # type: ignore\n",
    "        \"SLP_frequency\": 1/30, # type: ignore\n",
    "        \"sleep_stage_label\": copy.deepcopy(gif_sleep_stage_label)\n",
    "    }\n",
    "\n",
    "    gif_data_manager.save(new_datapoint, unique_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data to overlapping windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_data_manager.transform_signals_to_windows(\n",
    "    number_windows = 1197, \n",
    "    window_duration_seconds = 120, \n",
    "    overlap_seconds = 90, \n",
    "    priority_order = [0, 1, 2, 3, 5, -1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training-, Validation- and Test- Datasets\n",
    "\n",
    "For easier application we will split our database into main-, training-, validation- and test- files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shhs_data_manager.separate_train_test_validation(\n",
    "    train_size = 0.8, \n",
    "    validation_size = 0.1, \n",
    "    test_size = 0.1, \n",
    "    random_state = None, \n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "gif_data_manager.separate_train_test_validation(\n",
    "    train_size = 0.8, \n",
    "    validation_size = 0.1, \n",
    "    test_size = 0.1, \n",
    "    random_state = None, \n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have 3 additional files in the same directory where our main data is saved (\"file_path_to_sleep_data\").\n",
    "\n",
    "Each can be accessed separately with another instance of the class SleepDataManager. Note that their functionality\n",
    "is limited, as they are only meant to return (load) data.\n",
    "\n",
    "The data in these files can be reshuffled by calling the above code cell again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
