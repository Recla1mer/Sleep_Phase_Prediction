{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** *Johannes Peter Knoll*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates all functionalities this project offers:\n",
    "- preprocess data (unify data, split training data into training- and validation- pids)\n",
    "- train neural network\n",
    "- predict sleep stages of Validation data and evaluate neural network performance\n",
    "- predict sleep stages of non-training and non-validation data\n",
    "\n",
    "It is basically the commented version of the file: \"main.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The autoreload extension allows you to tweak the code in the imported modules\n",
    "# and rerun cells to reflect the changes.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL IMPORTS\n",
    "from main import *\n",
    "\n",
    "# IMPORTS\n",
    "import h5py # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data\n",
    "\n",
    "We essentially want to map RRI (and MAD) data to sleep stages. So we need datasets providing exactly this.\n",
    "For training our model, we used the following:\n",
    "- SHHS: provides RRI and sleep stage\n",
    "- GIF: provides RRI, MAD and sleep stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHHS Dataset\n",
    "\n",
    "The [Sleep Heart Health Study (SHHS)](https://sleepdata.org/datasets/shhs) is a multi-center cohort study implemented by the National Heart Lung & Blood Institute to determine the cardiovascular and other consequences of sleep-disordered breathing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget \"https://onedrive.live.com/download?cid=45D5A10F94E33861&resid=45D5A10F94E33861%21248707&authkey=AKRa5kb3XFj4G-o\" -O shhs_dataset.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_shhs_dataset = \"../Training_Data/SHHS_dataset.h5\"\n",
    "path_to_shhs_dataset = \"Raw_Data/SHHS_dataset.h5\"\n",
    "\n",
    "shhs_dataset = h5py.File(path_to_shhs_dataset, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GIF Dataset\n",
    "\n",
    "Unfortunately, the GIF data is not publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Configuration\n",
    "\n",
    "Project Configuration includes [setting file paths](#setting-file-paths) and [adjusting parameters](#adjusting-parameters).\n",
    "\n",
    "Preprocessing data and training the neural network can be controlled using various parameters. To ensure that\n",
    "for later predictions we use the same parameters to set up the neural network and preprocess our data we will\n",
    "save those as dictionary to a pickle file, which will be accessed at every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Parameters\n",
    "\n",
    "It is highly UNRECOMMENDED to change the following default parameters, especially if you aim to adjust a few \n",
    "parameters.\n",
    "\n",
    "In the following section ['Creating Project Configuration'](#creating-project-configuration) we will see the \n",
    "recommended way of adjusting parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for 'SleepDataManager' class\n",
    "\n",
    "The 'SleepDataManager' class and all data processing functions are thoroughly explained in the jupyter \n",
    "notebook: 'Processing_Demo'. However, below is a short summary of the important basics you need to for this\n",
    "project:\n",
    "\n",
    "This class resaves your training data to a seperate pickle file and ensures that the data is uniform and can\n",
    "be accessed and passed to the neural network in a memory saving way. During the saving process it might perform\n",
    "the following actions:\n",
    "- scale number of datapoints in signal so that the current signals sampling frequency matches the uniform \n",
    "    database signal frequency\n",
    "- alter sleep labels\n",
    "- remove RRI and/or MAD outliers\n",
    "- split signal into multiple signals if signal is longer than the uniform maximum signal length: \n",
    "    'signal_length_seconds'\n",
    "\n",
    "All other functionalities of this class will be explained during this project when necessary.\n",
    "\n",
    "See 'SleepDataManager' class in 'dataset_processing.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_data_manager_parameters = {\n",
    "    \"RRI_frequency\": 4,\n",
    "    \"MAD_frequency\": 1,\n",
    "    \"SLP_frequency\": 1/30,\n",
    "    \"RRI_inlier_interval\": [0.3, 2],\n",
    "    \"MAD_inlier_interval\": [None, None],\n",
    "    \"sleep_stage_label\": {\"wake\": 0, \"LS\": 1, \"DS\": 2, \"REM\": 3, \"artifect\": 0},\n",
    "    \"signal_length_seconds\": 36000,\n",
    "    \"wanted_shift_length_seconds\": 5400,\n",
    "    \"absolute_shift_deviation_seconds\": 1800,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into pids\n",
    "\n",
    "Splitting our data into training-, validation- (and test-) pids can be performed using the 'SleepDataManager' \n",
    "class.\n",
    "\n",
    "See: 'separate_train_test_validation' function of 'SleepDataManager' class in 'dataset_processing.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data_parameters = {\n",
    "    \"train_size\": 0.8,\n",
    "    \"validation_size\": 0.2,\n",
    "    \"test_size\": None,\n",
    "    \"random_state\": None,\n",
    "    \"shuffle\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for 'CustomSleepDataset' class\n",
    "\n",
    "Custom Dataset class for our Sleep Stage Data. The class is used to load data from a file \n",
    "(using 'SleepDataManager' class) and prepare it for training the neural network.\n",
    "\n",
    "The whole project including this [class](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "were created in analogy to the [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/basics/). \n",
    "Knowing this tutorial is not necessary for using this project, but highly recommended should you be interested\n",
    "in understanding and editing the code.\n",
    "\n",
    "See: 'CustomSleepDataset' class in 'neural_network_model.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_class_transform_parameters = {\n",
    "    \"transform\": ToTensor(), \n",
    "    \"target_transform\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape signals to overlapping windows\n",
    "\n",
    "Reshape a signal with shape (n <= nn_signal_duration_seconds * target_frequency) to \n",
    "(number_windows, window_size), where windows overlap by 'overlap_seconds' and adjust the signal to the neural \n",
    "network's requirements.\n",
    "\n",
    "This will be performed when accessing the data using 'CustomSleepDataset' class.\n",
    "\n",
    "See: 'reshape_signal_to_overlapping_windows' function in 'dataset_processing.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_reshape_parameters = {\n",
    "    \"nn_signal_duration_seconds\": sleep_data_manager_parameters[\"signal_length_seconds\"],\n",
    "    \"number_windows\": 1197,\n",
    "    \"window_duration_seconds\": 120,\n",
    "    \"overlap_seconds\": 90,\n",
    "    \"priority_order\": [3, 2, 1, 0],\n",
    "    \"pad_feature_with\": 0,\n",
    "    \"pad_target_with\": 0\n",
    "}\n",
    "sleep_data_manager_parameters[\"SLP_expected_predicted_frequency\"] = 1/window_reshape_parameters[\"window_duration_seconds\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalize the signal into range: (normalization_min, normalization_max) using the unity based normalization \n",
    "method.\n",
    "\n",
    "This will be performed when accessing the data using 'CustomSleepDataset' class.\n",
    "\n",
    "See: 'unity_based_normalization' function in 'dataset_processing.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_normalization_parameters = {\n",
    "    \"normalize_rri\": False,\n",
    "    \"normalize_mad\": False,\n",
    "    \"normalization_max\": 1,\n",
    "    \"normalization_min\": 0,\n",
    "    \"normalization_mode\": \"global\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture\n",
    "\n",
    "Parameters affecting the neural network architecture.\n",
    "\n",
    "See 'SleepStageModel' or 'YaoModel' class in 'neural_network_model.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_model_parameters = {\n",
    "    \"number_sleep_stages\": 4,\n",
    "    \"datapoints_per_rri_window\": int(sleep_data_manager_parameters[\"RRI_frequency\"] * window_reshape_parameters[\"window_duration_seconds\"]),\n",
    "    \"datapoints_per_mad_window\": int(sleep_data_manager_parameters[\"MAD_frequency\"] * window_reshape_parameters[\"window_duration_seconds\"]),\n",
    "    \"windows_per_signal\": window_reshape_parameters[\"number_windows\"],\n",
    "    \"number_window_learning_features\": 128,\n",
    "    \"rri_convolutional_channels\": [1, 8, 16, 32, 64],\n",
    "    \"mad_convolutional_channels\": [1, 8, 16, 32, 64],\n",
    "    \"window_learning_dilations\": [2, 4, 8, 16, 32],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Training Hyperparameters\n",
    "\n",
    "Hyperparameters used when training the neural network. \n",
    "\n",
    "These are the only ones that will not be saved to the project configuration file, because they might differ \n",
    "based on what data is currently used for training.\n",
    "\n",
    "See 'main_model_training' function in 'main.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_hyperparameters_shhs = {\n",
    "    \"batch_size\": 8,\n",
    "    \"number_epochs\": 40,\n",
    "    \"lr_scheduler_parameters\": {\n",
    "        \"number_updates_to_max_lr\": 10,\n",
    "        \"start_learning_rate\": 2.5 * 1e-5,\n",
    "        \"max_learning_rate\": 1 * 1e-4,\n",
    "        \"end_learning_rate\": 5 * 1e-5\n",
    "    }\n",
    "}\n",
    "\n",
    "neural_network_hyperparameters_gif = {\n",
    "    \"batch_size\": 8,\n",
    "    \"number_epochs\": 100,\n",
    "    \"lr_scheduler_parameters\": {\n",
    "        \"number_updates_to_max_lr\": 25,\n",
    "        \"start_learning_rate\": 2.5 * 1e-5,\n",
    "        \"max_learning_rate\": 1 * 1e-4,\n",
    "        \"end_learning_rate\": 1 * 1e-5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting File Paths\n",
    "\n",
    "During this project, you will likely create a lot of files. Most of them are assigned an intuitive name in \n",
    "'main.py'. It is recommended to leave them be or changing them once before creating your first project.\n",
    "\n",
    "Now you only need to set the directory where you want to store your files or from which you want to access the\n",
    "trained model for making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_shhs_path = \"Processed_Data/shhs_data.pkl\"\n",
    "processed_gif_path = \"Processed_Data/gif_data.pkl\"\n",
    "\n",
    "# Create directory to store configurations and results\n",
    "model_directory_path = \"Neural_Network/\"\n",
    "create_directories_along_path(model_directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Project Configuration\n",
    "\n",
    "We will now create a dictionary that holds all parameters introduced in the previous section \n",
    "['Default Parameters'](#default-parameters).\n",
    "\n",
    "The current default parameters correspond to the idea of: overlapping windows, artifact = wake stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_configuration = dict()\n",
    "project_configuration.update(sleep_data_manager_parameters)\n",
    "project_configuration.update(window_reshape_parameters)\n",
    "project_configuration.update(signal_normalization_parameters)\n",
    "project_configuration.update(split_data_parameters)\n",
    "project_configuration.update(dataset_class_transform_parameters)\n",
    "project_configuration.update(neural_network_model_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Parameters\n",
    "\n",
    "If you aim to test one of the additional ideas below, just run the corresponding cell. \n",
    "DO NOT RUN BOTH CELLS! \n",
    "\n",
    "To prevent accidentally runnning one of the cells, they were set up to raise an error. This line needs to be\n",
    "removed if you aim to adjust it, obviously.\n",
    "\n",
    "This would also be the ideal place to create your own paragraph with your desired adjustments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Idea: non-overlapping windows, artifect = wake stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"This Error was intentionally placed here to prevent the user from running this cell\" +\n",
    "                    \" accidentally. Uncomment this line only if you know what you are doing.\")\n",
    "\n",
    "project_configuration[\"overlap_seconds\"] = 0\n",
    "project_configuration[\"number_windows\"] = 300\n",
    "project_configuration[\"windows_per_signal\"] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Idea: overlapping windows, artifect is a unique stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"This Error was intentionally placed here to prevent the user from running this cell\" +\n",
    "                    \" accidentally. Uncomment this line only if you know what you are doing.\")\n",
    "\n",
    "project_configuration[\"sleep_stage_label\"] = {\"wake\": 1, \"LS\": 2, \"DS\": 3, \"REM\": 4, \"artifect\": 0}\n",
    "project_configuration[\"priority_order\"] = [4, 3, 2, 1, 0]\n",
    "project_configuration[\"number_sleep_stages\"] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"This Error was intentionally placed here to prevent the user from running this cell\" +\n",
    "                    \" accidentally. Uncomment this line only if you know what you are doing.\")\n",
    "\n",
    "project_configuration[\"...\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking and Saving Project Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_project_configuration(project_configuration)\n",
    "\n",
    "if os.path.isfile(model_directory_path + project_configuration_file):\n",
    "    os.remove(model_directory_path + project_configuration_file)\n",
    "save_to_pickle(project_configuration, model_directory_path + project_configuration_file)\n",
    "\n",
    "del project_configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding on 'SleepDataManager' class\n",
    "\n",
    "As mentioned above: During the saving process, the SleepDataManager makes sure that the data is uniform and\n",
    "might perform the following actions:\n",
    "- Scale number of datapoints in signal if sampling frequency does not match\n",
    "- Alter sleep stage labels if they do not refer to the same context\n",
    "- Remove outliers from RRI and/or MAD data\n",
    "- Split datapoint into multiple ones, if signal duration is too long to be processable by the neural network\n",
    "\n",
    "To do all of this, we need to provide more information than just the signal itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saveable Datapoint:\n",
    "\"\"\"\n",
    "{\n",
    "    \"ID\": str,                  # always required\n",
    "    \"RRI\": np.ndarray,\n",
    "    \"MAD\": np.ndarray,\n",
    "    \"SLP\": np.ndarray,\n",
    "    \"RRI_frequency\": int,       # required if RRI signal is provided\n",
    "    \"MAD_frequency\": int,       # required if MAD signal is provided\n",
    "    \"SLP_frequency\": int,       # required if SLP signal is provided\n",
    "    \"sleep_stage_label\": list   # required if SLP signal is provided\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the keys are save explaining, except for the last one ('sleep_stage_label'):\n",
    "\n",
    "Different sources might use different numbers to label the sleep stages. The last key is used to ensure\n",
    "this projection is uniform. Here is a possible problem, where 'SSM stage' (SleepStageModel stage) refers to \n",
    "the projection used for training the neural network: \n",
    "\n",
    "|number|SHHS stage|GIF stage| SSM stage         |\n",
    "|------|----------|---------|-------------------|\n",
    "|  0   | wake     | wake    | wake & artifact   |\n",
    "|  1   | N1       | N1      | LS                |\n",
    "|  2   | N2       | N2      | DS                |\n",
    "|  3   | N3       | N3      | REM               |\n",
    "|  5   | REM      | REM     |                   |\n",
    "| other| artifact | artifact|                   |\n",
    "\n",
    "As you see, we have different sleep stages, which are assigned different numbers. Looking at transforming SHHS\n",
    "stages for example: We want to map wake (0) N1 (1) to wake (0), N2 (2) to LS (1), N3 (3) to DS (2), REM (5) to\n",
    "REM (3) and artifact (other) to artifact (0).\n",
    "\n",
    "So, we have to tell which numbers correspond to which desired sleep stage in the data you want to save\n",
    "('sleep_stage_label' key above) and which desired sleep stage corresponds to which number ('sleep_stage_label' \n",
    "key in SleepDataManager's 'file_info' variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to save\n",
    "shhs_sleep_stage_label = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "gif_sleep_stage_label = {\"wake\": [0, 1], \"LS\": [2], \"DS\": [3], \"REM\": [5], \"artifect\": [\"other\"]}\n",
    "\n",
    "# SleepDataManager' file_info:\n",
    "# file_info[\"sleep_stage_label\"] = {\"wake\": 0, \"LS\": 1, \"DS\": 2, \"REM\": 3, \"artifect\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing SHHS and GIF\n",
    "\n",
    "Because it is very specific to the individual dataset, the following functions might not work for your data.\n",
    "Nonetheless, they are well documented (in 'main.py') and demonstrate how to use the 'SleepDataManager' class\n",
    "to preprocess your data. Therefore, the preprocessing function should be easy to set up to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to the data\n",
    "original_shhs_data_path = \"Raw_Data/SHHS_dataset.h5\"\n",
    "original_gif_data_path = \"Raw_Data/GIF_dataset.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Process_SHHS_Dataset(\n",
    "    path_to_shhs_dataset = original_shhs_data_path,\n",
    "    path_to_save_processed_data = processed_shhs_path,\n",
    "    path_to_project_configuration = model_directory_path + project_configuration_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Process_GIF_Dataset(\n",
    "    path_to_gif_dataset = original_gif_data_path,\n",
    "    path_to_save_processed_data = processed_gif_path,\n",
    "    path_to_project_configuration = model_directory_path + project_configuration_file\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Network\n",
    "\n",
    "The following function is designed to have the same structure as taught in the \n",
    "[PyTorch Tutorials](https://pytorch.org/tutorials/beginner/basics/). \n",
    "The only major difference is that the learning rate is not a fixed value, but dependend on the epoch using the\n",
    "'CosineScheduler' class.\n",
    "\n",
    "Again, the function is well documented. See 'main_model_training' in 'main.py'.\n",
    "\n",
    "First, the model will be trained on the SHHS dataset for a certain number of epochs. During training, the \n",
    "accuracy and loss are saved in a pickle file for every epoch. The final model state dictionary is saved in a \n",
    ".pth file.\n",
    "\n",
    "Afterwards, the model will be further trained on the GIF dataset, again saving the course of accuracy and loss\n",
    "to a pickle file. The updated model state will be saved to another .pth file.\n",
    "\n",
    "All files will be saved to the directory set above ([Setting File Paths](#setting-file-paths))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model on SHHS dataset\n",
    "main_model_training(\n",
    "    neural_network_model = SleepStageModel,\n",
    "    neural_network_hyperparameters = neural_network_hyperparameters_shhs,\n",
    "    path_to_processed_data = processed_shhs_path,\n",
    "    path_to_project_configuration = model_directory_path + project_configuration_file,\n",
    "    path_to_model_state = None,\n",
    "    path_to_updated_model_state = model_directory_path + model_state_after_shhs_file,\n",
    "    path_to_loss_per_epoch = model_directory_path + loss_per_epoch_shhs_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model on GIF dataset\n",
    "main_model_training(\n",
    "    neural_network_model = SleepStageModel,\n",
    "    neural_network_hyperparameters = neural_network_hyperparameters_gif,\n",
    "    path_to_processed_data = processed_gif_path,\n",
    "    path_to_project_configuration = model_directory_path + project_configuration_file,\n",
    "    path_to_model_state = model_directory_path + model_state_after_shhs_file,\n",
    "    path_to_updated_model_state = model_directory_path + model_state_after_shhs_gif_file,\n",
    "    path_to_loss_per_epoch = model_directory_path + loss_per_epoch_gif_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All IDs are unique.\n",
      "\n",
      "Preproccessing datapoints from NAKO dataset (ensuring uniformity):\n",
      "   ✅: 100.0% [██████████████████████] 7365 / 7365 | 7m 51s / 7m 51s (0.1s/it) | \n"
     ]
    }
   ],
   "source": [
    "Process_NAKO_Dataset(\n",
    "    path_to_nako_dataset = \"/Volumes/NaKo-UniHalle/RRI_and_MAD/NAKO-33a.pkl\",\n",
    "    path_to_save_processed_data = \"Processed_NAKO/NAKO-33a.pkl\",\n",
    "    path_to_project_configuration = \"SSM_no_overlap/Project_Configuration.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cpu device\n",
      "\n",
      "Predicting Sleep Stages:\n",
      "   ⏳: 3.2% [█░░░░░░░░░░░░░░░░░░░] 484 / 15333 | 1m 25s / 44m 52s (0.2s/it) |"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain_model_predicting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneural_network_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSleepStageModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_to_model_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSSM_no_overlap/Model_State.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_to_processed_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessed_NAKO/NAKO-33a.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_to_project_configuration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSSM_no_overlap/Project_Configuration.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Sleep_Phase_Prediction/main.py:1031\u001b[0m, in \u001b[0;36mmain_model_predicting\u001b[0;34m(neural_network_model, path_to_model_state, path_to_processed_data, path_to_project_configuration, path_to_save_results)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     original_signal_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(signal_length_seconds \u001b[38;5;241m*\u001b[39m slp_frequency))\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03mApplying Neural Network Model\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1031\u001b[0m predictions_in_windows \u001b[38;5;241m=\u001b[39m \u001b[43mneural_network_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03mPreparing Predicted Sleep Phases\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m predictions_in_windows \u001b[38;5;241m=\u001b[39m predictions_in_windows\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/mambaforge/envs/master/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/master/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Sleep_Phase_Prediction/neural_network_model.py:815\u001b[0m, in \u001b[0;36mSleepStageModel.forward\u001b[0;34m(self, rri_signal, mad_signal)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03m========================\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03mSignal Feature Learning\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03m========================\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# Process RRI Signal\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m rri_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrri_signal_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrri_signal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# Process MAD Signal or create 0 tensor if MAD signal is not provided\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mad_signal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/master/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/master/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/master/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/master/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/master/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/master/lib/python3.12/site-packages/torch/nn/modules/conv.py:308\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/master/lib/python3.12/site-packages/torch/nn/modules/conv.py:304\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    302\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    303\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_model_predicting(\n",
    "    neural_network_model = SleepStageModel,\n",
    "    path_to_model_state = \"SSM_no_overlap/Model_State.pth\",\n",
    "    path_to_processed_data = \"Processed_NAKO/NAKO-33a.pkl\",\n",
    "    path_to_project_configuration = \"SSM_no_overlap/Project_Configuration.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing progress bar...True\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/propeter/mambaforge/envs/master/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from side_functions import *\n",
    "\n",
    "p = DynamicProgressBar(total=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All IDs are unique.\n",
      "\n",
      "Preproccessing datapoints from GIF dataset (ensuring uniformity):\n",
      "   ✅: 100.0% [█████████████████████████████] 293 / 293 | 3s / 3s (0.0s/it) |\n",
      "Distributing 80.0% / 20.0% of datapoints into training / validation pids, respectively:\n",
      "   ✅: 100.0% [█████████████████████████] 731 / 731 | 0.6s / 0.6s (0.0s/it) |[0, 1, 2, 3, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "processed_gif_path = \"Processed_Data_2/gif_data.pkl\"\n",
    "model_directory_path = \"SSM_no_overlap/\"\n",
    "\n",
    "Process_GIF_Dataset(\n",
    "    path_to_gif_dataset = original_gif_data_path,\n",
    "    path_to_save_processed_data = processed_gif_path,\n",
    "    path_to_project_configuration = model_directory_path + project_configuration_file\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
